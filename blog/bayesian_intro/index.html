<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta name=Description  content="Practical, extensible, and open-source actuarial modeling and analysis."> <meta property="og:description" content="Practical, extensible, and open-source actuarial modeling and analysis."> <meta property="og:image" content="/assets/logo_square.svg"> <script> MathJax = { tex: { inlineMath: [['$', '$'], ["\\(", "\\)"]], processEscapes: true, } }; </script> <script async src="/assets/MathJax/MathJax-3.2.2/es5/tex-mml-chtml.js"> </script> <style> @font-face { src: url("//assets/MathJax/MathJax-3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff"> } </style> <style> @font-face { src: url("//assets/MathJax/MathJax-3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff"> } </style> <style> @font-face { src: url("//assets/MathJax/MathJax-3.2.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff"> } </style> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Modern Bayesian Statistics for Actuaries | JuliaActuary</title> <link rel=apple-touch-icon  sizes=180x180  href="/assets/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="/assets/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="/assets/favicon-16x16.png"> <link rel=manifest  href="/assets/site.webmanifest"> <script src="https://cdn.jsdelivr.net/npm/swiffy-slider@1.2.0/dist/js/swiffy-slider.min.js" crossorigin=anonymous  defer></script> <link href="/css/slider.css" rel=stylesheet > <header> <div class=blog-name > <a href="/"> <div class=blog-name-logo-container > <div class=blog-name-logo > <img src="/assets/logo_square.svg" alt=logo  height=100px  width=100px  /> </div> <div>JuliaActuary</div> </div> </a> <div class=tagline >Practical, extensible, and open-source actuarial modeling and analysis. </div> </div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/packages">Packages</a> <li><a href="/community">Community</a> <ul> <li><a href="/community/#learn">Learn</a> <li><a href="/community/#integration_with_r_and_python">Integration</a> <li><a href="/community/#contributing">Contributing</a> </ul> <li><a href="/#blog">Blog</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h1 id=modern_bayesian_statistics_for_actuaries ><a href="#modern_bayesian_statistics_for_actuaries" class=header-anchor >Modern Bayesian Statistics for Actuaries</a></h1> <p>One of the first probabilistic theorems everyone learns is Bayes&#39; Theorem, but that theorem is conspicuously absent from most applications and practice. The reason for this is that outside of trivial introductory examples &#40;&quot;you tested positive for a disease...&quot;&#41; is that Bayes&#39; Theorem becomes intractably complicated to calculate the posterior distribution. Modern advances in computing power, algorithms, and open-source libraries have made it possible to start applying the most powerful theorem to much more complex problems.</p> <p>The advantage of this is that actuaries can now apply these techniques to problems in a principled and flexible way to understand uncertainty better than we have before by explicitly looking at the posterior distribution of the parameters in our model.</p> <h2 id=what_is_modern_bayesian_statistics ><a href="#what_is_modern_bayesian_statistics" class=header-anchor >What is modern Bayesian Statistics?</a></h2> <p>A Bayesian statistical model has four main components to focus on:</p> <ol> <li><p><strong>Prior</strong> beliefs about the random variables related to the problem at hand.</p> <li><p>A <strong>Model</strong> which defines how the random variables give rise to the observed outcome</p> <li><p><strong>Data</strong> which we use to update our prior beliefs</p> <li><p><strong>Posterior</strong> distributions of our random variables, conditioned on the observed data and our model</p> </ol> <p>Having defined the first two components and collected our data, the workflow involves computationally sampling the posterior distribution, often using a technique called Markov Chain Monte-Carlo &#40;MCMC&#41;. The result is a series of values that are sampled statistically from the posterior distribution.</p> <h2 id=advantages_of_the_bayesian_approach ><a href="#advantages_of_the_bayesian_approach" class=header-anchor >Advantages of the Bayesian Approach</a></h2> <p>The main advantages of this approach over traditional actuarial techniques are:</p> <ol> <li><p><strong>Focus on distributions rather than point estimates of the posterior&#39;s mean or mode.</strong> We are often interested in the distribution of the parameters and a focus on a single parameter estimate will understate the risk distribution.</p> <li><p><strong>Model flexibility.</strong> A Bayesian model can be as simple as an ordinary linear regression, but as complex as modeling a full insurance mechanics.</p> <li><p><strong>Simpler mental model.</strong> Fundamentally, Bayes&#39; theorem could be distilled down to an approach where you count the ways that things could occur and update your beliefs accordingly.</p> <li><p><strong>Explicit Assumptions.</strong>: Enumerating the random variables in your model and explicitly parameterizing prior beliefs avoids ambiguity of the assumptions inside the statistical model.</p> </ol> <h2 id=challenges_with_the_bayesian_approach ><a href="#challenges_with_the_bayesian_approach" class=header-anchor >Challenges with the Bayesian Approach</a></h2> <p>With the Bayesian approach, there are a handful of things that are challenging. Many of the listed items are not unique to the Bayesian approach, but there are different facets of the issues that arise.</p> <ol> <li><p><strong>Model Construction</strong> - One must be thoughtful about the model and how variables interact. However, with the flexibility of modeling, you can apply &#40;actuarial&#41; science to makes better models&#33; </p> <li><p><strong>Model Diagnostics</strong> Instead of R^2 values, there are unique diagnostics that one must monitor to ensure that the posterior sampling </p> </ol> <p>Algorithm choice</p> <ol start=3 > <li><p><strong>Model Complexity and Size of Data</strong> - The sampling algorithms are computationally intensive - as the amount of data grows and model complexity grows, the runtime demands cluster computing.</p> </ol> <h2 id=why_now ><a href="#why_now" class=header-anchor >Why now?</a></h2> <p>There are both philosophical and practical reasons why Bayesian analysis is rapidly changing the statistical landscape.</p> <p><em>Philosophically</em>, one of the main reasons why Bayesian thinking is appealing is its ability to provide a straightforward interpretation of statistical conclusions.</p> <p>For example, when estimating an unknown quantity, a Bayesian probability interval can be directly understood as having a high probability of containing that quantity. In contrast, a frequentist confidence interval is typically interpreted only in the context of a series of similar inferences that could be made in repeated practice. In recent years, there has been a growing emphasis on interval estimation rather than hypothesis testing in applied statistics. This shift has strengthened the Bayesian perspective since it is likely that many users of standard confidence intervals intuitively interpret them in a manner consistent with Bayesian thinking.</p> <p><em>Practically</em>, recent advances in computational power, algorithm development, and open-source libraries have enabled practitioners to adapt the Bayesian workflow.</p> <p>Deriving the posterior distribution is analytically intractable so computational methods must be used. Advances in raw computing power only in the 1990&#39;s made non-trivial Bayesian analysis possible, and recent advances in algorithms have made the computations more efficient. For example, one of the most popular algorithms, NUTS, was only published in the 2010&#39;s. </p> <p>Many problems require the use of compute clusters to manage runtime, but if there is any place to invest in understanding posterior probability distributions, its insurance companies trying to manage risk&#33;</p> <p>Moreover, the availability of open-source libraries, such as Turing.jl, PyMC3, and Stan provide access to the core routines in an accessible interface.</p> <h2 id=subjectivity_of_the_priors ><a href="#subjectivity_of_the_priors" class=header-anchor >Subjectivity of the Priors?</a></h2> <p>Two ways one might react to subjectivity in a Bayesian context: it&#39;s a feature that should be embraced or itâ€™s a flaw that should be avoided.</p> <h3 id=as_a_feature ><a href="#as_a_feature" class=header-anchor >As a feature</a></h3> <p><strong>A Bayesian approach to defining a statistical model is an approach that allows for explicitly incorporating actuarial judgment.</strong> Establishing prior beliefs in a Bayesian model would force the actuary to be explicit about othewise fuzzy predilections. The explicit assumption is also more amenable to productive debate about its merits and biases than an implicit judgemental override.</p> <h3 id=subjectivity_as_a_flaw ><a href="#subjectivity_as_a_flaw" class=header-anchor >Subjectivity as a flaw</a></h3> <p>Subjectivity is inherent in all useful statistical methods. Subjectivity in traditional approaches include how the data was collected, which hypothesis to test, what significant levels to use, and assumptions about the data-generating processes. </p> <p>In fact, the &quot;objective&quot; approach to null hypothesis testing is so prone to abuse and misinterpretation that in 2016, the American Statistical Association issued a statement intended to steer statistical analysis into a &quot;post p&lt;0.05 era&quot;. That &quot;p&lt;0.05&quot; approach is embedded in most traditional approaches to actuarial credibility<sup id="fnref:1"><a href="#fndef:1" class=fnref >[1]</a></sup> and therefore should be similarly reconsidered.</p> <h3 id=maximum_entropy_distributions ><a href="#maximum_entropy_distributions" class=header-anchor >Maximum Entropy Distributions</a></h3> <p>Further, when assigning a prior assumption to a random variable, there are mathematically most conservative choices to pull from. These are called Maximum Entropy Distributions &#40;MED&#41; and it can be shown that for certain minimal constraints these are the information-theoretic least informative choices. Least informative means that the prior will have the least influence on the resulting posterior distribution. </p> <p>For example, if all you know is that the mean of a random process is positive, then the Exponential Distribution is your MED. If you know that a mean and variance must exist for the process, then the Normal distribution is your MED. If you know nothing at all, you can use a Uniform distribution for the possible values.</p> <h2 id=bayesian_vs_machine_learning ><a href="#bayesian_vs_machine_learning" class=header-anchor >Bayesian vs Machine Learning</a></h2> <p>Machine learning &#40;ML&#41; is <em>fully compatible</em> with Bayesian analysis - one can derive posterior distributions for the ML parameters like any other statistical model and the combination of approaches may be fruitful in practice.</p> <p>However, to the extent that actuaries have leaned on ML approaches due to the shortcomings of traditional actuarial approaches, Bayesian modeling may provide an attractive alternative without resorting to notoriously finicky and difficult-to-explain ML models. The Bayesian framework provides an explainable model and offers several analytic extensions beyond the scope of this introductory article:</p> <ul> <li><p>Causal Modeling: identifying not just correlated relationships, but causal ones, in contexts where a traditional experiment is unavailable</p> <li><p>Bayes Action: optimizing a parameter for, e.g., a CTE95 level instead of a parameter mean</p> <li><p>Information Criterion: principled techniques to compare model fit and complexity</p> <li><p>Missing data: mechanisms to handle the different kinds of missing data</p> <li><p>Model averaging: posteriors can be combined from different models to synthesize different approaches</p> </ul> <h2 id=paving_the_way_forward_for_actuaries ><a href="#paving_the_way_forward_for_actuaries" class=header-anchor >Paving the way forward for Actuaries</a></h2> <p>Bayesian approaches to statistical problems are rapidly changing the professional statistical field. To the extent that the actuarial profession incorporates statistical procedures we should consider adopting the same practices. The benefits of this are a better understanding of the distribution of risks, results that are more interpretable and explainable, and techniques that can applied to a wider range of problems.</p> <p>For actuaries interested in learning more, there are number of available resources to be found. Textbooks recommended by the author are:</p> <ul> <li><p>Statistical Rethinking &#40;McElreath&#41;</p> <li><p>Bayes Rules&#33; &#40;Johnson, Ott, Dogucu&#41;</p> <li><p>Bayesian Data Analysis &#40;Gelman, et. al.&#41;</p> </ul> <p>Additionally, the author has published a few examples of actuarial analysis on JuliaActuary.org</p> <table class=fndef  id="fndef:1"> <tr> <td class=fndef-backref ><a href="#fnref:1">[1]</a> <td class=fndef-content >Note that the approach discussed here is much more encompassing than the Buhlman-Straub Bayesian approach described in the Actuarial literature. </table> <div class=page-foot > The packages in JuliaActuary are open-source and liberally licensed (MIT License) to allow wide private and commercial usage of the packages, like the base Julia language and many other packages in the ecosystem. See <a href="/terms/" >terms of this site.</a> <div class=copyright > &copy; Alec Loudenback. Last modified: July 13, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and <a href="https://julialang.org">Julia</a>. </div> </div></div> <script data-goatcounter="https://juliaactuary.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>