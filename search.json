[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem\n\n\n\n\n\n\ncode\n\n\nblog\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nModern Bayesian Statistics for Actuaries\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\nblog\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nJuliaActuary and Hacktoberfest 2022\n\n\n\n\n\n\ncode\n\n\ncontributing\n\n\nblog\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Julia for Actuaries\n\n\n\n\n\n\nindustry\n\n\nblog\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nOct 24, 2021\n\n\nAlec Loudenback and Dimitar Vanguelov\n\n\n\n\n\n\n\n\n\n\n\n\nThe Life Modeling Problem: A Comparison of Julia, Rust, Python, and R\n\n\n\n\n\n\ncode\n\n\nblog\n\n\n\n\n\n\n\n\n\nMay 16, 2021\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nJulia for Actuaries\n\n\n\n\n\n\ncode\n\n\nindustry\n\n\nr\n\n\npython\n\n\nblog\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nCoding the Future\n\n\n\n\n\n\ncode\n\n\nindustry\n\n\nblog\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\nAlec Loudenback\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Packages",
    "section": "",
    "text": "These packages are available for use in your project. Scroll down for more information and links to the associated repository for each one."
  },
  {
    "objectID": "packages.html#adding-and-using-packages",
    "href": "packages.html#adding-and-using-packages",
    "title": "Packages",
    "section": "Adding and Using Packages",
    "text": "Adding and Using Packages\nThere are two ways to add packages:\n\nIn the code itself: using Pkg; Pkg.add(\"MortalityTables\")\nIn the REPL, hit ] to enter Pkg mode and type add MortalityTables More info can be found at the Pkg manager documentation.\n\nTo use packages in your code:\nusing PackageName"
  },
  {
    "objectID": "packages.html#mortalitytables.jl",
    "href": "packages.html#mortalitytables.jl",
    "title": "Packages",
    "section": "MortalityTables.jl",
    "text": "MortalityTables.jl\n\nHassle-free mortality and other rate tables.\n\n\nFeatures\n\nFull set of SOA mort.soa.org tables included\nsurvival and decrement functions to calculate decrements over period of time\nPartial year mortality calculations (Uniform, Constant, Balducci)\nFriendly syntax and flexible usage\nExtensive set of parametric mortality models.\n\n\n\nQuickstart\nLoad and see information about a particular table:\njulia&gt; vbt2001 = MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\")\n\nMortalityTable (Insured Lives Mortality):\n   Name:\n       2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\n   Fields:\n       (:select, :ultimate, :metadata)\n   Provider:\n       Society of Actuaries\n   mort.SOA.org ID:\n       1118\n   mort.SOA.org link:\n       https://mort.soa.org/ViewTable.aspx?&TableIdentity=1118\n   Description:\n       2001 Valuation Basic Table (VBT) Residual Standard Select and Ultimate Table -  Male Nonsmoker.\n       Basis: Age Nearest Birthday. \n       Minimum Select Age: 0. \n       Maximum Select Age: 99. \n       Minimum Ultimate Age: 25. \n       Maximum Ultimate Age: 120\nThe package revolves around easy-to-access vectors which are indexed by attained age:\njulia&gt; vbt2001.select[35]          # vector of rates for issue age 35\n 0.00036\n 0.00048\n ⋮\n 0.94729\n 1.0\n \njulia&gt; vbt2001.select[35][35]      # issue age 35, attained age 35\n 0.00036\n \njulia&gt; vbt2001.select[35][50:end] # issue age 35, attained age 50 through end of table\n0.00316\n0.00345\n ⋮\n0.94729\n1.0\n\njulia&gt; vbt2001.ultimate[95]        # ultimate vectors only need to be called with the attained age\n 0.24298\nCalculate the force of mortality or survival over a range of time:\njulia&gt; survival(vbt2001.ultimate,30,40) # the survival between ages 30 and 40\n0.9894404665434904\n\njulia&gt; decrement(vbt2001.ultimate,30,40) # the decrement between ages 30 and 40\n0.010559533456509618\nNon-whole periods of time are supported when you specify the assumption (Constant(), Uniform(), or Balducci()) for fractional periods:\njulia&gt; survival(vbt2001.ultimate,30,40.5,Uniform()) # the survival between ages 30 and 40.5\n0.9887676470262408\n\n\nParametric Models\nOver 20 different models included. Example with the Gompertz model\nm = MortalityTables.Gompertz(a=0.01,b=0.2)\n\nm[20]                 # the mortality rate at age 20\ndecrement(m,20,25)    # the five year cumulative mortality rate\nsurvival(m,20,25) # the five year survival rate\nMortalityTables package on Github 🡭"
  },
  {
    "objectID": "packages.html#actuaryutilities.jl",
    "href": "packages.html#actuaryutilities.jl",
    "title": "Packages",
    "section": "ActuaryUtilities.jl",
    "text": "ActuaryUtilities.jl\n\nA collection of common functions/manipulations used in Actuarial Calculations.\n\nA collection of common functions/manipulations used in Actuarial Calculations.\n\nQuickstart\ncfs = [5, 5, 105]\ntimes    = [1, 2, 3]\n\ndiscount_rate = 0.03\n\npresent_value(discount_rate, cfs, times)           # 105.65\nduration(Macaulay(), discount_rate, cfs, times)    #   2.86\nduration(discount_rate, cfs, times)                #   2.78\nconvexity(discount_rate, cfs, times)               #  10.62\n\n\nFeatures\n\nFinancial Maths\n\nduration:\n\nCalculate the Macaulay, Modified, or DV01 durations for a set of cashflows\n\nconvexity for price sensitivity\nFlexible interest rate options via the FinanceModels.jl package.\ninternal_rate_of_return or irr to calculate the IRR given cashflows (including at timepoints like Excel’s XIRR)\nbreakeven to calculate the breakeven time for a set of cashflows\naccum_offset to calculate accumulations like survivorship from a mortality vector\n\n\n\nOptions Pricing\n\neurocall and europut for Black-Scholes option prices\n\n\n\nRisk Measures\n\nCalculate risk measures for a given vector of risks:\n\nCTE for the Conditional Tail Expectation, or\nVaR for the percentile/Value at Risk.\n\n\n\n\nInsurance mechanics\n\nduration:\n\nCalculate the duration given an issue date and date (a.k.a. policy duration)\n\n\nActuaryUtilities package on GitHub 🡭"
  },
  {
    "objectID": "packages.html#lifecontingencies.jl",
    "href": "packages.html#lifecontingencies.jl",
    "title": "Packages",
    "section": "LifeContingencies.jl",
    "text": "LifeContingencies.jl\n\nCommon life contingent calculations with a convenient interface.\n\n\nFeatures\n\nIntegration with other JuliaActuary packages such as MortalityTables.jl\nFast calculations, with some parts utilizing parallel processing power automatically\nUse functions that look more like the math you are used to (e.g. A, ä) with Unicode support\nAll of the power, speed, convenience, tooling, and ecosystem of Julia\nFlexible and modular modeling approach\n\n\n\nPackage Overview\n\nLeverages MortalityTables.jl for the mortality calculations\nContains common insurance calculations such as:\n\nInsurance(life,yield): Whole life\nInsurance(life,yield,n): Term life for n years\nä(life,yield): present_value of life-contingent annuity\nä(life,yield,n): present_value of life-contingent annuity due for n years\n\nContains various commutation functions such as D(x),M(x),C(x), etc.\nSingleLife and JointLife capable\nInterest rate mechanics via FinanceModels.jl\nMore documentation available by clicking the DOCS badges at the top of this README\n\n\n\nExamples\n\nBasic Functions\nCalculate various items for a 30-year-old male nonsmoker using 2015 VBT base table and a 5% interest rate\n\nusing LifeContingencies\nusing MortalityTables\nusing FinanceModels\nimport LifeContingencies: V, ä     # pull the shortform notation into scope\n\n# load mortality rates from MortalityTables.jl\nvbt2001 = MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\")\n\nissue_age = 30\nlife = SingleLife(                 # The life underlying the risk\n    mortality = vbt2001.select[issue_age],    # -- Mortality rates\n)\n\nyield = FinanceModels.Yield.Constant(0.05) \n\nlc = LifeContingency(life, yield)  # LifeContingency joins the risk with interest\n\n\nins = Insurance(lc)                # Whole Life insurance\nins = Insurance(life, yield)       # alternate way to construct\nWith the above life contingent data, we can calculate vectors of relevant information:\ncashflows(ins)                     # A vector of the unit cashflows\ntimepoints(ins)                    # The timepoints associated with the cashflows\nsurvival(ins)                      # The survival vector\nbenefit(ins)                       # The unit benefit vector\nprobability(ins)                   # The probability of benefit payment\nSome of the above will return lazy results. For example, cashflows(ins) will return a Generator which can be efficiently used in most places you’d use a vector of cashflows (e.g. pv(...) or sum(...)) but has the advantage of being non-allocating (less memory used, faster computations). To get a computed vector instead of the generator, simply call collect(...) on the result: collect(cashflows(ins)).\nOr calculate summary scalars:\npresent_value(ins)                 # The actuarial present value\npremium_net(lc)                    # Net whole life premium \nV(lc,5)                            # Net premium reserve for whole life insurance at time 5\nOther types of life contingent benefits:\nInsurance(lc,10)                 # 10 year term insurance\nAnnuityImmediate(lc)               # Whole life annuity due\nAnnuityDue(lc)                     # Whole life annuity due\nä(lc)                              # Shortform notation\nä(lc, 5)                           # 5 year annuity due\nä(lc, 5, certain=5,frequency=4)    # 5 year annuity due, with 5 year certain payable 4x per year\n...                                # and more!\n\n\nConstructing Lives\nSingleLife(vbt2001.select[50])                 # no keywords, just a mortality vector\nSingleLife(vbt2001.select[50],issue_age = 60)  # select at 50, but now 60\nSingleLife(vbt2001.select,issue_age = 50)      # use issue_age to pick the right select vector\nSingleLife(mortality=vbt2001.select,issue_age = 50) # mort can also be a keyword\n\nLifeContingencies package on GitHub 🡭"
  },
  {
    "objectID": "packages.html#financemodels.jl",
    "href": "packages.html#financemodels.jl",
    "title": "Packages",
    "section": "FinanceModels.jl",
    "text": "FinanceModels.jl\n\nFlexible and composable yield curves and interest functions.\n\nFinanceModels.jl provides a set of composable contracts, models, and functions that allow for modeling of both simple and complex financial instruments. The resulting models, such as discount rates or term structures, can then be used across the JuliaActuary ecosystem to perform actuarial and financial analysis.\nAdditionally, the models can be used to project contracts through time: most basically as a series of cashflows but more complex output can be defined for contracts.\n\n\n\nYields Curve Fitting in FinanceModels.jl\n\n\n\nQuickStart\nusing FinanceModels\n\n# a set of market-observed prices we wish to calibrate the model to\n# annual effective unless otherwise specified\nq_rate = ZCBYield([0.01,0.02,0.03]);\nq_spread = ZCBYield([0.01,0.01,0.01]);\n\n# bootstrap a linear spline yield model\nmodel_rate = fit(Spline.Linear(),q_rate,Fit.Bootstrap());⠀           \nmodel_spread = fit(Spline.Linear(),q_spread,Fit.Bootstrap());\n\n# the zero rate is the combination of the two underlying rates\nzero(m_spread + m_rate,1) # 0.02 annual effective rate \n\n# the discount is the same as if we added the underlying zero rates\ndiscount(m_spread + m_rate,0,3) ≈ discount(0.01 + 0.03,3)   # true\n\n# compute the present value of a contract (a cashflow of 10 at time 3)\npresent_value(m_rate,Cashflow(10,3)) # 9.15...\n\n\nOverview of FinanceModels\n\n\n\nA conceptual sketch of FinanceModels.jl\n\n\nOften we start with observed or assumed values for existing contracts. We want to then use those assumed values to extend the valuation logic to new contracts. For example, we may have a set of bond yields which we then want to discount a series of insurance obligations.\nIn the language of FinanceModels, we would have a set of Quotes which are used to fit a Model. That model is then used to discount a new series of cashflows.\nThat’s just an example, and we can use the various components in different ways depending on the objective of the analysis.\n\n\nContracts and Quotes\nContracts are a way to represent financial obligations. These can be valued using a model, projected into a future steam of values, or combined with assumed prices as a Quote.\nIncluded are a number of primitives and convenience methods for contracts:\nExisting structs:\n\nCashflow\nBond.Fixed\nBond.Floating\nForward (an obligation with a forward start time)\nComposite (combine two other contracts, e.g. into a swap)\nEuroCall\nCommonEquity\n\nCommonly, we deal with conventions that imply a contract and an observed price. For example, we may talk about a treasury yield of 0.03. This is a description that implies a Quoteed price for an underling fixed bond. In FinanceModels, we could use CMTYield(rate,tenor) which would create a Quote(price,Bond.Fixed(...)). In this way, we can conveniently create a number of Quotes which can be used to fit models. Such convenience methods include:\n\nZCBYield\nZCBPrice\nCMTYield\nParYield\nParSwapYield\nForwardYield\n\nFinanceModels offers a way to define new contracts as well.\n\nCashflows\nA Cashflows obligation are themselves a contract, but other contracts can be considered as essentially anything that can be combined with assumptions (a model) to derive a collection of cashflows.\nFor example, a obligation that pays 1.75 at time 2 could be represented as: Cashflow(1.75,2).\n\n\n\nModels\nModels are objects that can be fit to observed prices and then subsequently used to make valuations of other cashflows/contracts.\nYield models include:\n\nYield.Constant\nBootstrapped Splines\nYield.SmithWilson\nYield.NelsonSiegel\nYield.NelsonSiegelSvensson\n\n\nYield-related functions\nThe models can be used to compute various rates of interest:\n\ndiscount(curve,from,to) or discount(curve,to) gives the discount factor\naccumulation(curve,from,to) or accumulation(curve,to) gives the accumulation factor\nzero(curve,time) or zero(curve,time,Frequency) gives the zero-coupon spot rate for the given time.\nforward(curve,from,to) gives the zero rate between the two given times\npar(curve,time;frequency=2) gives the coupon-paying par equivalent rate for the given time.\n\nOther models include:\n\nBlackScholesMerton derivative valuation\n\n\n\n\nProjections\nMost basically, we can project a contract into a series of Cashflows:\njulia&gt; b = Bond.Fixed(0.04,Periodic(2),3)\nFinanceModels.Bond.Fixed{Periodic, Float64, Int64}(0.04, Periodic(2), 3)\n\njulia&gt; collect(b)\n6-element Vector{Cashflow{Float64, Float64}}:\n Cashflow{Float64, Float64}(0.02, 0.5)\n Cashflow{Float64, Float64}(0.02, 1.0)\n Cashflow{Float64, Float64}(0.02, 1.5)\n Cashflow{Float64, Float64}(0.02, 2.0)\n Cashflow{Float64, Float64}(0.02, 2.5)\n Cashflow{Float64, Float64}(1.02, 3.0)\nHowever, Projections allow one to combine three elements which can be extended to define any desired output (such as amortization schedules, financial statement projections, or account value rollforwards). The three elements are:\n\nthe underlying contract of interest\nthe model which includes assumptions of how the contract will behave\na ProjectionKind which indicates the kind of output desired (cashflow stream, amortization schedule, etc…)\n\n\n\nFitting Models\n       Model                                                               Method\n          |                                                                   |\n    |------------|                                                     |---------------|\nfit(Spline.Cubic(), CMTYield.([0.04,0.05,0.055,0.06,0055],[1,2,3,4,5]), Fit.Bootstrap())\n                    |-------------------------------------------------|\n                                              |\n                                              Quotes\n\nModel could be Spline.Linear(), Yield.NelsonSiegelSvensson(), Equity.BlackScholesMerton(...), etc.\nQuote could be CMTYields, ParYields, Option.Eurocall, etc.\nMethod could be Fit.Loss(x-&gt;x^2), Fit.Loss(x-&gt;abs(x)), Fit.Bootstrap(), etc.\n\nThis unified way to fit models offers a much simpler way to extend functionality to new models or contract types.\n\nUsing Models\nAfter being fit, models can be used to value contracts:\npresent_value(model,cashflows)\nAdditionally, ActuaryUtilities.jl offers a number of other methods that can be used, such as duration, convexity, price which can be used for analysis with the fitted models.\n\n\n\nRates\nRates are types that wrap scalar values to provide information about how to determine discount and accumulation factors.\nThere are two Frequency types:\n\nPeriodic(m) for rates that compound m times per period (e.g. m times per year if working with annual rates).\nContinuous() for continuously compounding rates.\n\n\nExamples\nContinuous(0.05)       # 5% continuously compounded\nPeriodic(0.05,2)       # 5% compounded twice per period\nThese are both subtypes of the parent Rate type and are instantiated as:\nRate(0.05,Continuous())       # 5% continuously compounded\nRate(0.05,Periodic(2))        # 5% compounded twice per period\nRates can also be constructed by specifying the Frequency and then passing a scalar rate:\nPeriodic(1)(0.05)\nContinuous()(0.05)\n\n\nConversion\nConvert rates between different types with convert. E.g.:\nr = Rate(FinanceModels.Periodic(12),0.01)             # rate that compounds 12 times per rate period (ie monthly)\n\nconvert(FinanceModels.Periodic(1),r)                  # convert monthly rate to annual effective\nconvert(FinanceModels.Continuous(),r)          # convert monthly rate to continuous\n\n\nArithmetic\nAdding, substracting, multiplying, dividing, and comparing rates is supported.\nFinanceModels package on GitHub 🡭"
  },
  {
    "objectID": "packages.html#experienceanalysis.jl",
    "href": "packages.html#experienceanalysis.jl",
    "title": "Packages",
    "section": "ExperienceAnalysis.jl",
    "text": "ExperienceAnalysis.jl\n\nMeeting your exposure calculation needs.\n\n\nQuickstart\ndf = DataFrame(\n    policy_id = 1:3,\n    issue_date = [Date(2020,5,10), Date(2020,4,5), Date(2019, 3, 10)],\n    end_date = [Date(2022, 6, 10), Date(2022, 8, 10), Date(2022,12,31)],\n    status = [\"claim\", \"lapse\", \"inforce\"]\n)\n\ndf.policy_year = exposure.(\n    ExperienceAnalysis.Anniversary(Year(1)),\n    df.issue_date,\n    df.end_date,\n    df.status .== \"claim\"; # continued exposure\n    study_start = Date(2020, 1, 1),\n    study_end = Date(2022, 12, 31)\n)\n\ndf = flatten(df, :policy_year)\n\ndf.exposure_fraction =\n        map(e -&gt; yearfrac(e.from, e.to + Day(1), DayCounts.Thirty360()), df.policy_year) \n# + Day(1) above because DayCounts has Date(2020, 1, 1) to Date(2021, 1, 1) as an exposure of 1.0\n# here we end the interval at Date(2020, 12, 31), so we need to add a day to get the correct exposure fraction.\n\n\n\n\n\n\n\n\n\n\n\npolicy_idInt64\nissue_dateDate\nend_dateDate\nstatusString\npolicy_year@NamedTuple{from::Date, to::Date, policy\\_timestep::Int64}\nexposure_fractionFloat64\n\n\n\n\n1\n2020-05-10\n2022-06-10\nclaim\n(from = Date(“2020-05-10”), to = Date(“2021-05-09”), policy_timestep = 1)\n1.0\n\n\n1\n2020-05-10\n2022-06-10\nclaim\n(from = Date(“2021-05-10”), to = Date(“2022-05-09”), policy_timestep = 2)\n1.0\n\n\n1\n2020-05-10\n2022-06-10\nclaim\n(from = Date(“2022-05-10”), to = Date(“2023-05-09”), policy_timestep = 3)\n1.0\n\n\n2\n2020-04-05\n2022-08-10\nlapse\n(from = Date(“2020-04-05”), to = Date(“2021-04-04”), policy_timestep = 1)\n1.0\n\n\n2\n2020-04-05\n2022-08-10\nlapse\n(from = Date(“2021-04-05”), to = Date(“2022-04-04”), policy_timestep = 2)\n1.0\n\n\n2\n2020-04-05\n2022-08-10\nlapse\n(from = Date(“2022-04-05”), to = Date(“2022-08-10”), policy_timestep = 3)\n0.35\n\n\n3\n2019-03-10\n2022-12-31\ninforce\n(from = Date(“2020-01-01”), to = Date(“2020-03-09”), policy_timestep = 1)\n0.191667\n\n\n3\n2019-03-10\n2022-12-31\ninforce\n(from = Date(“2020-03-10”), to = Date(“2021-03-09”), policy_timestep = 2)\n1.0\n\n\n3\n2019-03-10\n2022-12-31\ninforce\n(from = Date(“2021-03-10”), to = Date(“2022-03-09”), policy_timestep = 3)\n1.0\n\n\n3\n2019-03-10\n2022-12-31\ninforce\n(from = Date(“2022-03-10”), to = Date(“2022-12-31”), policy_timestep = 4)\n0.808333\n\n\n\n\n\nAvailable Exposure Basis\n\nExperienceAnalysis.Anniversary(period) will give exposures periods based on the first date\nExperienceAnalysis.Calendar(period) will follow calendar periods (e.g. month or year)\nExperienceAnalysis.AnniversaryCalendar(period,period) will split into the smaller of the calendar or policy period.\n\nWhere period is a Period Type from the Dates standard library.\nCalculate exposures with exposures(basis,from,to,continue_exposure).\n\ncontinue_exposures indicates whether the exposure should be extended through the full exposure period rather than terminate at the to date.\n\nExperienceAnalysis package on GitHub 🡭"
  },
  {
    "objectID": "packages.html#economicscenariogenerators.jl",
    "href": "packages.html#economicscenariogenerators.jl",
    "title": "Packages",
    "section": "EconomicScenarioGenerators.jl",
    "text": "EconomicScenarioGenerators.jl\n\nEasy-to-use scenario generation that’s FinanceModels.jl compatible.\n\n\nModels\n\nInterest Rate Models\n\nVasicek\nCoxIngersolRoss\nHullWhite\n\n\n\nEquityModels\n\nBlackScholesMerton\n\n\n\n\nInterest Rate Model Examples\n\nVasicek\nm = Vasicek(0.136,0.0168,0.0119,Continuous(0.01)) # a, b, σ, initial Rate\ns = ScenarioGenerator(\n        1,  # timestep\n        30, # projection horizon\n        m,  # model\n    )\nThis can be iterated over, or you can collect all of the rates like:\nrates = collect(s)\nor\nfor r in s\n    # do something with r\nend\nAnd the package integrates with FinanceModels.jl:\nYieldCurve(s)\n\nwill produce a yield curve object:\n              ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Yield Curve (FinanceModels.BootstrapCurve)⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀           \n              ┌────────────────────────────────────────────────────────────┐           \n         0.03 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠤⠔⠒⠉⠉⠒⠒⠒⠒⠒⠤⣄⣀│ Zero rates\n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠀⠀⣀⡤⠖⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠖⠋⠁⠀⠀⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n   Continuous │⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠒⠓⠦⠤⠖⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⢰⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⣀⠖⠢⡀⡰⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠉⠉⠁⠀⠀⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n            0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n              └────────────────────────────────────────────────────────────┘           \n              ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀time⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀30⠀           \nEconomicScenarioGenerators package on GitHub 🡭"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JuliaActuary",
    "section": "",
    "text": "Practical, extensible, and open-source actuarial modeling and analysis.\n\n\n\n\nDesigned for speed and efficiency, enabling high-performance numerical computing, quantitative analysis, modeling, and simulation.\n\n\n\nExtensive ecosystem of libraries and tools catering to technical computing, data science, machine learning, and domain-specific tasks in finance and actuarial science.\n\n\n\nClean, readable syntax, comprehensive documentation, and seamless integration with existing languages and tools enhance productivity and accessibility for a wide range of users.\nJuliaActuary is an ecosystem of packages that makes Julia the easiest language to get started for actuarial workflows.\nJulia is an ideal language for Actuaries and other financial professionals.\nIt is free, open-source software and you can join the development on Github."
  },
  {
    "objectID": "index.html#code-examples",
    "href": "index.html#code-examples",
    "title": "JuliaActuary",
    "section": "Code Examples",
    "text": "Code Examples\n\n    \n        \n        \n          \n    Getting Mortality Tables\n  \njulia&gt; using MortalityTables\n\njulia&gt; MortalityTables.table(\"2015 VBT Smoker Distinct Male Non-Smoker ALB\")\nMortalityTable (Insured Lives Mortality):\n   Name:\n       2015 VBT Smoker Distinct Male Non-Smoker ALB\n   Fields:\n       (:select, :ultimate, :metadata)\n   Provider:\n       American Academy of Actuaries along with the Society of Actuaries\n   mort.SOA.org ID:\n       3269\n   mort.SOA.org link:\n       https://mort.soa.org/ViewTable.aspx?&TableIdentity=3269\n   Description:\n       2015 Valuation Basic Table (VBT) Smoker Distinct Table...\n       \n        \n        \n        \n          \n        Working with Yield Curves\n  \n\n\njulia&gt; using FinanceModels\n\njulia&gt; maturities = [0.5, 1.0, 1.5, 2.0]\njulia&gt; rates      = [5.0, 5.8, 6.4, 6.8] ./ 100\njulia&gt; quotes     = ZCBYield.(rates,maturities)\n\njulia&gt; rf_curve = fit(Spline.Cubic(), quotes, Fit.Bootstrap())\n\n\n               ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Yield Curve (FinanceModels.YieldCurve)⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n               ┌────────────────────────────────────────────────────────────┐\n           0.4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ Zero rates\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠒⠋│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⠤⠖⠊⠉⠁⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n   Periodic(1) │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡤⠤⠒⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠀⢀⡠⠤⠒⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               │⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n             0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│\n               └────────────────────────────────────────────────────────────┘\n               ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀time⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀30⠀\n\n\n        \n                \n        \n                \n          \n        Financial Maths\n  \n\njulia&gt; using ActuaryUtilities\n\njulia&gt; cashflows = [5, 5, 105]\njulia&gt; discount_rate = 0.03\n\njulia&gt; present_value(discount_rate, cashflows)           \n105.65\n\njulia&gt; duration(Macaulay(), discount_rate, cashflows)    \n2.86\n\njulia&gt; duration(discount_rate, cashflows)                \n2.78\n\njulia&gt; convexity(discount_rate, cashflows)               \n10.62\n\n\n        \n                \n        \n                \n          \n        Life Contingencies\n  \nusing LifeContingencies\nusing MortalityTables\nusing FinanceModels\n\n# load mortality rates from MortalityTables.jl\nvbt2015 = MortalityTables.table(\"2015 VBT Smoker Distinct Male Non-Smoker ALB\")\n\nissue_age = 30\nlife = SingleLife(                       # The life underlying the risk\n    mort = vbt2015.select[issue_age],    # -- Mortality rates\n)\n\nyield = Yield.Constant(0.05)            # Using a flat 5% interest rate\n\nins = Insurance(life, yield)             # alternate way to construct\n\n# Summary Scalars\npresent_value(ins)                       # The actuarial present value\npremium_net(lc)                          # Net whole life premium \nV(lc,5)                                  # Net premium reserve for whole life insurance at time 5"
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "JuliaActuary",
    "section": "Packages",
    "text": "Packages\nThese packages are available for use in your project. See more on the Packages page.\nMortalityTables.jl - Easily work with standard mort.SOA.org tables and parametric models with common survival calculations.\nLifeContingencies.jl - Insurance, annuity, premium, and reserve maths.\nActuaryUtilities.jl - Robust and fast calculations for internal_rate_of_return, duration, convexity, present_value, breakeven, and more.\nFinanceModels.jl - Composable contracts, models, and functions that allow for modeling of both simple and complex financial instruments.\nExperienceAnalysis.jl - Meeting your exposure calculation needs.\nEconomicScenarioGenerators.jl - Easy-to-use scenario generation that’s FinanceModels.jl compatible."
  },
  {
    "objectID": "posts/mortality-comparison/index.html",
    "href": "posts/mortality-comparison/index.html",
    "title": "Interactive Mortality Comparison Tool",
    "section": "",
    "text": "Scroll down below the sample video to see how to run it.\nThe recording above shows a Pluto.jl notebook visualizing the difference between different mortality tables."
  },
  {
    "objectID": "posts/mortality-comparison/index.html#instructions-to-run",
    "href": "posts/mortality-comparison/index.html#instructions-to-run",
    "title": "Interactive Mortality Comparison Tool",
    "section": "Instructions to Run",
    "text": "Instructions to Run\nBecause JuliaActuary doesn’t have an active server to run this on, you have to run it locally.\n\nOpen a Julia REPL and copy and paste the following:\n\n# install these dependencies\nimport Pkg; Pkg.add(\"Pluto\") \n\n# use and start Pluto\nusing Pluto; Pluto.run()\n\nIn the Pluto window that opens, enter this URL into the Open from file: box:\n\nhttps://raw.githubusercontent.com/JuliaActuary/Learn/master/MortalityTableComparison.jl"
  },
  {
    "objectID": "posts/exposures-example/index.html",
    "href": "posts/exposures-example/index.html",
    "title": "Exposure Calculation with ExperienceAnalysis.jl",
    "section": "",
    "text": "In this tutorial, we will walk through how to calculate exposures using the ExperienceAnalysis.jl package.\nIn summary, the package will help calculate the exposure periods given parameters about the kind of period and timepoints under consideration. This will return an array of tuples with a from and to date:\nusing ExperienceAnalysis\nusing Dates\n\nissue = Date(2016, 7, 4)\ntermination = Date(2020, 1, 17)\nbasis = ExperienceAnalysis.Anniversary(Year(1))\nexposure(basis, issue, termination)\n\n4-element Vector{@NamedTuple{from::Date, to::Date, policy_timestep::Int64}}:\n (from = Date(\"2016-07-04\"), to = Date(\"2017-07-03\"), policy_timestep = 1)\n (from = Date(\"2017-07-04\"), to = Date(\"2018-07-03\"), policy_timestep = 2)\n (from = Date(\"2018-07-04\"), to = Date(\"2019-07-03\"), policy_timestep = 3)\n (from = Date(\"2019-07-04\"), to = Date(\"2020-01-17\"), policy_timestep = 4)"
  },
  {
    "objectID": "posts/exposures-example/index.html#available-exposure-basis",
    "href": "posts/exposures-example/index.html#available-exposure-basis",
    "title": "Exposure Calculation with ExperienceAnalysis.jl",
    "section": "Available Exposure Basis",
    "text": "Available Exposure Basis\n\nExperienceAnalysis.Anniversary(period) will give exposures periods based on the first date\nExperienceAnalysis.Calendar(period) will follow calendar periods (e.g. month or year)\nExperienceAnalysis.AnniversaryCalendar(period,period) will split into the smaller of the calendar or policy anniversary period.\n\nWhere period is a Period Type from the Dates standard library.\nCalculate exposures with exposures(basis,from,to,continue_exposure).\n\ncontinue_exposures indicates whether the exposure should be extended through the full exposure period rather than terminate at the to date."
  },
  {
    "objectID": "posts/exposures-example/index.html#full-example",
    "href": "posts/exposures-example/index.html#full-example",
    "title": "Exposure Calculation with ExperienceAnalysis.jl",
    "section": "Full Example",
    "text": "Full Example\nWe’ll start with this as our data:\n\nusing DataFrames\ndf = DataFrame(\n    id=[1, 2, 3],\n    issue=[Date(2016, 7, 4), Date(2016, 1, 1), Date(2016, 1, 1)],\n    end_date=[Date(2020, 1, 17), Date(2018, 5, 4), Date(2020, 12, 31)],\n    status=[\"Claim\", \"Lapse\", \"Inforce\"]\n)\n\n3×4 DataFrame\n\n\n\nRow\nid\nissue\nend_date\nstatus\n\n\n\nInt64\nDate\nDate\nString\n\n\n\n\n1\n1\n2016-07-04\n2020-01-17\nClaim\n\n\n2\n2\n2016-01-01\n2018-05-04\nLapse\n\n\n3\n3\n2016-01-01\n2020-12-31\nInforce\n\n\n\n\n\n\nDefine the start and end of the study:\n\nstudy_end = Date(2020, 6, 30)\nstudy_start = Date(2018, 6, 30)\n\n2018-06-30\n\n\nCalculate the exposure by broadcasting the exposure function over the three arrays we are passing to it:\n\ndf.exposure = exposure.(\n    ExperienceAnalysis.Anniversary(Year(1)),   # The basis for our exposures\n    df.issue,                                  # The `from` date\n    df.end_date,                               # the last observed date\n    df.status .== \"Claim\";                        # a boolean vector indicating continuation\n    study_start=study_start,\n    study_end=study_end\n)\n\n3-element Vector{Vector{@NamedTuple{from::Date, to::Date, policy_timestep::Int64}}}:\n [(from = Date(\"2018-06-30\"), to = Date(\"2018-07-03\"), policy_timestep = 2), (from = Date(\"2018-07-04\"), to = Date(\"2019-07-03\"), policy_timestep = 3), (from = Date(\"2019-07-04\"), to = Date(\"2020-07-03\"), policy_timestep = 4)]\n []\n [(from = Date(\"2018-06-30\"), to = Date(\"2018-12-31\"), policy_timestep = 3), (from = Date(\"2019-01-01\"), to = Date(\"2019-12-31\"), policy_timestep = 4), (from = Date(\"2020-01-01\"), to = Date(\"2020-06-30\"), policy_timestep = 5)]\n\n\nIn our dataframe, we actually have a column that contains an array of tuples now, so to expand it so that each exposure period gets a row, we flatten the dataframe to get our exposures:\n\ndf = flatten(df, :exposure)\n\n6×5 DataFrame\n\n\n\nRow\nid\nissue\nend_date\nstatus\nexposure\n\n\n\nInt64\nDate\nDate\nString\nNamedTup…\n\n\n\n\n1\n1\n2016-07-04\n2020-01-17\nClaim\n(from = Date(\"2018-06-30\"), to = Date(\"2018-07-03\"), policy_timestep = 2)\n\n\n2\n1\n2016-07-04\n2020-01-17\nClaim\n(from = Date(\"2018-07-04\"), to = Date(\"2019-07-03\"), policy_timestep = 3)\n\n\n3\n1\n2016-07-04\n2020-01-17\nClaim\n(from = Date(\"2019-07-04\"), to = Date(\"2020-07-03\"), policy_timestep = 4)\n\n\n4\n3\n2016-01-01\n2020-12-31\nInforce\n(from = Date(\"2018-06-30\"), to = Date(\"2018-12-31\"), policy_timestep = 3)\n\n\n5\n3\n2016-01-01\n2020-12-31\nInforce\n(from = Date(\"2019-01-01\"), to = Date(\"2019-12-31\"), policy_timestep = 4)\n\n\n6\n3\n2016-01-01\n2020-12-31\nInforce\n(from = Date(\"2020-01-01\"), to = Date(\"2020-06-30\"), policy_timestep = 5)\n\n\n\n\n\n\n\nExposure Fraction\nThis can be extended to calculate the decimal fraction of the year under different day count conventions, such as assuming 30/360 or Actual/365, etc. using the DayCounts.jl package.\n\nusing DayCounts\n\ndf.exposure_fraction = map(e -&gt; yearfrac(e.from, e.to, DayCounts.Actual360()), df.exposure)\ndf[:, [:exposure, :exposure_fraction]]\n\n6×2 DataFrame\n\n\n\nRow\nexposure\nexposure_fraction\n\n\n\nNamedTup…\nFloat64\n\n\n\n\n1\n(from = Date(\"2018-06-30\"), to = Date(\"2018-07-03\"), policy_timestep = 2)\n0.00833333\n\n\n2\n(from = Date(\"2018-07-04\"), to = Date(\"2019-07-03\"), policy_timestep = 3)\n1.01111\n\n\n3\n(from = Date(\"2019-07-04\"), to = Date(\"2020-07-03\"), policy_timestep = 4)\n1.01389\n\n\n4\n(from = Date(\"2018-06-30\"), to = Date(\"2018-12-31\"), policy_timestep = 3)\n0.511111\n\n\n5\n(from = Date(\"2019-01-01\"), to = Date(\"2019-12-31\"), policy_timestep = 4)\n1.01111\n\n\n6\n(from = Date(\"2020-01-01\"), to = Date(\"2020-06-30\"), policy_timestep = 5)\n0.502778"
  },
  {
    "objectID": "posts/exposures-example/index.html#discussion-and-questions",
    "href": "posts/exposures-example/index.html#discussion-and-questions",
    "title": "Exposure Calculation with ExperienceAnalysis.jl",
    "section": "Discussion and Questions",
    "text": "Discussion and Questions\nIf you have other ideas or questions, feel free to also open an issue, or discuss on the community Zulip or Slack #actuary channel. We welcome all actuarial and related disciplines!\n\nReferences\n\nExperience Study Calculations by the Society of Actuaries\nExperienceAnalysis.jl package on GitHub"
  },
  {
    "objectID": "posts/stochastic-mortality/index.html",
    "href": "posts/stochastic-mortality/index.html",
    "title": "Stochastic claims projections demo",
    "section": "",
    "text": "using CSV, DataFrames\nusing MortalityTables, ActuaryUtilities\nusing Dates\nusing ThreadsX\nusing BenchmarkTools\nusing CairoMakie\nusing Random\nDefine a datatype. Not strictly necessary, but will make extending the program with more functions easier.\nType annotations are optional, but providing them is able to coerce the values to be all plain bits (i.e. simple, non-referenced values like arrays are) when the type is constructed. This makes the whole data be stored in the stack and is an example of data-oriented design. It’s much slower without the type annotations (~0.5 million policies per second, ~50x slower).\n@enum Sex Female = 1 Male = 2\n@enum Risk Standard = 1 Preferred = 2\n\nstruct Policy\n    id::Int\n    sex::Sex\n    benefit_base::Float64\n    COLA::Float64\n    mode::Int\n    issue_date::Date\n    issue_age::Int\n    risk::Risk\nend\nLoad the data:\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,COLA,mode,issue_date,issue_age,risk\n         1,M,100000.0,0.03,12,1999-12-05,30,Std\n         2,F,200000.0,0.03,12,1999-12-05,30,Pref\"\n    )\n\npolicies = let\n\n    # read CSV directly into a dataframe\n    # df = CSV.read(\"sample_inforce.csv\",DataFrame) # use local string for notebook\n    df = CSV.read(sample_csv_data, DataFrame)\n\n    # map over each row and construct an array of Policy objects\n    map(eachrow(df)) do row\n        Policy(\n            row.id,\n            row.sex == \"M\" ? Male : Female,\n            row.benefit_base,\n            row.COLA,\n            row.mode,\n            row.issue_date,\n            row.issue_age,\n            row.risk == \"Std\" ? Standard : Preferred,\n        )\n    end\n\n\nend\n\n2-element Vector{Policy}:\n Policy(1, Male, 100000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Standard)\n Policy(2, Female, 200000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Preferred)\nDefine what mortality gets used:\nmort = Dict(\n    Male =&gt; MortalityTables.table(988).ultimate,\n    Female =&gt; MortalityTables.table(992).ultimate,\n)\n\nfunction mortality(pol::Policy, params)\n    return params.mortality[pol.sex]\nend\n\nmortality (generic function with 1 method)\nThis defines the core logic of the policy projection and will write the results to the given out container (here, a named tuple of arrays).\nThis is using a threaded approach where it could be operating on any of the computer’s available threads, thus acheiving thread-based parallelism (as opposed to multi-processor (multi-machine) or GPU-based computation which requires formulating the problem a bit differently (array/matrix based). For the scale of computation here, I think I’d apply this model of parallelism.\nfunction pol_project!(out, policy, params)\n    # some starting values for the given policy\n    dur = duration(policy.issue_date, params.val_date)\n    start_age = policy.issue_age + dur - 1\n    COLA_factor = (1 + policy.COLA)\n    cur_benefit = policy.benefit_base * COLA_factor^(dur - 1)\n\n    # get the right mortality vector\n    qs = mortality(policy, params)\n\n    # grab the current thread's id to write to results container without conflicting with other threads\n    tid = Threads.threadid()\n\n    ω = lastindex(qs)\n\n    # inbounds turns off bounds-checking, which makes hot loops faster but first write loop without it to ensure you don't create an error (will crash if you have the error without bounds checking)\n    @inbounds for t in 1:min(params.proj_length, ω - start_age)\n\n        q = qs[start_age+t] # get current mortality\n\n        if (rand() &lt; q)\n            return # if dead then just return and don't increment the results anymore\n        else\n            # pay benefit, add a life to the output count, and increment the benefit for next year\n            out.benefits[t, tid] += cur_benefit\n            out.lives[t, tid] += 1\n            cur_benefit *= COLA_factor\n        end\n    end\nend\n\npol_project! (generic function with 1 method)\nParameters for our projection:\nparams = (\n    val_date=Date(2021, 12, 31),\n    proj_length=100,\n    mortality=mort,\n)\n\n(val_date = Date(\"2021-12-31\"), proj_length = 100, mortality = Dict{Sex, OffsetArrays.OffsetVector{Float64, Vector{Float64}}}(Male =&gt; [0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571  …  0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], Female =&gt; [0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745  …  0.376246, 0.386015, 0.393507, 0.398308, 0.4, 0.4, 0.4, 0.4, 0.4, 1.0]))\nCheck the number of threads we’re using:\nThreads.nthreads()\n\n4\nfunction project(policies, params)\n    threads = Threads.nthreads()\n    benefits = zeros(params.proj_length, threads)\n    lives = zeros(Int, params.proj_length, threads)\n    out = (; benefits, lives)\n    ThreadsX.foreach(policies) do pol\n        pol_project!(out, pol, params)\n    end\n    map(x -&gt; vec(reduce(+, x, dims=2)), out)\nend\n\nproject (generic function with 1 method)\nExample of single projection:\nproject(repeat(policies, 100_000), params)\n\n(benefits = [5.632041071730104e10, 5.673784917378363e10, 5.7095494627022224e10, 5.739296558536684e10, 5.765668319695172e10, 5.785147298991435e10, 5.79964498594893e10, 5.806223921979738e10, 5.809264362799425e10, 5.8029114887314896e10  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [195316, 190438, 185423, 180374, 175350, 170235, 165130, 159991, 154877, 149681  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
  },
  {
    "objectID": "posts/stochastic-mortality/index.html#benchmarking",
    "href": "posts/stochastic-mortality/index.html#benchmarking",
    "title": "Stochastic claims projections demo",
    "section": "Benchmarking",
    "text": "Benchmarking\nUsing a Macbook Air M3 laptop, about 45 million policies able to be stochastically projected per second:\n\npolicies_to_benchmark = 45_000_000\n# adjust the `repeat` depending on how many policies are already in the array\n# to match the target number for the benchmark\nn = policies_to_benchmark ÷ length(policies)\n\n@benchmark project(p, r) setup = (p = repeat($policies, $n); r = $params)\n\n\nBenchmarkTools.Trial: 4 samples with 1 evaluation.\n Range (min … max):  1.075 s …   1.161 s  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     1.083 s              ┊ GC (median):    0.00%\n Time  (mean ± σ):   1.101 s ± 40.427 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  █ █     █                                               █  \n  █▁█▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  1.07 s         Histogram: frequency by time        1.16 s &lt;\n Memory estimate: 29.27 KiB, allocs estimate: 227."
  },
  {
    "objectID": "posts/stochastic-mortality/index.html#stochastic-ensemble",
    "href": "posts/stochastic-mortality/index.html#stochastic-ensemble",
    "title": "Stochastic claims projections demo",
    "section": "Stochastic Ensemble",
    "text": "Stochastic Ensemble\nLoop through and calculate the reults n times (this is only running the two policies in the sample data” n times).\n\nfunction stochastic_proj(policies, params, n)\n\n    ThreadsX.map(1:n) do i\n        project(policies, params)\n    end\nend\n\nstoch = stochastic_proj(policies, params, 1000)\n\n1000-element Vector{@NamedTuple{benefits::Vector{Float64}, lives::Vector{Int64}}}:\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [383220.68177215644, 394717.3022253211, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 394717.3022253211, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n ⋮\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 394717.3022253211, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 418755.5859308432, 431318.2535087685, 444257.8011140316, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 1, 1, 1, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [383220.68177215644, 394717.3022253211, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 197358.65111266056, 203279.41064604037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 1, 1, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nv = [pv(0.03, s.benefits) for s in stoch]\nhist(v,\n    bins=15;\n    axis=(xlabel=\"Present Value\", ylabel=\"# scenarios\")\n)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/We6MY/src/scenes.jl:227"
  },
  {
    "objectID": "posts/stochastic-mortality/index.html#further-optimization",
    "href": "posts/stochastic-mortality/index.html#further-optimization",
    "title": "Stochastic claims projections demo",
    "section": "Further Optimization",
    "text": "Further Optimization\nIn no particular order:\n\nthe RNG could be made faster: https://bkamins.github.io/julialang/2020/11/20/rand.html\nCould make the stochastic set distributed, but at the current speed the overhead of distributed computing is probably more time than it would save. Same thing with GPU projections\n…"
  },
  {
    "objectID": "posts/academy-generator-rates/index.html",
    "href": "posts/academy-generator-rates/index.html",
    "title": "Interactive AAA Economic Scenario Generator",
    "section": "",
    "text": "Scroll down below the sample video to see how to run it.\nThe recording above shows a Pluto.jl notebook manipulating the long term mean reversion parameter for the Nelson-Siegel functional interest rate model, based on the American Academy of Actuaries’ (AAA) Economic Scenario Generator (ESG)."
  },
  {
    "objectID": "posts/academy-generator-rates/index.html#instructions-to-run",
    "href": "posts/academy-generator-rates/index.html#instructions-to-run",
    "title": "Interactive AAA Economic Scenario Generator",
    "section": "Instructions to Run",
    "text": "Instructions to Run\nBecause JuliaActuary doesn’t have a beefy server to run this on and let anybody run/visualize thousands of stochastic scenarios, for this one you have to run it locally. Assuming that you already have Julia installed but still need to install Pluto notebooks:\n\nOpen a Julia REPL and copy and paste the following:\n\n# install these dependencies\nimport Pkg; Pkg.add([\"Pluto\"]) \n\n# use and start Pluto\nusing Pluto; Pluto.run()                        \n\nIn the Pluto window that opens, enter this URL into the Open from file: box:\n\nhttps://raw.githubusercontent.com/JuliaActuary/Learn/master/AAA_ESG.jl"
  },
  {
    "objectID": "posts/academy-generator-rates/index.html#see-also",
    "href": "posts/academy-generator-rates/index.html#see-also",
    "title": "Interactive AAA Economic Scenario Generator",
    "section": "See also",
    "text": "See also\nReplicating the AAA equtity generator"
  },
  {
    "objectID": "posts/hacktoberfest/index.html",
    "href": "posts/hacktoberfest/index.html",
    "title": "JuliaActuary and Hacktoberfest 2022",
    "section": "",
    "text": "Hacktoberfest is DigitalOcean’s annual event that encourages people to contribute to open source throughout October.\n\nBy contributing to open source, you can get a t-shirt (or a tree planted!) upon submitting four contributions to open source projects. See the Hacktoberfest site for more details."
  },
  {
    "objectID": "posts/hacktoberfest/index.html#what-is-hacktoberfest",
    "href": "posts/hacktoberfest/index.html#what-is-hacktoberfest",
    "title": "JuliaActuary and Hacktoberfest 2022",
    "section": "",
    "text": "Hacktoberfest is DigitalOcean’s annual event that encourages people to contribute to open source throughout October.\n\nBy contributing to open source, you can get a t-shirt (or a tree planted!) upon submitting four contributions to open source projects. See the Hacktoberfest site for more details."
  },
  {
    "objectID": "posts/hacktoberfest/index.html#juliaactuary-participation",
    "href": "posts/hacktoberfest/index.html#juliaactuary-participation",
    "title": "JuliaActuary and Hacktoberfest 2022",
    "section": "JuliaActuary participation",
    "text": "JuliaActuary participation\nJuliaActuary has a number of “issues” with the hacktoberfest tag on Github across all the repositories which identify “easier” or introductory contribution ideas. See this project board for items with the hacktoberfest tag or browse individual packages’ issues page.\nIf you participate and submit at least two of your contributions to JuliaActuary, we will mail you a JuliaActuary sticker as thanks!\n\n\n\nJuliaActuaryStickers"
  },
  {
    "objectID": "posts/hacktoberfest/index.html#low-code-or-no-code-contributions",
    "href": "posts/hacktoberfest/index.html#low-code-or-no-code-contributions",
    "title": "JuliaActuary and Hacktoberfest 2022",
    "section": "Low-code or no-code contributions",
    "text": "Low-code or no-code contributions\nWe welcome contributions that have low or no-code in them. That could be as simple as new documentation, changing some website CSS, writing a blog post about your experience, etc. Hacktoberfest has a list of low/no-code suggestions.\nIf you have ideas but aren’t sure where to begin, start a discussion here: https://github.com/orgs/JuliaActuary/discussions."
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html",
    "href": "posts/bayes-vs-limited-fluctuation/index.html",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "",
    "text": "Alternate title: Why actuaries should cease to care about the number 1082."
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#introduction",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#introduction",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Introduction",
    "text": "Introduction\nThis notebook briefly discusses two approaches to experience analysis (rate setting). One traditional method, Limited Fluctuation Credibility (LFC), has been around for a very long time although was never intended to be the most accurate predictive method. However, many actuaries still discuss the notion of “full” or “partially” credible indicating a reliance on the LFC concepts.\nThe Bayesian approach uses explicit assumptions about the statistical relationships and all data given to the model to make inferences. Small datasets lead to greater uncertainty, while larger datasets never reach a point that could be considered “fully credible”, although the posterior density may narrow considerably.\nThis notebook argues that the Bayesian approach is superior to the LFC heuristics and should be adopted more widely in actuarial practice.\nThe example will use an auto-claims dataset and will use the first two years of experience to predict the third.\n\nusing CSV\nusing DataFramesMeta\nusing Distributions\nusing Turing\nusing MCMCChains\nusing DataFrames\nusing Logging; Logging.disable_logging(Logging.Warn);\nusing StatsFuns\nusing StatisticalRethinking\nusing CairoMakie"
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#limited-fluctuation-credibility",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#limited-fluctuation-credibility",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Limited Fluctuation Credibility",
    "text": "Limited Fluctuation Credibility\n\nThe Limited Fluctuation Method was so named because it allowed premiums to fluctuate from year to year based on experience, while limiting those fluctuations by giving less than full credibility to premiums based on limited data. In contrast, setting premium rates by giving full credibility to recent experience could be called the Full Fluctuation Method. While every credibility method serves to limit fluctuations, this method acquired its name because it was the first. The Limited Fluctuation Method, also known as Classical Credibility, is the most widely-used credibility method because it can be relatively simple to apply. Outside North America, this method is sometimes referred to as American Credibility.\n\nQuoted from Atkinson, 2019.\n\nFormulas\nThe Limited Fluctuation Method components: a credibility weight, \\[Z\\], an Observed Rate, and a Prior Rate.\n\\[\n\\text{Credibility-Weighted Rate} = Z × \\text{Observed Rate} + (1 – Z) × \\text{Prior Rate}\n\\]\nWith probability equal to \\(LC_p\\) that the Observed Rate does not differ from the true rate by more than \\(LC_r\\).\n\nLC_p = 0.9\nLC_r  = 0.05\n\n0.05\n\n\n\\[\n\\text{Claims for full credibility} = \\left(\\frac{\\text{Z-Score}}{\\text{ratio}}\\right)^{2}\n\\]\n\nLC_full_credibility = round(Int, (quantile(Normal(), 1 - (1 - LC_p) / 2) / LC_r)^2)\n\n1082\n\n\nUsing the inputs above, we have a z-score corresponding to 0.9 of 1.6448536269514717, so: ( 1.6448536269514717 ÷ 0.05)² ≈ 1082. ““”\nAtkinson goes on to nicely summarize the square root method which assigns full credibility, \\[Z = 1\\], when the number of actual claims equals or exceeds the full credibility threshold of $LC_full_credibility claims.\nWhen the number of claims is less than full credibility:\n\\[\nZ = \\sqrt{\\frac{\\text{no. claims in group}}{\\text{no. claims full credibility}}}\n\\]\n\n\nIssues with Limited Fluctuation\n\nIgnores available information:\n\nWhy does our \\(Z\\) not vary by exposures?\nNo mechanism for a related group to inform the credibility of another\nResults in a single point estimate with only approximate measure of estimate uncertainty\n\nRelies on a number of assumptions/constraints:\n\nThat aggregated claims can be approximated by a normal distribution\nThe thresholds of $LC_p and $LC_r are arbitrary (e.g. all of the same arguments against p-value thresholds can apply to this approach) ““”"
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#bayesian-approach",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#bayesian-approach",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Bayesian Approach",
    "text": "Bayesian Approach\nBayes’ formula of updating the posterior probability conditioned on the observed data:\n\\[\nP(A\\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}\n\\]\nThis is well known to most actuaries… but generally not applied frequently in practice!\nThe issue is that once the probability distributions and data become non-trivial, the formula becomes analytically intractable. Work over the last several decades has set the stage for addressing these situations by developing algorithms that let you sample the Bayesian posterior, even if you can’t analytically say what that is.\nA full overview is beyond the scope of this notebook, which is simply to demonstrate that Limited Fluctuation Credibility is of questionable use in practice and that superior tools exist. For references on modern Bayesian statistics, see the end notes.\nNote that this is describing a different, more first-principles approach than the the Buhlman Bayesian approach, which attempts to simply relate group experience to population experience. The Bayesian approach described here is much more general and extensible.\n\nThe formulation\nContrary to Limited Fluctuation, the Bayesian approach forces one to be explicit about the presumed structure of the probability model. The flexibility of the statistical model allows one to incorporate actuarial judgement in a quantitative way. For example, in this example we assume that the claims experience of each group informs a global (hyperparameter) prior distribution which we could use as a starting point for a new type of observation. More on this once the data is introduced."
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#sample-claims-prediction",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#sample-claims-prediction",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Sample Claims Prediction",
    "text": "Sample Claims Prediction\nThe data comes from an Allstate auto-claims data via Kaggle. It contains exposure level information about predictor variable and claim amounts for calendar years 2005-2007.\nFor simplicity, we will focus on the narrow objective of estimating the claims rate at the level of automobile make and model. We will use the years 2005 and 2006 as the training data set and then 2007 to evaluate the predictive model.\nThe original data is over 13 million rows, we will load an already summarized CSV and split it into train and test sets based on the Calendar_Year.\n\ntrain,test = let\n    pth = download(\"https://raw.githubusercontent.com/JuliaActuary/Learn/master/data/condensed_auto_claims.csv\")\n    df = CSV.read(pth,DataFrame,normalizenames=true)\n    df.p_observed = df.claims ./ df.n\n    \n\n    train = df[df.Calendar_Year .&lt; 2007,:]\n    test  = df[df.Calendar_Year .== 2007,:]\n\n    train, test\n    \nend\n\n\n(2413×5 DataFrame\n  Row │ Calendar_Year  Blind_Model  n      claims  p_observed \n      │ Int64          String7      Int64  Int64   Float64    \n──────┼───────────────────────────────────────────────────────\n    1 │          2005  K.78          6132      41  0.00668624\n    2 │          2005  Q.22         35141     270  0.00768333\n    3 │          2005  AR.41        18719     174  0.00929537\n    4 │          2006  AR.41        18868     196  0.010388\n    5 │          2005  D.20          4573      27  0.00590422\n    6 │          2006  D.20          5446      23  0.00422328\n    7 │          2006  AJ.129       50803     342  0.00673189\n    8 │          2006  AQ.17        44018     263  0.00597483\n    9 │          2005  AQ.17        42684     278  0.00651298\n   10 │          2005  BW.3         37903     215  0.00567237\n   11 │          2006  BW.3         37752     195  0.00516529\n  ⋮   │       ⋮             ⋮         ⋮      ⋮         ⋮\n 2404 │          2005  CA.5             1       0  0.0\n 2405 │          2005  J.5              1       0  0.0\n 2406 │          2006  J.5              1       0  0.0\n 2407 │          2005  CB.8             1       0  0.0\n 2408 │          2005  AQ.1             1       0  0.0\n 2409 │          2005  AJ.96            1       0  0.0\n 2410 │          2005  AJ.51            1       0  0.0\n 2411 │          2006  AJ.51            1       0  0.0\n 2412 │          2005  BQ.6             1       0  0.0\n 2413 │          2006  BQ.6             1       0  0.0\n                                             2392 rows omitted, 1289×5 DataFrame\n  Row │ Calendar_Year  Blind_Model  n       claims  p_observed \n      │ Int64          String7      Int64   Int64   Float64    \n──────┼────────────────────────────────────────────────────────\n    1 │          2007  X.45          99368     789  0.00794018\n    2 │          2007  Y.29          55783     435  0.00779807\n    3 │          2007  P.18           4923      46  0.0093439\n    4 │          2007  X.40           5573      22  0.0039476\n    5 │          2007  Y.42          25800     168  0.00651163\n    6 │          2007  AH.164        12564      88  0.00700414\n    7 │          2007  AH.119         4251      26  0.00611621\n    8 │          2007  W.3            7680      60  0.0078125\n    9 │          2007  BW.107        10897      53  0.00486372\n   10 │          2007  BW.79          6938      35  0.00504468\n   11 │          2007  AU.58         53424     508  0.00950883\n  ⋮   │       ⋮             ⋮         ⋮       ⋮         ⋮\n 1280 │          2007  BW.123            1       0  0.0\n 1281 │          2007  V.8               1       0  0.0\n 1282 │          2007  Z.18              1       0  0.0\n 1283 │          2007  X.14              2       0  0.0\n 1284 │          2007  AM.7              1       0  0.0\n 1285 │          2007  BU.37             1       0  0.0\n 1286 │          2007  AE.6              2       0  0.0\n 1287 │          2007  CB.6              1       0  0.0\n 1288 │          2007  BU.32             1       0  0.0\n 1289 │          2007  CA.5              1       0  0.0\n                                              1268 rows omitted)\n\n\n\n\nDiscussion of Bayesian model\nEach group (make and model combination) has an expected claim rate \\(μ_i\\), which is informed by the global hyperparameter \\(μ\\) and variance \\(\\sigma^2\\). Then, the observed claims by group are assumed to be distributed according to a Poisson distribution.\nA complete overview of modern Bayesian models is beyond the scope of this notebook, but a few key points:\n\nWe are forced with the Bayesian approach to be explicit about the assumptions (versus all of the implicit assumptions of alternative techniques like LF)\nWe set priors which are the assumed distribution of model parameters before “learning” from the observations. With enough data, it can result in a posterior that the prior was very skeptical of beforehad.\nWith the volume of data in the training set, the priors we select here are not that important. Some comments on why they were chosen though:\n\nThe rate of claims is linked with a logistic function, which looks like an integral sign of sorts. logistic(0.0) equals 0.5 while very negative inputs approach 0.0 and positive numbers approach 1.0. We do this so that the rate of claims is always constrained in the range \\((0,1)\\)\nμ \\sim Normal(-2,4) says that we expect the population of auto claims rate to be less than 0.5 (`logistic(-2) ≈ .12) but very wide range of possible values given the wide standard deviation.\nσ \\sim Exponential(0.25) says that we expect the standard devation of an individual group to be positive, but not super wide.\nμ_i \\sim Normal(μ,σ) is the actual prior for each group’s rate of claim and is informed by the above hyperparameters.\n\n@model is a Turing.jl macro which enables interpreting the nice ~ syntax, which makes the Julia code look very similar to the traditional mathematical notation.\nNote the use of broadcasting with the dot syntax (e.g. the . in .~). This tells Julia to vectorize and fuse the computation.\n\ndata.claims .~ Poisson.(data.n .* logistic.(μ_i)) means “each value in data.claims is a random outcome (.~) distributed accroding to a corresponding Poisson distribution with \\lambda =n \\times \\text{logistic}(\\mu_i) where text{logistic}(\\mu_i) is the average claims rate for each group.\n\n\n\n@model function model_poisson(data)\n    # hyperparameter that informs the prior for each group\n    μ ~ Normal(-2,4) \n    σ ~ Exponential(0.25)\n\n    # the random variable representing the average claim rate for each group\n    # filldist creates a set of random variable without needing to list out each one\n    μ_i ~ filldist(Normal(μ,σ),length(unique(data.Blind_Model)))\n\n    # use the poisson appproximation to the binomial claim outcome with a \n    # logisitc link function to keep the probability between 0 and 1\n    data.claims .~ Poisson.(data.n .* logistic.(μ_i))\nend\n\nmodel_poisson (generic function with 2 methods)\n\n\nHere’s what the prior distribution of claims looks like… and peeking ahead to what the posterior for a single group of claims looks like. See how even though our posterior was very wide (favoring small claims rates), it was dominated by the data to create a very narrow posterior average claims rate.\nThe model is combined with the data and Turing.jl is used to computationally arrive at the posterior distribution for the parameters in the statistical model, \\(\\mu\\), \\(\\mu_i\\), and \\(\\sigma\\).\n\nmp = let\n    # combine the different years in the training set \n    condensed = @chain train begin\n        groupby(:Blind_Model)\n        @combine begin\n            :n = sum(:n)\n            :claims = sum(:claims)\n        end\n    end\n    model_poisson(condensed);\nend\n\n\nDynamicPPL.Model{typeof(model_poisson), (:data,), (), (), Tuple{DataFrame}, Tuple{}, DynamicPPL.DefaultContext}(model_poisson, (data = 1238×3 DataFrame\n  Row │ Blind_Model  n       claims \n      │ String7      Int64   Int64  \n──────┼─────────────────────────────\n    1 │ K.78          14058      94\n    2 │ Q.22          71401     532\n    3 │ AR.41         37587     370\n    4 │ D.20          10019      50\n    5 │ AJ.129       100824     693\n    6 │ AQ.17         86702     541\n    7 │ BW.3          75655     410\n    8 │ BW.167        42737     294\n    9 │ Y.9           92206     755\n   10 │ BH.29         15795     162\n   11 │ BW.49         27811     156\n  ⋮   │      ⋮         ⋮       ⋮\n 1229 │ AM.7              2       0\n 1230 │ AE.6              3       0\n 1231 │ BU.32             2       0\n 1232 │ BG.8              1       0\n 1233 │ CA.5              2       0\n 1234 │ J.5               2       0\n 1235 │ AQ.1              1       0\n 1236 │ AJ.96             1       0\n 1237 │ AJ.51             2       0\n 1238 │ BQ.6              2       0\n                   1217 rows omitted,), NamedTuple(), DynamicPPL.DefaultContext())\n\n\n\n\n\nBayesian Posterior Sampling\nHere’s where recent advances in algorithms and computing power make Bayesian analyis possible. We can’t analytically compute the posterior distribution, but we can genererate samples from the posterior such that the frequency of the sampled result appears in proportion to the true posterior density. This uses a technique called Markov Chain Monte Carlo, abbreviated MCMC. There are different algorithms in this family, and we will use one called the No-U-Turn Sampler (NUTS for short).\nThe result of the sample function is a set of data containing data that is generated in proportion to the posterior distributions and we will use this data to make predictions and understand the distribution of our parameters.\nRun the chain:\n\ncp = sample(mp, NUTS(), 500) # this is the line that runs the MCMC sampler\n\n# ╔═╡ 6b5d2186-78ac-48ae-8529-e0dcd51d0b00\nlet \n    # sample from the priors before learning from any data\n    ch_prior = sample(mp,Prior(),1500)\n    \n    f = Figure()\n    ax = Axis(f[1,1],title=\"Prior distribution of expected claims rate for a group\")\n    μ = vec(ch_prior[\"μ_i[1]\"].data)\n    hist!(ax,logistic.(μ);bins=50)\n    ax2 = Axis(f[2,1],title=\"Posterior distribution of expected claims rate for a group\")\n    μ = vec(cp[\"μ_i[1]\"].data)\n    hist!(ax2,logistic.(μ);bins=50)\n    linkxaxes!(ax,ax2)\n    f\nend\n\n\n\n\n\n\n\n\n\n\nVisualizing the Posterior Density\nThis is a plot of the posterior density for all of the many, many \\(\\mu\\) parameters in our model. The black line shows the distribution which comes from our hyperparameters μ and σ. In the event of coming across a new group of interest (in our case, a new make of car), this is the prior distribution for the expected claims rate. The model has learned this from the data itself, and serves to regularize predictions.\n\n\nUtilitiy Function to Plot the Posterior\n\"\"\"\n    dplot(chain,parameter_string)\n\nplot the posterior density for model parameters matching the given string.\n\"\"\"\nfunction dplot(ch,param_string)\n    f = Figure()\n    ax = Axis(f[1, 1])\n\n    # plot each group posterior\n   for (ind, param) in enumerate(ch.name_map.parameters)\n        if contains(string(param),param_string)\n            v = vec(getindex(ch, param).data)\n            density!(ax,logistic.(v), color = (:red, 0.0),\n    strokecolor = (:red,0.3), strokewidth = 1, strokearound = false, label=\"Individual group posterior\")\n        end\n   end\n    \n    # Plot hyperparameters\n    hyper_mean = mean(getindex(ch, :μ).data)\n    hyper_sigma = mean(getindex(ch, :σ).data)\n    d = density!(ax,logistic.(rand(Normal(hyper_mean,hyper_sigma),1000)),color = (:black, 0.),\n    strokecolor = (:black,0.3), strokewidth = 3, strokearound = false,label=\"Hyper-parameter distribution\")\n\n    \n    xlims!(0.0,0.03)\n    hideydecorations!(ax)\n    axislegend(ax,unique=true)\n    f\nend\n\ndplot(cp, \"μ\")"
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#predictions-and-results",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#predictions-and-results",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Predictions and Results",
    "text": "Predictions and Results\nHere we compare four predictive models:\n\nNaive, where the predicted rate for the 2007 year is the average of each groups’ for 2005-2006\nLimited Fluctuation Overall, where the “prior” in the LFC formula is the overall mean claim rate for 2005-2006\nLimited Fluctuation Year-by-Year where the “prior” in the LFC formula is the claim rate for the ith group in 2005 and updated using 2006 claim rates.\nBayesian approach where the predicted claim rate is based on the bayesian model discussed above.\n\n\nPredictions\n\n\nBayesian approach has lower error\nLooking at the accumulated absolute error, the bayesian approach has about 16% lower error than the Limited Fluctuation approaches.\n\n\nTotal Actual to Expected\nHere, offsetting errors happen to make the naive and LF year-by-year (the latter is a good proxy for the former) such that their total A/E is better than the Bayesian approach.\nGiven the lower error of the Bayesian approach, one would expect that over multiple years that it would produce more accurate predictions than the alternative methods."
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#conclusion",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#conclusion",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis notebook shows that the Bayesian approach results in claims predictions with less total predictive error than the limited fluctuation method.\n\nFurther thoughts\nThe Bayesian approach could be extended to improve its accuracy even further:\n\nUsing vehicle make data to add more hierarchical structure to the statistical model. For example, one may observe that Porsches experience crashes at a higher rate than Volvos. LFC cannot embed that sort of overlapping hierarchy into its framework.\nThe Bayesian hyperparameter provides a framework to think about “unseen” make-model combinations\n\n\n\nDownsides to the Bayesian approach\n\nComputationally intensive. Complex @models can take very long time to run (many hours), compared to relatively quick frequentest methods like maximum likelihood estimation.\n\n\n\nFurther Reading\nIf this notebook has piqued your interest in Bayesian techniques, the following books are recommended learning resources (from easiest to most difficult):\n\nStatistical Rethinking\n\nSee also StatisticalRethinking.jl for Julia implementations of all of the book’s code and utility functions\n\nBayes Rules!\nBayeisan Data Analysis"
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#appendices",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#appendices",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Appendices",
    "text": "Appendices\nAdditional legwork to get the alternate limited fluctuation approach data:\n\n# An alternate approach to LFC where the first year becomes the prior, adjusted by data from the seonc year.\nLF2 = let \n    # split dataset by year and recombine\n    df2005 = @subset(train,:Calendar_Year .== 2005)\n    df2006 = @subset(train,:Calendar_Year .== 2006)\n    df = outerjoin(df2005,df2006,on=:Blind_Model,renamecols= \"_2005\" =&gt; \"_2006\")\n\n    # use 2005 actuals as LFC prior, and the overall mean if model is missing\n    μ_2005 = sum(skipmissing(df.claims_2005)) / sum(skipmissing(df.n_2005))\n    df.assumed = coalesce.(df.p_observed_2005,μ_2005)\n    \n    df.LF2_Z = min.(1, .√(coalesce.(df.claims_2006,0.) ./ LC_full_credibility))\n    df.LF2_μ = let Z = df.LF2_Z\n        # use the 2005 mean if the model not observed in 2006\n        Z .* coalesce.(df.p_observed_2006,μ_2005) .+ (1 .- Z) .* df.assumed\n    end\n    df\nend\n\nclaims_summary_posterior = let\n    # combine the dataset by model\n    df = @chain train begin\n        groupby(:Blind_Model)\n        @combine begin\n            :n = sum(:n)\n            :claims = sum(:claims)\n        end\n    end\n    pop_μ = sum(df.claims) / sum(df.n)\n    df[!,:pop_μ] .= pop_μ\n\n    ## Bayesian prediction\n    # get the mean of the posterior estimate for the ith model group\n    means = map(1:length(unique(df.Blind_Model))) do i\n        d = logistic.(getindex(cp, Symbol(\"μ_i[$i]\")).data)\n        (est = mean(d), se=std(d))\n    end\n\n    df.p_observed = df.claims ./ df.n\n    df.bayes_μ = [x.est for x in means]\n    df.bayes_se = [x.se for x in means]\n\n    ## Limited Fluctuation (square root rule)\n    # using overall population mean\n    # using the square-root rule\n    df.LF_Z = min.(1, .√(df.claims ./ LC_full_credibility))\n    df.LF_μ = let Z = df.LF_Z\n        Z .* (df.p_observed) .+ (1 .- Z) .* pop_μ\n    end\n\n    ## Limited Fluctuation \n    # using the first year as the prior, 2nd year as new data\n    # using some additional procesing to get the LF2 dataframe, see appendix\n    df2005 = @subset(train, :Calendar_Year .== 2005)\n    dict2005 = Dict(\n        model =&gt; rate \n        for (model, rate) in zip(df2005.Blind_Model,df2005.p_observed)\n    )\n    \n    df = leftjoin(df,LF2[:,[:Blind_Model,:LF2_μ]],on=:Blind_Model,)\n    \n    df = innerjoin(df,test;on=:Blind_Model,renamecols= \"_train\" =&gt; \"_test\")\n\n    # predictions on the test set using the predictive rates time exposures\n    df.pred_bayes = df.n_test .* df.bayes_μ_train\n    df.pred_LF = df.n_test .* df.LF_μ_train\n    df.pred_LF2 = df.n_test .* df.LF2_μ_train\n    df.pred_naive = df.n_test .* df.p_observed_train\n\n    sort!(df,:n_train,rev=true)\nend\n\n1224×18 DataFrame1199 rows omitted\n\n\n\nRow\nBlind_Model\nn_train\nclaims_train\npop_μ_train\np_observed_train\nbayes_μ_train\nbayes_se_train\nLF_Z_train\nLF_μ_train\nLF2_μ_train\nCalendar_Year_test\nn_test\nclaims_test\np_observed_test\npred_bayes\npred_LF\npred_LF2\npred_naive\n\n\n\nString7\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64?\nInt64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nK.7\n382210\n3365\n0.0072979\n0.00880406\n0.00878238\n0.000168854\n1.0\n0.00880406\n0.00853456\n2007\n215223\n1798\n0.00835413\n1890.17\n1894.84\n1836.83\n1894.84\n\n\n2\nX.45\n192591\n1542\n0.0072979\n0.0080066\n0.00800053\n0.000199085\n1.0\n0.0080066\n0.00801192\n2007\n99368\n789\n0.00794018\n794.997\n795.6\n796.129\n795.6\n\n\n3\nAU.14\n188702\n1687\n0.0072979\n0.00894002\n0.0089118\n0.00021057\n1.0\n0.00894002\n0.00874644\n2007\n114742\n1071\n0.00933398\n1022.56\n1025.8\n1003.58\n1025.8\n\n\n4\nW.16\n150304\n1064\n0.0072979\n0.00707899\n0.0070739\n0.000218869\n0.991647\n0.00708082\n0.00691906\n2007\n83039\n510\n0.00614169\n587.41\n587.984\n574.552\n587.832\n\n\n5\nAU.11\n133445\n1188\n0.0072979\n0.00890254\n0.0088575\n0.000264302\n1.0\n0.00890254\n0.00878751\n2007\n70028\n642\n0.00916776\n620.273\n623.427\n615.372\n623.427\n\n\n6\nBO.38\n125206\n775\n0.0072979\n0.0061898\n0.00619077\n0.000223624\n0.846325\n0.00636009\n0.0061673\n2007\n68966\n437\n0.00633646\n426.952\n438.63\n425.334\n426.886\n\n\n7\nAO.7\n109746\n1055\n0.0072979\n0.00961311\n0.00956512\n0.000305165\n0.987444\n0.00958404\n0.00956317\n2007\n59534\n543\n0.00912084\n569.45\n570.576\n569.334\n572.307\n\n\n8\nAJ.58\n107538\n891\n0.0072979\n0.00828544\n0.00826714\n0.000252584\n0.907455\n0.00819405\n0.00824136\n2007\n52176\n394\n0.00755136\n431.346\n427.533\n430.001\n432.301\n\n\n9\nAJ.52\n107252\n691\n0.0072979\n0.00644277\n0.00643023\n0.000263138\n0.799145\n0.00661453\n0.00644592\n2007\n51952\n292\n0.00562057\n334.063\n343.638\n334.879\n334.715\n\n\n10\nAJ.129\n100824\n693\n0.0072979\n0.00687336\n0.00684782\n0.000263334\n0.8003\n0.00695814\n0.00685673\n2007\n48920\n323\n0.00660262\n334.995\n340.392\n335.431\n336.245\n\n\n11\nAU.58\n98015\n970\n0.0072979\n0.00989644\n0.00980722\n0.000301882\n0.94683\n0.00975828\n0.00989406\n2007\n53424\n508\n0.00950883\n523.941\n521.326\n528.58\n528.708\n\n\n12\nX.38\n97642\n837\n0.0072979\n0.00857213\n0.00851555\n0.000300658\n0.879527\n0.00841862\n0.00854678\n2007\n49682\n396\n0.00797069\n423.069\n418.254\n424.621\n425.881\n\n\n13\nY.34\n92959\n734\n0.0072979\n0.00789595\n0.00785963\n0.000281347\n0.823634\n0.00779048\n0.00785037\n2007\n53780\n443\n0.00823726\n422.691\n418.972\n422.193\n424.644\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n1213\nBU.32\n2\n0\n0.0072979\n0.0\n0.00630026\n0.00164119\n0.0\n0.0072979\n0.0\n2007\n1\n0\n0.0\n0.00630026\n0.0072979\n0.0\n0.0\n\n\n1214\nCA.5\n2\n0\n0.0072979\n0.0\n0.00636515\n0.00160636\n0.0\n0.0072979\n0.0\n2007\n1\n0\n0.0\n0.00636515\n0.0072979\n0.0\n0.0\n\n\n1215\nD.18\n1\n0\n0.0072979\n0.0\n0.00630744\n0.0015326\n0.0\n0.0072979\n0.00748894\n2007\n512\n2\n0.00390625\n3.22941\n3.73652\n3.83434\n0.0\n\n\n1216\nQ.7\n1\n0\n0.0072979\n0.0\n0.0063304\n0.00166114\n0.0\n0.0072979\n0.00748894\n2007\n3\n0\n0.0\n0.0189912\n0.0218937\n0.0224668\n0.0\n\n\n1217\nBW.139\n1\n0\n0.0072979\n0.0\n0.00648606\n0.00170544\n0.0\n0.0072979\n0.00748894\n2007\n1\n0\n0.0\n0.00648606\n0.0072979\n0.00748894\n0.0\n\n\n1218\nBG.8\n1\n0\n0.0072979\n0.0\n0.00645148\n0.00169365\n0.0\n0.0072979\n0.00748894\n2007\n28\n0\n0.0\n0.180642\n0.204341\n0.20969\n0.0\n\n\n1219\nR.35\n1\n0\n0.0072979\n0.0\n0.00639538\n0.00160778\n0.0\n0.0072979\n0.00748894\n2007\n2\n0\n0.0\n0.0127908\n0.0145958\n0.0149779\n0.0\n\n\n1220\nI.6\n1\n0\n0.0072979\n0.0\n0.00648387\n0.00182023\n0.0\n0.0072979\n0.00748894\n2007\n1\n0\n0.0\n0.00648387\n0.0072979\n0.00748894\n0.0\n\n\n1221\nBM.8\n1\n0\n0.0072979\n0.0\n0.00642479\n0.00180695\n0.0\n0.0072979\n0.00748894\n2007\n1\n0\n0.0\n0.00642479\n0.0072979\n0.00748894\n0.0\n\n\n1222\nAJ.96\n1\n0\n0.0072979\n0.0\n0.00635007\n0.00161312\n0.0\n0.0072979\n0.0\n2007\n1\n0\n0.0\n0.00635007\n0.0072979\n0.0\n0.0\n\n\n1223\nAQ.1\n1\n0\n0.0072979\n0.0\n0.0063907\n0.00171822\n0.0\n0.0072979\n0.0\n2007\n1\n0\n0.0\n0.0063907\n0.0072979\n0.0\n0.0\n\n\n1224\nAJ.118\n1\n0\n0.0072979\n0.0\n0.00646154\n0.00171032\n0.0\n0.0072979\n0.0\n2007\n1\n0\n0.0\n0.00646154\n0.0072979\n0.0\n0.0\n\n\n\n\n\n\n\nlet df = claims_summary_posterior\n    f = Figure()\n    ax = Axis(f[1,1],title=\"Cumulative Predictive Residual Error\", subtitle=\"(Lower is better predictive accuracy)\",xlabel=L\"$i^{th}$ vehicle group\",ylabel=L\"absolute error\")\n    lines!(ax,cumsum(abs.(df.pred_LF .- df.claims_test)),label=\"Limited Fluctuation Overall\",color=(:purple,0.5))\n    lines!(ax,cumsum(abs.(df.pred_LF2 .- df.claims_test)),label=\"Limited Fluctuation Year-by-Year\",color=(:blue,0.5))\n    lines!(ax,cumsum(abs.(df.pred_bayes .- df.claims_test)),label=\"Bayesian\",color=(:red,0.5))\n    lines!(ax,cumsum(abs.(df.pred_naive .- df.claims_test)),label=\"Naive\",color=(:grey10,0.5))\n    # xlims!(0,40)\n    # Legend(f[1,1],[s1,s2],[\"LFC\"halign=:right,valign=:top)\n    axislegend(ax,position=:rb)\n    f\nend\n\n\n\n\n\n\n\n\n\nlet \n    post = claims_summary_posterior\n    X = @chain vcat(train,test) begin\n        groupby(:Calendar_Year)\n        @combine :claim_rate = sum(:claims) / sum(:n)\n    end\n    f = Figure()\n    ax = Axis(f[1,1],xlabel=\"year\",ylabel=\"claims rate\")\n    scatter!(ax,X.Calendar_Year,X.claim_rate, label=\"data\")\n    scatter!(ax,[2007],[sum(post.pred_bayes)/sum(post.n_test)], label=\"Bayes\", marker=:hline,markersize=30)\n    scatter!(ax,[2007],[sum(post.pred_LF)/sum(post.n_test)], label=\"LF Overall\",marker=:hline,markersize=30)\n    scatter!(ax,[2007],[sum(post.pred_LF2)/sum(post.n_test)], label=\"LF Year-by-Year\",marker=:hline,markersize=30)\n    scatter!(ax,[2007],[sum(post.pred_naive)/sum(post.n_test)], label=\"naive\",marker=:hline,markersize=30)\n    axislegend(ax,position=:rb)\n    ylims!(0.005,0.008)\n    f\nend"
  },
  {
    "objectID": "posts/bayes-vs-limited-fluctuation/index.html#some-remarks-about-the-results",
    "href": "posts/bayes-vs-limited-fluctuation/index.html#some-remarks-about-the-results",
    "title": "Bayesian vs Limited Fluctuation Experience Analysis",
    "section": "Some Remarks about the Results",
    "text": "Some Remarks about the Results\nWith limited, real-world data drawing conclusions is a little bit messy because we don’t know what the ground-truth should be, but here are some thoughts that seem to be consistent with what the data and results suggest:\n\nThe Bayesian approach partially pools the data: it’s an approach in-between assuming each group is independent from the rest and assuming that a single rate applies to all exposures.\nThe partial pooling limits over-fitting, which happens with the naive approach. Our Bayesian approach is somewhat skeptical of groups with low observation counts that stand out from the rest. But it also doesn’t forgoe useful data, as it learns from even low exposures according to what’s consistent with probability theory (Baye’s rule).\nThe naive approach is pretty close to the textbook definition of over-fitting. The LF approaches appear to be under-fitting group-level as there are only $(sum(claims_summary_posterior.LF_Z_train .&gt;= .9999)) groups with “full credibility” (Z=1), even though there are groups with less than “full credibility” with over 100,000 observations in the training set."
  },
  {
    "objectID": "posts/mortalitytables-and-dataframes/index.html",
    "href": "posts/mortalitytables-and-dataframes/index.html",
    "title": "Using MortaltiyTables.jl with DataFrames",
    "section": "",
    "text": "MortalityTables.jl stores the rates in a very efficient manner as a collection of vectors indexed by attained age.\n\nusing MortalityTables\n\nname = \"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\"\nvbt = MortalityTables.table(name)\n\n\n1\n\nOr any other mort.soa.org table\n\n\n\n\nMortalityTable (Insured Lives Mortality):\n   Name:\n       2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\n   Fields: \n       (:select, :ultimate, :metadata)\n   Provider:\n       Society of Actuaries\n   mort.SOA.org ID:\n       1118\n   mort.SOA.org link:\n       https://mort.soa.org/ViewTable.aspx?&TableIdentity=1118\n   Description:\n       2001 Valuation Basic Table (VBT) Residual Standard Select and Ultimate Table -  Male Nonsmoker. Basis: Age Nearest Birthday. Minimum Select Age: 0. Maximum Select Age: 99. Minimum Ultimate Age: 25. Maximum Ultimate Age: 120\n\n\nFirst, we include the package, and then we’ll pick a table, where all of the mort.soa.org tables are mirrored into your MortalityTables.jl installation.\nTo see how the data is represented, we can look at the the select data for a 55 year old and see the attained age and mortality rates:\n\nvbt.select[55]\n\n66-element OffsetArray(::Vector{Float64}, 55:120) with eltype Float64 with indices 55:120:\n 0.00139\n 0.00218\n 0.00288\n 0.00344\n 0.00403\n 0.00485\n 0.00599\n 0.00736\n 0.00879\n 0.01059\n 0.01211\n 0.014\n 0.01584\n ⋮\n 0.53905\n 0.57031\n 0.60339\n 0.63838\n 0.67541\n 0.71458\n 0.75603\n 0.79988\n 0.84627\n 0.89536\n 0.94729\n 1.0\n\n\nThis is very efficient and convienent for modeling, but a lot of times you want the data matched up with policy data in a DataFrame.\n\n\n\n\n\n\nusing DataFrames\n\nsample_size = 10_000\n\nsample_data = let\n    # generate fake data\n    df = DataFrame(\n        \"sex\" =&gt; rand([\"Male\",\"Female\"],sample_size),\n        \"smoke\" =&gt; rand([\"Smoker\",\"Nonsmoker\"],sample_size),\n        \"issue_age\" =&gt; rand(25:65,sample_size),\n        )\n    \n    # a random offset of issue age is the current attained age\n    df.attained_age = df.issue_age .+ rand(1:10,sample_size)\n    df\nend\n\n10000×4 DataFrame9975 rows omitted\n\n\n\nRow\nsex\nsmoke\nissue_age\nattained_age\n\n\n\nString\nString\nInt64\nInt64\n\n\n\n\n1\nFemale\nNonsmoker\n65\n71\n\n\n2\nMale\nNonsmoker\n33\n35\n\n\n3\nMale\nNonsmoker\n44\n45\n\n\n4\nMale\nSmoker\n36\n37\n\n\n5\nFemale\nSmoker\n57\n64\n\n\n6\nFemale\nSmoker\n40\n47\n\n\n7\nFemale\nNonsmoker\n31\n40\n\n\n8\nFemale\nNonsmoker\n44\n45\n\n\n9\nMale\nSmoker\n40\n49\n\n\n10\nMale\nNonsmoker\n47\n56\n\n\n11\nMale\nSmoker\n48\n56\n\n\n12\nMale\nNonsmoker\n54\n58\n\n\n13\nFemale\nNonsmoker\n25\n34\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9989\nMale\nSmoker\n36\n45\n\n\n9990\nMale\nSmoker\n39\n44\n\n\n9991\nMale\nNonsmoker\n49\n50\n\n\n9992\nMale\nSmoker\n45\n55\n\n\n9993\nMale\nSmoker\n65\n70\n\n\n9994\nMale\nSmoker\n60\n68\n\n\n9995\nMale\nNonsmoker\n27\n36\n\n\n9996\nMale\nNonsmoker\n52\n60\n\n\n9997\nFemale\nNonsmoker\n56\n65\n\n\n9998\nFemale\nNonsmoker\n40\n47\n\n\n9999\nFemale\nNonsmoker\n34\n39\n\n\n10000\nFemale\nNonsmoker\n63\n71\n\n\n\n\n\n\n\n\n\nThere are a lot of different possible combinations of parameters that you might want to use, such as rates that vary by sex, risk class, table set (VBT/CSO/etc), smoking status, relative risk, ALB/ANB, etc.\nIt’s easy to define the parameters applicable to your assumption set. Here, we’ll use a dictionary to define the relationship:\n\nrate_map = Dict(\n    \"Male\" =&gt; Dict(\n        \"Smoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Smoker, ANB\"),\n        \"Nonsmoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\"),\n        ),\n    \n    \"Female\" =&gt; Dict(\n        \"Smoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Female Smoker, ANB\"),\n        \"Nonsmoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Female Nonsmoker, ANB\"),\n        )\n    );\n\nAnd then we’ll define a function to look up the relevant rate. Note how the function matches the levels we defined for the assumption set dictionary above.\n\nfunction rate_lookup(assumption_map,sex,smoke,issue_age,attained_age)\n    # pick the relevant table\n    table = assumption_map[sex][smoke]\n    \n    # check if the select rate exists, otherwise look to the ulitmate table\n    if issue_age in eachindex(table.select)\n        table.select[issue_age][attained_age]\n    else\n        table.ultimate[attained_age]\n    end\nend\n\nrate_lookup (generic function with 1 method)\n\n\n\n\n\nBy mapping each row’s data to the lookup function, we get a vector of rates for our data:\n\nrates = map(eachrow(sample_data)) do row\n    rate_lookup(rate_map, row.sex, row.smoke, row.issue_age, row.attained_age)\nend\n\n10000-element Vector{Float64}:\n 0.00971\n 0.00056\n 0.00091\n 0.001\n 0.01237\n 0.00292\n 0.00081\n 0.00066\n 0.00527\n 0.00483\n 0.00977\n 0.00373\n 0.00064\n ⋮\n 0.00362\n 0.00263\n 0.00127\n 0.00981\n 0.02741\n 0.02602\n 0.00087\n 0.00615\n 0.00744\n 0.00145\n 0.00059\n 0.01198\n\n\nAnd finally, we can just add this to the dataframe:\n\nsample_data.expectation = rates\n\nsample_data\n\n10000×5 DataFrame9975 rows omitted\n\n\n\nRow\nsex\nsmoke\nissue_age\nattained_age\nexpectation\n\n\n\nString\nString\nInt64\nInt64\nFloat64\n\n\n\n\n1\nFemale\nNonsmoker\n65\n71\n0.00971\n\n\n2\nMale\nNonsmoker\n33\n35\n0.00056\n\n\n3\nMale\nNonsmoker\n44\n45\n0.00091\n\n\n4\nMale\nSmoker\n36\n37\n0.001\n\n\n5\nFemale\nSmoker\n57\n64\n0.01237\n\n\n6\nFemale\nSmoker\n40\n47\n0.00292\n\n\n7\nFemale\nNonsmoker\n31\n40\n0.00081\n\n\n8\nFemale\nNonsmoker\n44\n45\n0.00066\n\n\n9\nMale\nSmoker\n40\n49\n0.00527\n\n\n10\nMale\nNonsmoker\n47\n56\n0.00483\n\n\n11\nMale\nSmoker\n48\n56\n0.00977\n\n\n12\nMale\nNonsmoker\n54\n58\n0.00373\n\n\n13\nFemale\nNonsmoker\n25\n34\n0.00064\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9989\nMale\nSmoker\n36\n45\n0.00362\n\n\n9990\nMale\nSmoker\n39\n44\n0.00263\n\n\n9991\nMale\nNonsmoker\n49\n50\n0.00127\n\n\n9992\nMale\nSmoker\n45\n55\n0.00981\n\n\n9993\nMale\nSmoker\n65\n70\n0.02741\n\n\n9994\nMale\nSmoker\n60\n68\n0.02602\n\n\n9995\nMale\nNonsmoker\n27\n36\n0.00087\n\n\n9996\nMale\nNonsmoker\n52\n60\n0.00615\n\n\n9997\nFemale\nNonsmoker\n56\n65\n0.00744\n\n\n9998\nFemale\nNonsmoker\n40\n47\n0.00145\n\n\n9999\nFemale\nNonsmoker\n34\n39\n0.00059\n\n\n10000\nFemale\nNonsmoker\n63\n71\n0.01198"
  },
  {
    "objectID": "posts/mortalitytables-and-dataframes/index.html#mortalitytables.jl-standard-representation",
    "href": "posts/mortalitytables-and-dataframes/index.html#mortalitytables.jl-standard-representation",
    "title": "Using MortaltiyTables.jl with DataFrames",
    "section": "",
    "text": "MortalityTables.jl stores the rates in a very efficient manner as a collection of vectors indexed by attained age.\n\nusing MortalityTables\n\nname = \"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\"\nvbt = MortalityTables.table(name)\n\n\n1\n\nOr any other mort.soa.org table\n\n\n\n\nMortalityTable (Insured Lives Mortality):\n   Name:\n       2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\n   Fields: \n       (:select, :ultimate, :metadata)\n   Provider:\n       Society of Actuaries\n   mort.SOA.org ID:\n       1118\n   mort.SOA.org link:\n       https://mort.soa.org/ViewTable.aspx?&TableIdentity=1118\n   Description:\n       2001 Valuation Basic Table (VBT) Residual Standard Select and Ultimate Table -  Male Nonsmoker. Basis: Age Nearest Birthday. Minimum Select Age: 0. Maximum Select Age: 99. Minimum Ultimate Age: 25. Maximum Ultimate Age: 120\n\n\nFirst, we include the package, and then we’ll pick a table, where all of the mort.soa.org tables are mirrored into your MortalityTables.jl installation.\nTo see how the data is represented, we can look at the the select data for a 55 year old and see the attained age and mortality rates:\n\nvbt.select[55]\n\n66-element OffsetArray(::Vector{Float64}, 55:120) with eltype Float64 with indices 55:120:\n 0.00139\n 0.00218\n 0.00288\n 0.00344\n 0.00403\n 0.00485\n 0.00599\n 0.00736\n 0.00879\n 0.01059\n 0.01211\n 0.014\n 0.01584\n ⋮\n 0.53905\n 0.57031\n 0.60339\n 0.63838\n 0.67541\n 0.71458\n 0.75603\n 0.79988\n 0.84627\n 0.89536\n 0.94729\n 1.0\n\n\nThis is very efficient and convienent for modeling, but a lot of times you want the data matched up with policy data in a DataFrame."
  },
  {
    "objectID": "posts/mortalitytables-and-dataframes/index.html#getting-data-into-a-dataframe",
    "href": "posts/mortalitytables-and-dataframes/index.html#getting-data-into-a-dataframe",
    "title": "Using MortaltiyTables.jl with DataFrames",
    "section": "",
    "text": "using DataFrames\n\nsample_size = 10_000\n\nsample_data = let\n    # generate fake data\n    df = DataFrame(\n        \"sex\" =&gt; rand([\"Male\",\"Female\"],sample_size),\n        \"smoke\" =&gt; rand([\"Smoker\",\"Nonsmoker\"],sample_size),\n        \"issue_age\" =&gt; rand(25:65,sample_size),\n        )\n    \n    # a random offset of issue age is the current attained age\n    df.attained_age = df.issue_age .+ rand(1:10,sample_size)\n    df\nend\n\n10000×4 DataFrame9975 rows omitted\n\n\n\nRow\nsex\nsmoke\nissue_age\nattained_age\n\n\n\nString\nString\nInt64\nInt64\n\n\n\n\n1\nFemale\nNonsmoker\n65\n71\n\n\n2\nMale\nNonsmoker\n33\n35\n\n\n3\nMale\nNonsmoker\n44\n45\n\n\n4\nMale\nSmoker\n36\n37\n\n\n5\nFemale\nSmoker\n57\n64\n\n\n6\nFemale\nSmoker\n40\n47\n\n\n7\nFemale\nNonsmoker\n31\n40\n\n\n8\nFemale\nNonsmoker\n44\n45\n\n\n9\nMale\nSmoker\n40\n49\n\n\n10\nMale\nNonsmoker\n47\n56\n\n\n11\nMale\nSmoker\n48\n56\n\n\n12\nMale\nNonsmoker\n54\n58\n\n\n13\nFemale\nNonsmoker\n25\n34\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9989\nMale\nSmoker\n36\n45\n\n\n9990\nMale\nSmoker\n39\n44\n\n\n9991\nMale\nNonsmoker\n49\n50\n\n\n9992\nMale\nSmoker\n45\n55\n\n\n9993\nMale\nSmoker\n65\n70\n\n\n9994\nMale\nSmoker\n60\n68\n\n\n9995\nMale\nNonsmoker\n27\n36\n\n\n9996\nMale\nNonsmoker\n52\n60\n\n\n9997\nFemale\nNonsmoker\n56\n65\n\n\n9998\nFemale\nNonsmoker\n40\n47\n\n\n9999\nFemale\nNonsmoker\n34\n39\n\n\n10000\nFemale\nNonsmoker\n63\n71\n\n\n\n\n\n\n\n\n\nThere are a lot of different possible combinations of parameters that you might want to use, such as rates that vary by sex, risk class, table set (VBT/CSO/etc), smoking status, relative risk, ALB/ANB, etc.\nIt’s easy to define the parameters applicable to your assumption set. Here, we’ll use a dictionary to define the relationship:\n\nrate_map = Dict(\n    \"Male\" =&gt; Dict(\n        \"Smoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Smoker, ANB\"),\n        \"Nonsmoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\"),\n        ),\n    \n    \"Female\" =&gt; Dict(\n        \"Smoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Female Smoker, ANB\"),\n        \"Nonsmoker\" =&gt; MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Female Nonsmoker, ANB\"),\n        )\n    );\n\nAnd then we’ll define a function to look up the relevant rate. Note how the function matches the levels we defined for the assumption set dictionary above.\n\nfunction rate_lookup(assumption_map,sex,smoke,issue_age,attained_age)\n    # pick the relevant table\n    table = assumption_map[sex][smoke]\n    \n    # check if the select rate exists, otherwise look to the ulitmate table\n    if issue_age in eachindex(table.select)\n        table.select[issue_age][attained_age]\n    else\n        table.ultimate[attained_age]\n    end\nend\n\nrate_lookup (generic function with 1 method)\n\n\n\n\n\nBy mapping each row’s data to the lookup function, we get a vector of rates for our data:\n\nrates = map(eachrow(sample_data)) do row\n    rate_lookup(rate_map, row.sex, row.smoke, row.issue_age, row.attained_age)\nend\n\n10000-element Vector{Float64}:\n 0.00971\n 0.00056\n 0.00091\n 0.001\n 0.01237\n 0.00292\n 0.00081\n 0.00066\n 0.00527\n 0.00483\n 0.00977\n 0.00373\n 0.00064\n ⋮\n 0.00362\n 0.00263\n 0.00127\n 0.00981\n 0.02741\n 0.02602\n 0.00087\n 0.00615\n 0.00744\n 0.00145\n 0.00059\n 0.01198\n\n\nAnd finally, we can just add this to the dataframe:\n\nsample_data.expectation = rates\n\nsample_data\n\n10000×5 DataFrame9975 rows omitted\n\n\n\nRow\nsex\nsmoke\nissue_age\nattained_age\nexpectation\n\n\n\nString\nString\nInt64\nInt64\nFloat64\n\n\n\n\n1\nFemale\nNonsmoker\n65\n71\n0.00971\n\n\n2\nMale\nNonsmoker\n33\n35\n0.00056\n\n\n3\nMale\nNonsmoker\n44\n45\n0.00091\n\n\n4\nMale\nSmoker\n36\n37\n0.001\n\n\n5\nFemale\nSmoker\n57\n64\n0.01237\n\n\n6\nFemale\nSmoker\n40\n47\n0.00292\n\n\n7\nFemale\nNonsmoker\n31\n40\n0.00081\n\n\n8\nFemale\nNonsmoker\n44\n45\n0.00066\n\n\n9\nMale\nSmoker\n40\n49\n0.00527\n\n\n10\nMale\nNonsmoker\n47\n56\n0.00483\n\n\n11\nMale\nSmoker\n48\n56\n0.00977\n\n\n12\nMale\nNonsmoker\n54\n58\n0.00373\n\n\n13\nFemale\nNonsmoker\n25\n34\n0.00064\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9989\nMale\nSmoker\n36\n45\n0.00362\n\n\n9990\nMale\nSmoker\n39\n44\n0.00263\n\n\n9991\nMale\nNonsmoker\n49\n50\n0.00127\n\n\n9992\nMale\nSmoker\n45\n55\n0.00981\n\n\n9993\nMale\nSmoker\n65\n70\n0.02741\n\n\n9994\nMale\nSmoker\n60\n68\n0.02602\n\n\n9995\nMale\nNonsmoker\n27\n36\n0.00087\n\n\n9996\nMale\nNonsmoker\n52\n60\n0.00615\n\n\n9997\nFemale\nNonsmoker\n56\n65\n0.00744\n\n\n9998\nFemale\nNonsmoker\n40\n47\n0.00145\n\n\n9999\nFemale\nNonsmoker\n34\n39\n0.00059\n\n\n10000\nFemale\nNonsmoker\n63\n71\n0.01198"
  },
  {
    "objectID": "posts/nested-policy-projections/index.html",
    "href": "posts/nested-policy-projections/index.html",
    "title": "Nested Projection Mechanics",
    "section": "",
    "text": "A simple example of how one could define a nested projection system. Includes the following examples: - Outer loop policy projections only - Outer + Inner loop policy projections with padded cashflows determining reserves and capital - Outer + Inner loop with a stochastic interest rate for the reserves\nIn this notebook, we define a term life policy, implement the mechanics for “outer” projected values, as well as “inner” projections so that we can determine a projection-based reserve. This is done with both a deterministic and stochastic “inner” loop.\nusing ActuaryUtilities\nusing DataFrames\nusing Setfield"
  },
  {
    "objectID": "posts/nested-policy-projections/index.html#policy-data-methods",
    "href": "posts/nested-policy-projections/index.html#policy-data-methods",
    "title": "Nested Projection Mechanics",
    "section": "Policy Data & Methods",
    "text": "Policy Data & Methods\nWe will use a simple Term policy as an example for the mechanics. We don’t need to, but to illustrate that we could easily implement different product types, we first define an abstract type for Policys, and then the specific Term implementation.\nThe type annotations aren’t strictly necessary, but knowing the types in advance helps Julia specialize the code for it.\n\nabstract type Policy end\n\nstruct Term &lt;: Policy\n    inforce::Float64 # count of inforce\n    term::Int        # length of benefit period (in months)\n    premium::Float64\n    face::Float64\nend\n\nFor consistency across different calculated amounts, we will keep the function signature consistent, even if not all of the arguments are used:\n(policy,assumptions,time) -&gt; result\n\npremiums(pol::Term, assumptions, time) = pol.inforce * pol.premium / 12\n\nqx(pol::Term, assumptions, time) = assumptions.q / 12\n\ndeaths(pol::Term, assumptions, time) = pol.inforce * qx(pol, assumptions, time)\n\nclaims(pol::Term, assumptions, time) = deaths(pol, assumptions, time) * pol.face\n\nclaims (generic function with 1 method)"
  },
  {
    "objectID": "posts/nested-policy-projections/index.html#projection-assumptions",
    "href": "posts/nested-policy-projections/index.html#projection-assumptions",
    "title": "Nested Projection Mechanics",
    "section": "Projection Assumptions",
    "text": "Projection Assumptions\nWe define some global assumptions that get passed around. It’s good practice (and performant) to pass variables into functions instead of just referring to global variable values.\n\nassumptions = (\n    q=0.012,\n    int_reserve=0.02,\n    capital_factor=0.1, # rate * reserves\n)\n\n(q = 0.012, int_reserve = 0.02, capital_factor = 0.1)\n\n\n\nInner-loop assumption\nIn this example, we’re assuming just a PADed mortality rate for the inner loop. We take the assumption set and use Setfield.@set to return a new immutable named tuple with just that value modified:\n\ninnerloop_assumption(outer_assump) = @set outer_assump.q *= 1.2\n\ninnerloop_assumption (generic function with 1 method)"
  },
  {
    "objectID": "posts/nested-policy-projections/index.html#projection-logic",
    "href": "posts/nested-policy-projections/index.html#projection-logic",
    "title": "Nested Projection Mechanics",
    "section": "Projection Logic",
    "text": "Projection Logic\nThe architecture takes inspiration from web server architecture where data is passed through multiple processing steps before being returned. The logic is contained within a function called project, which:\n\nProject a single timestep and create a tuple of values: (policy, assumptions, result)\nApply the function additional_processing which takes as an argument (policy, assumptions, result).\nadditional_processing can then define an “inner” loop, which could just be to apply the project with a modified set of assumptions. In this way, one or more “inner” loops can be defined.\nThe final additional_processing function should return whatever you want to return as a result.\n\nBy default, the additional_processing will simply return the last argument, result and therefore will not have any inner loops.\n\n\"\"\"\n    project(policy,assumptions;start_time=1,additional_processing=res)\n\nThe kwarg `additional_processing` defines an intermediate processing step where one can take the current model state and perform additional work, including nested projections. If left out of the arguments, the default for `additional_processing` is `res`, where `res` is (pol,assumptions, result)-&gt;result (ie will just return the model point's results with no additional work being done).\n\n\"\"\"\nfunction project(\n    pol::Term,\n    assumptions;\n    start_time=1,\n    additional_processing=(pol, assumptions, result) -&gt; result\n)\n\n    # alias the assumptions to A for brevity\n    A = assumptions\n\n\n    # iterate over the policy from the start time to the end of the policy's term\n    map(start_time:pol.term) do t\n        # calculate components of the projection\n        timestep = t\n        premium = premiums(pol, A, t)\n        q = qx(pol, A, t)\n        death = deaths(pol, A, t)\n        claim = claims(pol, A, t)\n        net_cf = premium - claim\n        inforce = pol.inforce - death\n        pol = @set pol.inforce = inforce\n\n        # return a vector of name tuples with the results\n        result = (;\n            timestep,\n            premium,\n            death,\n            claim,\n            net_cf,\n            inforce,\n            q,\n        )\n\n        # apply additional processing function \n        additional_processing(pol, A, result)\n    end\nend\n\nproject\n\n\n\n# Function signature: (policy, assumptions, result) -&gt; updated result\nfunction run_inner(policy, assumptions, result)\n    additional_results = if result.timestep + 1 &lt;= policy.term\n        A = innerloop_assumption(assumptions)\n        p = project(policy, A; start_time=result.timestep + 1)\n\n        # calculate the reserves as the present value of the \n        # cashflows within the inner loop projections\n        # discounted at the reserve interest rate\n        reserves = -pv(A.int_reserve, [modelpoint.net_cf for modelpoint in p])\n        capital = reserves * A.capital_factor\n        (; reserves, capital)\n    else\n        reserves = 0.0\n        capital = 0.0\n        (; reserves, capital)\n\n    end\n\n    return merge(result, additional_results)\n\nend\n\nrun_inner (generic function with 1 method)\n\n\nAnd a stochastic version:\n\n# Function signature: (policy, assumptions, result) -&gt; updated result\nfunction run_inner_stochastic(policy, assumptions, result)\n    additional_results = if result.timestep + 1 &lt;= policy.term\n        A = innerloop_assumption(assumptions)\n        p = project(policy, A; start_time=result.timestep + 1)\n\n        # simple stochastic interest rate \n        n = 100\n\n        reserves = let\n            i = A.int_reserve\n            f = pv(i + 0.005 * randn(), [modelpoint.net_cf for modelpoint in p])\n\n            -sum(f for _ in 1:n) / n\n\n        end\n        capital = reserves * A.capital_factor\n        (; reserves, capital)\n    else\n        reserves = 0.0\n        capital = 0.0\n        (; reserves, capital)\n    end\n    return merge(result, additional_results)\n\nend\n\nrun_inner_stochastic (generic function with 1 method)"
  },
  {
    "objectID": "posts/nested-policy-projections/index.html#projections",
    "href": "posts/nested-policy-projections/index.html#projections",
    "title": "Nested Projection Mechanics",
    "section": "Projections",
    "text": "Projections\nFirst, define a sample policy:\n\np = Term(1.0, 120, 1300.0, 100_000.0)\n\nTerm(1.0, 120, 1300.0, 100000.0)\n\n\nA projection without any additional processing:\n\nproject(p, assumptions) |&gt; DataFrame\n\n120×7 DataFrame95 rows omitted\n\n\n\nRow\ntimestep\npremium\ndeath\nclaim\nnet_cf\ninforce\nq\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1\n108.333\n0.001\n100.0\n8.33333\n0.999\n0.001\n\n\n2\n2\n108.225\n0.000999\n99.9\n8.325\n0.998001\n0.001\n\n\n3\n3\n108.117\n0.000998001\n99.8001\n8.31667\n0.997003\n0.001\n\n\n4\n4\n108.009\n0.000997003\n99.7003\n8.30836\n0.996006\n0.001\n\n\n5\n5\n107.901\n0.000996006\n99.6006\n8.30005\n0.99501\n0.001\n\n\n6\n6\n107.793\n0.00099501\n99.501\n8.29175\n0.994015\n0.001\n\n\n7\n7\n107.685\n0.000994015\n99.4015\n8.28346\n0.993021\n0.001\n\n\n8\n8\n107.577\n0.000993021\n99.3021\n8.27517\n0.992028\n0.001\n\n\n9\n9\n107.47\n0.000992028\n99.2028\n8.2669\n0.991036\n0.001\n\n\n10\n10\n107.362\n0.000991036\n99.1036\n8.25863\n0.990045\n0.001\n\n\n11\n11\n107.255\n0.000990045\n99.0045\n8.25037\n0.989055\n0.001\n\n\n12\n12\n107.148\n0.000989055\n98.9055\n8.24212\n0.988066\n0.001\n\n\n13\n13\n107.04\n0.000988066\n98.8066\n8.23388\n0.987078\n0.001\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\n109\n97.2377\n0.000897579\n89.7579\n7.47983\n0.896682\n0.001\n\n\n110\n110\n97.1405\n0.000896682\n89.6682\n7.47235\n0.895785\n0.001\n\n\n111\n111\n97.0434\n0.000895785\n89.5785\n7.46487\n0.894889\n0.001\n\n\n112\n112\n96.9463\n0.000894889\n89.4889\n7.45741\n0.893994\n0.001\n\n\n113\n113\n96.8494\n0.000893994\n89.3994\n7.44995\n0.8931\n0.001\n\n\n114\n114\n96.7525\n0.0008931\n89.31\n7.4425\n0.892207\n0.001\n\n\n115\n115\n96.6558\n0.000892207\n89.2207\n7.43506\n0.891315\n0.001\n\n\n116\n116\n96.5591\n0.000891315\n89.1315\n7.42762\n0.890424\n0.001\n\n\n117\n117\n96.4626\n0.000890424\n89.0424\n7.4202\n0.889533\n0.001\n\n\n118\n118\n96.3661\n0.000889533\n88.9533\n7.41278\n0.888644\n0.001\n\n\n119\n119\n96.2697\n0.000888644\n88.8644\n7.40536\n0.887755\n0.001\n\n\n120\n120\n96.1735\n0.000887755\n88.7755\n7.39796\n0.886867\n0.001\n\n\n\n\n\n\nAnd an example which uses a PADed inner loop to determine the resserves and capital:\n\nproject(p, assumptions; additional_processing=run_inner) |&gt; DataFrame\n\n120×9 DataFrame95 rows omitted\n\n\n\nRow\ntimestep\npremium\ndeath\nclaim\nnet_cf\ninforce\nq\nreserves\ncapital\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1\n108.333\n0.001\n100.0\n8.33333\n0.999\n0.001\n504.61\n50.461\n\n\n2\n2\n108.225\n0.000999\n99.9\n8.325\n0.998001\n0.001\n503.148\n50.3148\n\n\n3\n3\n108.117\n0.000998001\n99.8001\n8.31667\n0.997003\n0.001\n501.668\n50.1668\n\n\n4\n4\n108.009\n0.000997003\n99.7003\n8.30836\n0.996006\n0.001\n500.169\n50.0169\n\n\n5\n5\n107.901\n0.000996006\n99.6006\n8.30005\n0.99501\n0.001\n498.652\n49.8652\n\n\n6\n6\n107.793\n0.00099501\n99.501\n8.29175\n0.994015\n0.001\n497.117\n49.7117\n\n\n7\n7\n107.685\n0.000994015\n99.4015\n8.28346\n0.993021\n0.001\n495.561\n49.5561\n\n\n8\n8\n107.577\n0.000993021\n99.3021\n8.27517\n0.992028\n0.001\n493.986\n49.3986\n\n\n9\n9\n107.47\n0.000992028\n99.2028\n8.2669\n0.991036\n0.001\n492.391\n49.2391\n\n\n10\n10\n107.362\n0.000991036\n99.1036\n8.25863\n0.990045\n0.001\n490.775\n49.0775\n\n\n11\n11\n107.255\n0.000990045\n99.0045\n8.25037\n0.989055\n0.001\n489.138\n48.9138\n\n\n12\n12\n107.148\n0.000989055\n98.9055\n8.24212\n0.988066\n0.001\n487.479\n48.7479\n\n\n13\n13\n107.04\n0.000988066\n98.8066\n8.23388\n0.987078\n0.001\n485.799\n48.5799\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\n109\n97.2377\n0.000897579\n89.7579\n7.47983\n0.896682\n0.001\n101.795\n10.1795\n\n\n110\n110\n97.1405\n0.000896682\n89.6682\n7.47235\n0.895785\n0.001\n93.3884\n9.33884\n\n\n111\n111\n97.0434\n0.000895785\n89.5785\n7.46487\n0.894889\n0.001\n84.8223\n8.48223\n\n\n112\n112\n96.9463\n0.000894889\n89.4889\n7.45741\n0.893994\n0.001\n76.0936\n7.60936\n\n\n113\n113\n96.8494\n0.000893994\n89.3994\n7.44995\n0.8931\n0.001\n67.199\n6.7199\n\n\n114\n114\n96.7525\n0.0008931\n89.31\n7.4425\n0.892207\n0.001\n58.1351\n5.81351\n\n\n115\n115\n96.6558\n0.000892207\n89.2207\n7.43506\n0.891315\n0.001\n48.8986\n4.88986\n\n\n116\n116\n96.5591\n0.000891315\n89.1315\n7.42762\n0.890424\n0.001\n39.4858\n3.94858\n\n\n117\n117\n96.4626\n0.000890424\n89.0424\n7.4202\n0.889533\n0.001\n29.8932\n2.98932\n\n\n118\n118\n96.3661\n0.000889533\n88.9533\n7.41278\n0.888644\n0.001\n20.1172\n2.01172\n\n\n119\n119\n96.2697\n0.000888644\n88.8644\n7.40536\n0.887755\n0.001\n10.1541\n1.01541\n\n\n120\n120\n96.1735\n0.000887755\n88.7755\n7.39796\n0.886867\n0.001\n0.0\n0.0\n\n\n\n\n\n\nAnd a stochastic example:\n\nproject(p, assumptions; additional_processing=run_inner_stochastic) |&gt; DataFrame\n\n120×9 DataFrame95 rows omitted\n\n\n\nRow\ntimestep\npremium\ndeath\nclaim\nnet_cf\ninforce\nq\nreserves\ncapital\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1\n108.333\n0.001\n100.0\n8.33333\n0.999\n0.001\n495.008\n49.5008\n\n\n2\n2\n108.225\n0.000999\n99.9\n8.325\n0.998001\n0.001\n551.777\n55.1777\n\n\n3\n3\n108.117\n0.000998001\n99.8001\n8.31667\n0.997003\n0.001\n456.478\n45.6478\n\n\n4\n4\n108.009\n0.000997003\n99.7003\n8.30836\n0.996006\n0.001\n437.01\n43.701\n\n\n5\n5\n107.901\n0.000996006\n99.6006\n8.30005\n0.99501\n0.001\n681.328\n68.1328\n\n\n6\n6\n107.793\n0.00099501\n99.501\n8.29175\n0.994015\n0.001\n512.651\n51.2651\n\n\n7\n7\n107.685\n0.000994015\n99.4015\n8.28346\n0.993021\n0.001\n538.304\n53.8304\n\n\n8\n8\n107.577\n0.000993021\n99.3021\n8.27517\n0.992028\n0.001\n848.208\n84.8208\n\n\n9\n9\n107.47\n0.000992028\n99.2028\n8.2669\n0.991036\n0.001\n523.956\n52.3956\n\n\n10\n10\n107.362\n0.000991036\n99.1036\n8.25863\n0.990045\n0.001\n404.1\n40.41\n\n\n11\n11\n107.255\n0.000990045\n99.0045\n8.25037\n0.989055\n0.001\n508.197\n50.8197\n\n\n12\n12\n107.148\n0.000989055\n98.9055\n8.24212\n0.988066\n0.001\n468.548\n46.8548\n\n\n13\n13\n107.04\n0.000988066\n98.8066\n8.23388\n0.987078\n0.001\n456.656\n45.6656\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\n109\n97.2377\n0.000897579\n89.7579\n7.47983\n0.896682\n0.001\n104.005\n10.4005\n\n\n110\n110\n97.1405\n0.000896682\n89.6682\n7.47235\n0.895785\n0.001\n97.0335\n9.70335\n\n\n111\n111\n97.0434\n0.000895785\n89.5785\n7.46487\n0.894889\n0.001\n83.7453\n8.37453\n\n\n112\n112\n96.9463\n0.000894889\n89.4889\n7.45741\n0.893994\n0.001\n75.1751\n7.51751\n\n\n113\n113\n96.8494\n0.000893994\n89.3994\n7.44995\n0.8931\n0.001\n65.4564\n6.54564\n\n\n114\n114\n96.7525\n0.0008931\n89.31\n7.4425\n0.892207\n0.001\n59.3372\n5.93372\n\n\n115\n115\n96.6558\n0.000892207\n89.2207\n7.43506\n0.891315\n0.001\n49.5258\n4.95258\n\n\n116\n116\n96.5591\n0.000891315\n89.1315\n7.42762\n0.890424\n0.001\n39.476\n3.9476\n\n\n117\n117\n96.4626\n0.000890424\n89.0424\n7.4202\n0.889533\n0.001\n29.9494\n2.99494\n\n\n118\n118\n96.3661\n0.000889533\n88.9533\n7.41278\n0.888644\n0.001\n20.2168\n2.02168\n\n\n119\n119\n96.2697\n0.000888644\n88.8644\n7.40536\n0.887755\n0.001\n10.1431\n1.01431\n\n\n120\n120\n96.1735\n0.000887755\n88.7755\n7.39796\n0.886867\n0.001\n0.0\n0.0"
  },
  {
    "objectID": "posts/nested-policy-projections/index.html#endnotes",
    "href": "posts/nested-policy-projections/index.html#endnotes",
    "title": "Nested Projection Mechanics",
    "section": "Endnotes",
    "text": "Endnotes\n\nFurther Work\nThis example is simple, but could be greatly optimized to reduce intermediate variable allocations, refine the timing of cashflows, add additional decrements, handle different types of Policys, abstract some of the projection mechanics into an Iterable object, etc.\n\n\nDisclaimer\nCreated as a proof of concept and not indended to be interpreted as a meaningful projection."
  },
  {
    "objectID": "posts/bayesian-claims-demo/index.html",
    "href": "posts/bayesian-claims-demo/index.html",
    "title": "Bayesian Markov-Chain-Monte-Carlo and Claims Data",
    "section": "",
    "text": "using MortalityTables\nusing Turing\nusing UUIDs\nusing DataFramesMeta\nusing MCMCChains, Plots, StatsPlots\nusing LinearAlgebra\nusing Pipe\nusing StatsFuns"
  },
  {
    "objectID": "posts/bayesian-claims-demo/index.html#generating-fake-data",
    "href": "posts/bayesian-claims-demo/index.html#generating-fake-data",
    "title": "Bayesian Markov-Chain-Monte-Carlo and Claims Data",
    "section": "Generating fake data",
    "text": "Generating fake data\nThe problem of interest is to look at mortality rates, which are given in terms of exposures (whether or not a life experienced a death in a given year).\nWe’ll grab some example rates from an insurance table, which has a “selection” component: When someone enters observation, say at age 50, their mortality is path dependent (so for someone who started being observed at 50 will have a different risk/mortality rate at age 55 than someone who started being observed at 45).\nAddtionally, there may be additional groups of interest, such as: - high/medium/low risk classification - sex - group (e.g. company, data source, etc.) - type of insurance product offered\nThe example data will start with only the risk classification above\n\nsrc = MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\")\n\nMortalityTable (Insured Lives Mortality):\n   Name:\n       2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\n   Fields: \n       (:select, :ultimate, :metadata)\n   Provider:\n       Society of Actuaries\n   mort.SOA.org ID:\n       1118\n   mort.SOA.org link:\n       https://mort.soa.org/ViewTable.aspx?&TableIdentity=1118\n   Description:\n       2001 Valuation Basic Table (VBT) Residual Standard Select and Ultimate Table -  Male Nonsmoker. Basis: Age Nearest Birthday. Minimum Select Age: 0. Maximum Select Age: 99. Minimum Ultimate Age: 25. Maximum Ultimate Age: 120\n\n\n\nn = 10_000\n\nfunction generate_data_individual(tbl, issue_age=rand(50:55), inforce_years=rand(1:30), risklevel=rand(1:3))\n    # risk_factors will scale the \"true\" parameter up or down\n    # we observe the assigned risklevel, but not risk_factor\n    risk_factors = [0.7, 1.0, 1.5]\n    rf = risk_factors[risklevel]\n    deaths = rand(inforce_years) .&lt; (tbl.select[issue_age][issue_age.+inforce_years.-1] .* rf)\n\n    endpoint = if sum(deaths) == 0\n        last(inforce_years)\n    else\n        findfirst(deaths)\n    end\n    id = uuid1()\n    map(1:endpoint) do i\n        (\n            issue_age=issue_age,\n            risklevel=risklevel,\n            att_age=issue_age + i - 1,\n            death=deaths[i],\n            id=id,\n        )\n    end\n\nend\n\nexposures = vcat([generate_data_individual(src) for _ in 1:n]...) |&gt; DataFrame\n\n108913×5 DataFrame108888 rows omitted\n\n\n\nRow\nissue_age\nrisklevel\natt_age\ndeath\nid\n\n\n\nInt64\nInt64\nInt64\nBool\nUUID\n\n\n\n\n1\n54\n2\n54\nfalse\n19d9bd90-2c56-11ef-221d-ab3d14a6f167\n\n\n2\n54\n2\n55\nfalse\n19d9bd90-2c56-11ef-221d-ab3d14a6f167\n\n\n3\n55\n1\n55\nfalse\n19dd329a-2c56-11ef-246b-334b05232974\n\n\n4\n55\n1\n56\nfalse\n19dd329a-2c56-11ef-246b-334b05232974\n\n\n5\n55\n1\n57\ntrue\n19dd329a-2c56-11ef-246b-334b05232974\n\n\n6\n55\n2\n55\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n7\n55\n2\n56\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n8\n55\n2\n57\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n9\n55\n2\n58\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n10\n55\n2\n59\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n11\n55\n2\n60\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n12\n55\n2\n61\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n13\n55\n2\n62\nfalse\n19dd32f4-2c56-11ef-1da4-c926e1b3d176\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n108902\n52\n3\n56\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108903\n52\n3\n57\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108904\n52\n3\n58\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108905\n52\n3\n59\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108906\n52\n3\n60\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108907\n52\n3\n61\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108908\n52\n3\n62\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108909\n52\n3\n63\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108910\n52\n3\n64\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108911\n52\n3\n65\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108912\n52\n3\n66\nfalse\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n108913\n52\n3\n67\ntrue\n19deb6c4-2c56-11ef-2c57-9b6e03cb9296\n\n\n\n\n\n\nTwo groupings, one with and without the risk level:\n\ndata = combine(groupby(exposures, [:issue_age, :att_age])) do subdf\n    (exposures=nrow(subdf),\n        deaths=sum(subdf.death),\n        fraction=sum(subdf.death) / nrow(subdf))\nend\n\n180×5 DataFrame155 rows omitted\n\n\n\nRow\nissue_age\natt_age\nexposures\ndeaths\nfraction\n\n\n\nInt64\nInt64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n50\n50\n1672\n35\n0.020933\n\n\n2\n50\n51\n1576\n22\n0.0139594\n\n\n3\n50\n52\n1509\n28\n0.0185553\n\n\n4\n50\n53\n1412\n19\n0.0134561\n\n\n5\n50\n54\n1334\n32\n0.023988\n\n\n6\n50\n55\n1257\n38\n0.0302307\n\n\n7\n50\n56\n1159\n23\n0.0198447\n\n\n8\n50\n57\n1080\n22\n0.0203704\n\n\n9\n50\n58\n996\n27\n0.0271084\n\n\n10\n50\n59\n921\n21\n0.0228013\n\n\n11\n50\n60\n861\n27\n0.0313589\n\n\n12\n50\n61\n781\n24\n0.0307298\n\n\n13\n50\n62\n709\n11\n0.0155148\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n169\n55\n73\n220\n8\n0.0363636\n\n\n170\n55\n74\n182\n12\n0.0659341\n\n\n171\n55\n75\n147\n7\n0.047619\n\n\n172\n55\n76\n122\n3\n0.0245902\n\n\n173\n55\n77\n102\n6\n0.0588235\n\n\n174\n55\n78\n76\n5\n0.0657895\n\n\n175\n55\n79\n56\n7\n0.125\n\n\n176\n55\n80\n37\n5\n0.135135\n\n\n177\n55\n81\n26\n1\n0.0384615\n\n\n178\n55\n82\n21\n3\n0.142857\n\n\n179\n55\n83\n12\n0\n0.0\n\n\n180\n55\n84\n5\n0\n0.0\n\n\n\n\n\n\n\n# ╔═╡ 45237199-f8e8-4f61-b644-89ab37c31a5d\ndata2 = combine(groupby(exposures, [:issue_age, :att_age, :risklevel])) do subdf\n    (exposures=nrow(subdf),\n        deaths=sum(subdf.death),\n        fraction=sum(subdf.death) / nrow(subdf))\nend\n\n531×6 DataFrame506 rows omitted\n\n\n\nRow\nissue_age\natt_age\nrisklevel\nexposures\ndeaths\nfraction\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n50\n50\n1\n550\n16\n0.0290909\n\n\n2\n50\n50\n2\n601\n9\n0.014975\n\n\n3\n50\n50\n3\n521\n10\n0.0191939\n\n\n4\n50\n51\n1\n518\n6\n0.011583\n\n\n5\n50\n51\n2\n568\n7\n0.0123239\n\n\n6\n50\n51\n3\n490\n9\n0.0183673\n\n\n7\n50\n52\n1\n498\n6\n0.0120482\n\n\n8\n50\n52\n2\n542\n12\n0.0221402\n\n\n9\n50\n52\n3\n469\n10\n0.021322\n\n\n10\n50\n53\n1\n474\n1\n0.0021097\n\n\n11\n50\n53\n2\n507\n6\n0.0118343\n\n\n12\n50\n53\n3\n431\n12\n0.0278422\n\n\n13\n50\n54\n1\n450\n7\n0.0155556\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n520\n55\n80\n2\n8\n2\n0.25\n\n\n521\n55\n80\n3\n4\n0\n0.0\n\n\n522\n55\n81\n1\n19\n0\n0.0\n\n\n523\n55\n81\n2\n3\n1\n0.333333\n\n\n524\n55\n81\n3\n4\n0\n0.0\n\n\n525\n55\n82\n1\n15\n2\n0.133333\n\n\n526\n55\n82\n2\n2\n0\n0.0\n\n\n527\n55\n82\n3\n4\n1\n0.25\n\n\n528\n55\n83\n1\n9\n0\n0.0\n\n\n529\n55\n83\n2\n1\n0\n0.0\n\n\n530\n55\n83\n3\n2\n0\n0.0\n\n\n531\n55\n84\n1\n5\n0\n0.0"
  },
  {
    "objectID": "posts/bayesian-claims-demo/index.html#a-single-binomial-parameter-model",
    "href": "posts/bayesian-claims-demo/index.html#a-single-binomial-parameter-model",
    "title": "Bayesian Markov-Chain-Monte-Carlo and Claims Data",
    "section": "1: A single binomial parameter model",
    "text": "1: A single binomial parameter model\nEstiamte \\(p\\), the average mortality rate, not accounting for any variation within the population/sample:\n\n@model function mortality(data, deaths)\n    p ~ Beta(1, 1)\n    for i = 1:nrow(data)\n        deaths[i] ~ Binomial(data.exposures[i], p)\n    end\nend\n\n\nm1 = mortality(data, data.deaths)\n\nnum_chains = 4\n\n4\n\n\n\nSampling from the posterior\nWe use a No-U-Turn-Sampler (NUTS) technique to sample multile chains at once:\n\nchain = sample(m1, NUTS(), 1000)\n\nplot(chain)\n\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling:   4%|█▌                                       |  ETA: 0:00:03Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting samples from the posterior\nWe can see that the sampling of possible posterior parameters doesn’t really fit the data very well since our model was so simplified. The lines represent the posterior binomial probability.\nThis is saying that for the observed data, if there really is just a single probability p that governs the true process that came up with the data, there’s a pretty narrow range of values it could possibly be:\n\nlet\n    data_weight = data.exposures ./ sum(data.exposures)\n    data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        xlabel=\"age\",\n        ylim=(0.0, 0.25),\n        ylabel=\"mortality rate\",\n        title=\"Parametric Bayseian Mortality\"\n    )\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    for i in 1:n\n        p_posterior = sample(chain, 1)[:p][1]\n        hline!([p_posterior], label=\"\", alpha=0.1)\n    end\n    p\n\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe posterior mean of p is of course very close to the simple proportoin of claims to exposures:\n\nlet\n    a = mean(chain, :p)\n    b = sum(data.deaths) / sum(data.exposures)\n    a, b\nend\n\n(0.03054276093985464, 0.030538135943367642)"
  },
  {
    "objectID": "posts/bayesian-claims-demo/index.html#parametric-model",
    "href": "posts/bayesian-claims-demo/index.html#parametric-model",
    "title": "Bayesian Markov-Chain-Monte-Carlo and Claims Data",
    "section": "2. Parametric model",
    "text": "2. Parametric model\nIn this example, we utilize a MakehamBeard parameterization because it’s already very similar in form to a logistic function. This is important because our desired output is a probability (ie the probablity of a death at a given age), so the value must be constrained to be in the interval between zero and one.\nThe prior values for a,b,c, and k are chosen to constrain the hazard (mortality) rate to be between zero and one.\nThis isn’t an ideal parameterization (e.g. we aren’t including information about the select underwriting period), but is an example of utilizing Bayesian techniques on life experience data.\n\n@model function mortality2(data, deaths)\n    a ~ Exponential(0.1)\n    b ~ Exponential(0.1)\n    c = 0.0\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n    m = MortalityTables.MakehamBeard(; a, b, c, k)\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nmortality2 (generic function with 2 methods)\n\n\n\nSampling from the posterior\nWe combine the model with the data and a use use a No-U-Turn-Sampler (NUTS) technique to sample:\n\nm2 = mortality2(data, data.deaths)\nchain2 = sample(m2, NUTS(), 1000)\nsummarize(chain2)\n\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling:   2%|▊                                        |  ETA: 0:00:06Sampling:   3%|█▎                                       |  ETA: 0:00:12Sampling:   4%|█▊                                       |  ETA: 0:00:14Sampling:   5%|█▉                                       |  ETA: 0:00:15Sampling:   6%|██▎                                      |  ETA: 0:00:15Sampling:   6%|██▋                                      |  ETA: 0:00:14Sampling:   7%|███                                      |  ETA: 0:00:14Sampling:   8%|███▎                                     |  ETA: 0:00:14Sampling:   9%|███▉                                     |  ETA: 0:00:14Sampling:  11%|████▍                                    |  ETA: 0:00:13Sampling:  12%|█████                                    |  ETA: 0:00:12Sampling:  14%|█████▌                                   |  ETA: 0:00:11Sampling:  16%|██████▌                                  |  ETA: 0:00:10Sampling:  18%|███████▎                                 |  ETA: 0:00:09Sampling:  19%|███████▉                                 |  ETA: 0:00:09Sampling:  21%|████████▊                                |  ETA: 0:00:08Sampling:  23%|█████████▌                               |  ETA: 0:00:08Sampling:  26%|██████████▌                              |  ETA: 0:00:07Sampling:  28%|███████████▎                             |  ETA: 0:00:07Sampling:  30%|████████████▍                            |  ETA: 0:00:06Sampling:  33%|█████████████▍                           |  ETA: 0:00:06Sampling:  34%|██████████████▏                          |  ETA: 0:00:06Sampling:  36%|██████████████▉                          |  ETA: 0:00:05Sampling:  39%|████████████████▏                        |  ETA: 0:00:05Sampling:  42%|█████████████████                        |  ETA: 0:00:05Sampling:  44%|██████████████████▏                      |  ETA: 0:00:04Sampling:  47%|███████████████████▏                     |  ETA: 0:00:04Sampling:  48%|███████████████████▉                     |  ETA: 0:00:04Sampling:  50%|████████████████████▋                    |  ETA: 0:00:04Sampling:  53%|█████████████████████▋                   |  ETA: 0:00:03Sampling:  55%|██████████████████████▍                  |  ETA: 0:00:03Sampling:  57%|███████████████████████▍                 |  ETA: 0:00:03Sampling:  59%|████████████████████████▏                |  ETA: 0:00:03Sampling:  61%|████████████████████████▉                |  ETA: 0:00:03Sampling:  63%|█████████████████████████▉               |  ETA: 0:00:03Sampling:  65%|██████████████████████████▋              |  ETA: 0:00:02Sampling:  67%|███████████████████████████▌             |  ETA: 0:00:02Sampling:  69%|████████████████████████████▎            |  ETA: 0:00:02Sampling:  71%|█████████████████████████████▎           |  ETA: 0:00:02Sampling:  74%|██████████████████████████████▎          |  ETA: 0:00:02Sampling:  76%|███████████████████████████████          |  ETA: 0:00:02Sampling:  77%|███████████████████████████████▊         |  ETA: 0:00:02Sampling:  79%|████████████████████████████████▌        |  ETA: 0:00:01Sampling:  82%|█████████████████████████████████▌       |  ETA: 0:00:01Sampling:  84%|██████████████████████████████████▎      |  ETA: 0:00:01Sampling:  85%|███████████████████████████████████      |  ETA: 0:00:01Sampling:  88%|████████████████████████████████████     |  ETA: 0:00:01Sampling:  90%|████████████████████████████████████▉    |  ETA: 0:00:01Sampling:  92%|█████████████████████████████████████▋   |  ETA: 0:00:01Sampling:  94%|██████████████████████████████████████▋  |  ETA: 0:00:00Sampling:  96%|███████████████████████████████████████▍ |  ETA: 0:00:00Sampling:  98%|████████████████████████████████████████▏|  ETA: 0:00:00Sampling:  99%|████████████████████████████████████████▉|  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:06\n\n\n\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           a    0.0036    0.0006    0.0000   240.9349   198.5353    0.9998     ⋯\n           b    0.0366    0.0031    0.0002   215.0610   171.8594    1.0001     ⋯\n           k    2.0919    1.1672    0.0756   269.2057   214.9856    1.0071     ⋯\n                                                                1 column omitted\n\n\n\n\n\nplot(chain2)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting samples from the posterior\nWe can see that the sampling of possible posterior parameters fits the data well:\n\nlet\n    data_weight = data.exposures ./ sum(data.exposures)\n    data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        xlabel=\"age\",\n        ylim=(0.0, 0.25),\n        ylabel=\"mortality rate\",\n        title=\"Parametric Bayseian Mortality\"\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    for i in 1:n\n        s = sample(chain2, 1)\n        a = only(s[:a])\n        b = only(s[:b])\n        k = only(s[:k])\n        c = 0\n        m = MortalityTables.MakehamBeard(; a, b, c, k)\n        plot!(ages, age -&gt; MortalityTables.hazard(m, age), alpha=0.1, label=\"\")\n    end\n    p\nend"
  },
  {
    "objectID": "posts/bayesian-claims-demo/index.html#parametric-model-1",
    "href": "posts/bayesian-claims-demo/index.html#parametric-model-1",
    "title": "Bayesian Markov-Chain-Monte-Carlo and Claims Data",
    "section": "3. Parametric model",
    "text": "3. Parametric model\nThis model extends the prior to create a multi-level model. Each risk class (risklevel) gets its own \\(a\\) paramater in the MakhamBeard model. The prior for \\(a_i\\) is determined by the hyperparameter \\(\\bar{a}\\).\n\n@model function mortality3(data, deaths)\n    risk_levels = length(levels(data.risklevel))\n    b ~ Exponential(0.1)\n    ā ~ Exponential(0.1)\n    a ~ filldist(Exponential(ā), risk_levels)\n    c = 0\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        risk = data.risklevel[i]\n\n        m = MortalityTables.MakehamBeard(; a=a[risk], b, c, k)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nmortality3 (generic function with 2 methods)\n\n\nInstantiate model with the data, sample, and summarize:\n\nm3 = mortality3(data2, data2.deaths)\nchain3 = sample(m3, NUTS(), 1000)\nsummarize(chain3)\n\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling:   1%|▋                                        |  ETA: 0:00:17Sampling:   2%|▊                                        |  ETA: 0:00:19Sampling:   3%|█▏                                       |  ETA: 0:00:34Sampling:   3%|█▎                                       |  ETA: 0:00:52Sampling:   4%|█▌                                       |  ETA: 0:01:06Sampling:   4%|█▊                                       |  ETA: 0:01:03Sampling:   5%|█▉                                       |  ETA: 0:01:06Sampling:   5%|██▏                                      |  ETA: 0:01:08Sampling:   6%|██▎                                      |  ETA: 0:01:10Sampling:   6%|██▌                                      |  ETA: 0:01:09Sampling:   6%|██▋                                      |  ETA: 0:01:10Sampling:   7%|██▉                                      |  ETA: 0:01:13Sampling:   7%|███                                      |  ETA: 0:01:11Sampling:   8%|███▎                                     |  ETA: 0:01:09Sampling:   8%|███▌                                     |  ETA: 0:01:08Sampling:   9%|███▉                                     |  ETA: 0:01:02Sampling:  10%|████                                     |  ETA: 0:01:00Sampling:  11%|████▍                                    |  ETA: 0:00:56Sampling:  11%|████▋                                    |  ETA: 0:00:54Sampling:  12%|████▊                                    |  ETA: 0:00:53Sampling:  13%|█████▏                                   |  ETA: 0:00:50Sampling:  14%|█████▌                                   |  ETA: 0:00:47Sampling:  14%|█████▉                                   |  ETA: 0:00:45Sampling:  15%|██████▍                                  |  ETA: 0:00:42Sampling:  16%|██████▋                                  |  ETA: 0:00:40Sampling:  17%|██████▉                                  |  ETA: 0:00:40Sampling:  17%|███████                                  |  ETA: 0:00:39Sampling:  18%|███████▎                                 |  ETA: 0:00:39Sampling:  18%|███████▌                                 |  ETA: 0:00:38Sampling:  19%|███████▋                                 |  ETA: 0:00:38Sampling:  19%|███████▉                                 |  ETA: 0:00:37Sampling:  20%|████████▎                                |  ETA: 0:00:36Sampling:  20%|████████▍                                |  ETA: 0:00:36Sampling:  21%|████████▊                                |  ETA: 0:00:34Sampling:  22%|█████████                                |  ETA: 0:00:34Sampling:  22%|█████████▏                               |  ETA: 0:00:33Sampling:  23%|█████████▌                               |  ETA: 0:00:32Sampling:  24%|█████████▊                               |  ETA: 0:00:32Sampling:  24%|█████████▉                               |  ETA: 0:00:32Sampling:  25%|██████████▍                              |  ETA: 0:00:30Sampling:  26%|██████████▌                              |  ETA: 0:00:30Sampling:  26%|██████████▊                              |  ETA: 0:00:30Sampling:  27%|███████████▏                             |  ETA: 0:00:29Sampling:  28%|███████████▌                             |  ETA: 0:00:28Sampling:  28%|███████████▋                             |  ETA: 0:00:27Sampling:  29%|████████████                             |  ETA: 0:00:26Sampling:  30%|████████████▎                            |  ETA: 0:00:26Sampling:  31%|████████████▋                            |  ETA: 0:00:26Sampling:  31%|████████████▊                            |  ETA: 0:00:25Sampling:  32%|█████████████▎                           |  ETA: 0:00:25Sampling:  33%|█████████████▋                           |  ETA: 0:00:24Sampling:  34%|██████████████                           |  ETA: 0:00:24Sampling:  35%|██████████████▍                          |  ETA: 0:00:23Sampling:  35%|██████████████▌                          |  ETA: 0:00:23Sampling:  36%|██████████████▉                          |  ETA: 0:00:22Sampling:  37%|███████████████▏                         |  ETA: 0:00:22Sampling:  37%|███████████████▎                         |  ETA: 0:00:22Sampling:  38%|███████████████▌                         |  ETA: 0:00:21Sampling:  39%|███████████████▉                         |  ETA: 0:00:21Sampling:  39%|████████████████▏                        |  ETA: 0:00:21Sampling:  40%|████████████████▌                        |  ETA: 0:00:20Sampling:  41%|████████████████▊                        |  ETA: 0:00:20Sampling:  42%|█████████████████▎                       |  ETA: 0:00:19Sampling:  43%|█████████████████▋                       |  ETA: 0:00:19Sampling:  44%|██████████████████                       |  ETA: 0:00:18Sampling:  45%|██████████████████▍                      |  ETA: 0:00:18Sampling:  46%|██████████████████▊                      |  ETA: 0:00:17Sampling:  47%|███████████████████▏                     |  ETA: 0:00:17Sampling:  48%|███████████████████▌                     |  ETA: 0:00:16Sampling:  49%|████████████████████▎                    |  ETA: 0:00:15Sampling:  52%|█████████████████████▎                   |  ETA: 0:00:14Sampling:  54%|██████████████████████▏                  |  ETA: 0:00:13Sampling:  55%|██████████████████████▌                  |  ETA: 0:00:13Sampling:  56%|███████████████████████                  |  ETA: 0:00:13Sampling:  57%|███████████████████████▍                 |  ETA: 0:00:12Sampling:  57%|███████████████████████▌                 |  ETA: 0:00:12Sampling:  58%|███████████████████████▉                 |  ETA: 0:00:12Sampling:  59%|████████████████████████▏                |  ETA: 0:00:12Sampling:  59%|████████████████████████▎                |  ETA: 0:00:12Sampling:  60%|████████████████████████▌                |  ETA: 0:00:11Sampling:  60%|████████████████████████▋                |  ETA: 0:00:11Sampling:  61%|█████████████████████████                |  ETA: 0:00:11Sampling:  62%|█████████████████████████▍               |  ETA: 0:00:11Sampling:  62%|█████████████████████████▋               |  ETA: 0:00:10Sampling:  63%|██████████████████████████               |  ETA: 0:00:10Sampling:  64%|██████████████████████████▎              |  ETA: 0:00:10Sampling:  65%|██████████████████████████▋              |  ETA: 0:00:10Sampling:  66%|███████████████████████████              |  ETA: 0:00:09Sampling:  66%|███████████████████████████▏             |  ETA: 0:00:09Sampling:  67%|███████████████████████████▍             |  ETA: 0:00:09Sampling:  67%|███████████████████████████▌             |  ETA: 0:00:09Sampling:  68%|███████████████████████████▉             |  ETA: 0:00:09Sampling:  69%|████████████████████████████▏            |  ETA: 0:00:09Sampling:  70%|████████████████████████████▌            |  ETA: 0:00:08Sampling:  70%|████████████████████████████▉            |  ETA: 0:00:08Sampling:  71%|█████████████████████████████▏           |  ETA: 0:00:08Sampling:  72%|█████████████████████████████▌           |  ETA: 0:00:08Sampling:  72%|█████████████████████████████▋           |  ETA: 0:00:08Sampling:  73%|██████████████████████████████           |  ETA: 0:00:07Sampling:  74%|██████████████████████████████▎          |  ETA: 0:00:07Sampling:  75%|██████████████████████████████▋          |  ETA: 0:00:07Sampling:  76%|███████████████████████████████          |  ETA: 0:00:07Sampling:  76%|███████████████████████████████▏         |  ETA: 0:00:06Sampling:  77%|███████████████████████████████▋         |  ETA: 0:00:06Sampling:  77%|███████████████████████████████▊         |  ETA: 0:00:06Sampling:  78%|████████████████████████████████         |  ETA: 0:00:06Sampling:  79%|████████████████████████████████▎        |  ETA: 0:00:06Sampling:  80%|████████████████████████████████▊        |  ETA: 0:00:05Sampling:  81%|█████████████████████████████████▏       |  ETA: 0:00:05Sampling:  81%|█████████████████████████████████▎       |  ETA: 0:00:05Sampling:  82%|█████████████████████████████████▋       |  ETA: 0:00:05Sampling:  83%|██████████████████████████████████       |  ETA: 0:00:04Sampling:  84%|██████████████████████████████████▎      |  ETA: 0:00:04Sampling:  84%|██████████████████████████████████▋      |  ETA: 0:00:04Sampling:  85%|███████████████████████████████████      |  ETA: 0:00:04Sampling:  86%|███████████████████████████████████▏     |  ETA: 0:00:04Sampling:  87%|███████████████████████████████████▋     |  ETA: 0:00:03Sampling:  88%|████████████████████████████████████     |  ETA: 0:00:03Sampling:  88%|████████████████████████████████████▏    |  ETA: 0:00:03Sampling:  89%|████████████████████████████████████▍    |  ETA: 0:00:03Sampling:  89%|████████████████████████████████████▌    |  ETA: 0:00:03Sampling:  90%|████████████████████████████████████▊    |  ETA: 0:00:03Sampling:  90%|█████████████████████████████████████▏   |  ETA: 0:00:02Sampling:  91%|█████████████████████████████████████▌   |  ETA: 0:00:02Sampling:  92%|█████████████████████████████████████▋   |  ETA: 0:00:02Sampling:  92%|█████████████████████████████████████▉   |  ETA: 0:00:02Sampling:  93%|██████████████████████████████████████▎  |  ETA: 0:00:02Sampling:  94%|██████████████████████████████████████▌  |  ETA: 0:00:02Sampling:  95%|██████████████████████████████████████▉  |  ETA: 0:00:01Sampling:  95%|███████████████████████████████████████  |  ETA: 0:00:01Sampling:  96%|███████████████████████████████████████▎ |  ETA: 0:00:01Sampling:  97%|███████████████████████████████████████▋ |  ETA: 0:00:01Sampling:  98%|████████████████████████████████████████ |  ETA: 0:00:01Sampling:  98%|████████████████████████████████████████▏|  ETA: 0:00:01Sampling:  98%|████████████████████████████████████████▍|  ETA: 0:00:00Sampling:  99%|████████████████████████████████████████▌|  ETA: 0:00:00Sampling:  99%|████████████████████████████████████████▊|  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:25\n\n\n\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           b    0.0447    0.0053    0.0012    33.2602    21.5493    1.0398     ⋯\n           ā    0.0065    0.0071    0.0005    54.2101    85.6676    1.0458     ⋯\n        a[1]    0.0016    0.0004    0.0001    34.7822    21.6119    1.0297     ⋯\n        a[2]    0.0022    0.0005    0.0001    34.5119    18.3744    1.0348     ⋯\n        a[3]    0.0035    0.0008    0.0001    37.7816    42.2177    1.0315     ⋯\n           k    2.8153    1.9264    0.4387    38.2738    20.6857    1.0268     ⋯\n                                                                1 column omitted\n\n\n\n\n\nlet data = data2\n\n    data_weight = data.exposures ./ sum(data.exposures)\n    data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n    color_i = data.risklevel\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        color=color_i,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        xlabel=\"age\",\n        ylim=(0.0, 0.25),\n        ylabel=\"mortality rate\",\n        title=\"Parametric Bayseian Mortality\"\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 100\n\n    ages = sort!(unique(data.att_age))\n    for r in 1:3\n        for i in 1:n\n            s = sample(chain3, 1)\n            a = only(s[Symbol(\"a[$r]\")])\n            b = only(s[:b])\n            k = only(s[:k])\n            c = 0\n            m = MortalityTables.MakehamBeard(; a, b, c, k)\n            if i == 1\n                plot!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"risk level $r\", alpha=0.2, color=r)\n            else\n                plot!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"\", alpha=0.2, color=r)\n            end\n        end\n    end\n    p\nend"
  },
  {
    "objectID": "posts/bayesian-claims-demo/index.html#handling-non-unit-exposures",
    "href": "posts/bayesian-claims-demo/index.html#handling-non-unit-exposures",
    "title": "Bayesian Markov-Chain-Monte-Carlo and Claims Data",
    "section": "Handling non-unit exposures",
    "text": "Handling non-unit exposures\nThe key is to use the Poisson distribution:\n\n@model function mortality4(data, deaths)\n    risk_levels = length(levels(data.risklevel))\n    b ~ Exponential(0.1)\n    ā ~ Exponential(0.1)\n    a ~ filldist(Exponential(ā), risk_levels)\n    c ~ Beta(4, 18)\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        risk = data.risklevel[i]\n\n        m = MortalityTables.MakehamBeard(; a=a[risk], b, c, k)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Poisson(data.exposures[i] * q)\n    end\nend\n\nmortality4 (generic function with 2 methods)\n\n\nInstantiate model with the data, sample, and summarize:\n\nm4 = mortality4(data2, data2.deaths)\nchain4 = sample(m4, NUTS(), 1000)\nsummarize(chain4)\n\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling:   1%|▋                                        |  ETA: 0:00:35Sampling:   2%|▊                                        |  ETA: 0:00:39Sampling:   3%|█▏                                       |  ETA: 0:00:32Sampling:   4%|█▌                                       |  ETA: 0:00:34Sampling:   4%|█▊                                       |  ETA: 0:00:50Sampling:   5%|█▉                                       |  ETA: 0:00:49Sampling:   5%|██▏                                      |  ETA: 0:00:56Sampling:   6%|██▎                                      |  ETA: 0:00:58Sampling:   6%|██▌                                      |  ETA: 0:01:01Sampling:   6%|██▋                                      |  ETA: 0:01:03Sampling:   7%|██▉                                      |  ETA: 0:01:02Sampling:   7%|███                                      |  ETA: 0:01:01Sampling:   8%|███▎                                     |  ETA: 0:01:00Sampling:   8%|███▌                                     |  ETA: 0:00:58Sampling:   9%|███▋                                     |  ETA: 0:00:58Sampling:   9%|███▉                                     |  ETA: 0:00:58Sampling:  10%|████                                     |  ETA: 0:00:56Sampling:  10%|████▏                                    |  ETA: 0:00:55Sampling:  11%|████▍                                    |  ETA: 0:00:54Sampling:  11%|████▋                                    |  ETA: 0:00:52Sampling:  12%|████▊                                    |  ETA: 0:00:52Sampling:  12%|█████                                    |  ETA: 0:00:51Sampling:  13%|█████▏                                   |  ETA: 0:00:50Sampling:  13%|█████▍                                   |  ETA: 0:00:50Sampling:  14%|█████▌                                   |  ETA: 0:00:49Sampling:  14%|█████▊                                   |  ETA: 0:00:48Sampling:  14%|█████▉                                   |  ETA: 0:00:47Sampling:  15%|██████▏                                  |  ETA: 0:00:47Sampling:  16%|██████▌                                  |  ETA: 0:00:45Sampling:  16%|██████▋                                  |  ETA: 0:00:44Sampling:  17%|██████▉                                  |  ETA: 0:00:43Sampling:  18%|███████▎                                 |  ETA: 0:00:41Sampling:  18%|███████▌                                 |  ETA: 0:00:41Sampling:  19%|███████▋                                 |  ETA: 0:00:41Sampling:  19%|███████▉                                 |  ETA: 0:00:40Sampling:  20%|████████                                 |  ETA: 0:00:39Sampling:  20%|████████▎                                |  ETA: 0:00:39Sampling:  20%|████████▍                                |  ETA: 0:00:38Sampling:  21%|████████▋                                |  ETA: 0:00:38Sampling:  21%|████████▊                                |  ETA: 0:00:37Sampling:  22%|█████████                                |  ETA: 0:00:37Sampling:  23%|█████████▍                               |  ETA: 0:00:35Sampling:  23%|█████████▌                               |  ETA: 0:00:35Sampling:  24%|█████████▉                               |  ETA: 0:00:34Sampling:  25%|██████████▏                              |  ETA: 0:00:33Sampling:  25%|██████████▍                              |  ETA: 0:00:33Sampling:  26%|██████████▊                              |  ETA: 0:00:32Sampling:  27%|██████████▉                              |  ETA: 0:00:32Sampling:  27%|███████████▏                             |  ETA: 0:00:31Sampling:  28%|███████████▎                             |  ETA: 0:00:31Sampling:  28%|███████████▌                             |  ETA: 0:00:30Sampling:  28%|███████████▋                             |  ETA: 0:00:30Sampling:  29%|███████████▉                             |  ETA: 0:00:30Sampling:  29%|████████████                             |  ETA: 0:00:29Sampling:  30%|████████████▎                            |  ETA: 0:00:29Sampling:  30%|████████████▍                            |  ETA: 0:00:29Sampling:  31%|████████████▋                            |  ETA: 0:00:29Sampling:  31%|████████████▊                            |  ETA: 0:00:29Sampling:  32%|█████████████                            |  ETA: 0:00:28Sampling:  32%|█████████████▎                           |  ETA: 0:00:28Sampling:  33%|█████████████▋                           |  ETA: 0:00:27Sampling:  34%|█████████████▊                           |  ETA: 0:00:27Sampling:  34%|██████████████                           |  ETA: 0:00:27Sampling:  34%|██████████████▏                          |  ETA: 0:00:27Sampling:  35%|██████████████▍                          |  ETA: 0:00:27Sampling:  35%|██████████████▌                          |  ETA: 0:00:26Sampling:  36%|██████████████▊                          |  ETA: 0:00:26Sampling:  36%|██████████████▉                          |  ETA: 0:00:26Sampling:  37%|███████████████▏                         |  ETA: 0:00:26Sampling:  37%|███████████████▎                         |  ETA: 0:00:25Sampling:  38%|███████████████▌                         |  ETA: 0:00:25Sampling:  38%|███████████████▋                         |  ETA: 0:00:25Sampling:  39%|███████████████▉                         |  ETA: 0:00:24Sampling:  39%|████████████████▏                        |  ETA: 0:00:24Sampling:  40%|████████████████▎                        |  ETA: 0:00:24Sampling:  40%|████████████████▌                        |  ETA: 0:00:24Sampling:  41%|████████████████▋                        |  ETA: 0:00:24Sampling:  41%|████████████████▊                        |  ETA: 0:00:23Sampling:  42%|█████████████████                        |  ETA: 0:00:23Sampling:  42%|█████████████████▎                       |  ETA: 0:00:23Sampling:  42%|█████████████████▍                       |  ETA: 0:00:23Sampling:  43%|█████████████████▋                       |  ETA: 0:00:23Sampling:  43%|█████████████████▊                       |  ETA: 0:00:22Sampling:  44%|██████████████████                       |  ETA: 0:00:22Sampling:  44%|██████████████████▏                      |  ETA: 0:00:22Sampling:  45%|██████████████████▍                      |  ETA: 0:00:22Sampling:  45%|██████████████████▌                      |  ETA: 0:00:21Sampling:  46%|██████████████████▊                      |  ETA: 0:00:21Sampling:  46%|███████████████████                      |  ETA: 0:00:21Sampling:  47%|███████████████████▏                     |  ETA: 0:00:21Sampling:  47%|███████████████████▎                     |  ETA: 0:00:21Sampling:  48%|███████████████████▌                     |  ETA: 0:00:20Sampling:  48%|███████████████████▋                     |  ETA: 0:00:20Sampling:  48%|███████████████████▉                     |  ETA: 0:00:20Sampling:  49%|████████████████████▏                    |  ETA: 0:00:20Sampling:  49%|████████████████████▎                    |  ETA: 0:00:20Sampling:  50%|████████████████████▌                    |  ETA: 0:00:19Sampling:  50%|████████████████████▋                    |  ETA: 0:00:19Sampling:  51%|████████████████████▉                    |  ETA: 0:00:19Sampling:  51%|█████████████████████                    |  ETA: 0:00:19Sampling:  52%|█████████████████████▎                   |  ETA: 0:00:19Sampling:  52%|█████████████████████▍                   |  ETA: 0:00:18Sampling:  53%|█████████████████████▋                   |  ETA: 0:00:18Sampling:  53%|█████████████████████▊                   |  ETA: 0:00:18Sampling:  54%|██████████████████████                   |  ETA: 0:00:18Sampling:  54%|██████████████████████▏                  |  ETA: 0:00:18Sampling:  55%|██████████████████████▍                  |  ETA: 0:00:17Sampling:  55%|██████████████████████▌                  |  ETA: 0:00:17Sampling:  56%|██████████████████████▊                  |  ETA: 0:00:17Sampling:  56%|███████████████████████                  |  ETA: 0:00:17Sampling:  56%|███████████████████████▏                 |  ETA: 0:00:17Sampling:  57%|███████████████████████▍                 |  ETA: 0:00:16Sampling:  57%|███████████████████████▌                 |  ETA: 0:00:16Sampling:  58%|███████████████████████▊                 |  ETA: 0:00:16Sampling:  58%|███████████████████████▉                 |  ETA: 0:00:16Sampling:  59%|████████████████████████▏                |  ETA: 0:00:16Sampling:  59%|████████████████████████▎                |  ETA: 0:00:16Sampling:  60%|████████████████████████▌                |  ETA: 0:00:15Sampling:  60%|████████████████████████▋                |  ETA: 0:00:15Sampling:  61%|████████████████████████▉                |  ETA: 0:00:15Sampling:  61%|█████████████████████████                |  ETA: 0:00:15Sampling:  62%|█████████████████████████▍               |  ETA: 0:00:14Sampling:  62%|█████████████████████████▋               |  ETA: 0:00:14Sampling:  63%|█████████████████████████▉               |  ETA: 0:00:14Sampling:  63%|██████████████████████████               |  ETA: 0:00:14Sampling:  64%|██████████████████████████▎              |  ETA: 0:00:14Sampling:  64%|██████████████████████████▍              |  ETA: 0:00:13Sampling:  65%|██████████████████████████▋              |  ETA: 0:00:13Sampling:  65%|██████████████████████████▊              |  ETA: 0:00:13Sampling:  66%|███████████████████████████              |  ETA: 0:00:13Sampling:  66%|███████████████████████████▏             |  ETA: 0:00:13Sampling:  67%|███████████████████████████▍             |  ETA: 0:00:12Sampling:  67%|███████████████████████████▌             |  ETA: 0:00:12Sampling:  68%|███████████████████████████▊             |  ETA: 0:00:12Sampling:  68%|███████████████████████████▉             |  ETA: 0:00:12Sampling:  69%|████████████████████████████▎            |  ETA: 0:00:11Sampling:  70%|████████████████████████████▌            |  ETA: 0:00:11Sampling:  70%|████████████████████████████▊            |  ETA: 0:00:11Sampling:  70%|████████████████████████████▉            |  ETA: 0:00:11Sampling:  71%|█████████████████████████████▏           |  ETA: 0:00:11Sampling:  71%|█████████████████████████████▎           |  ETA: 0:00:10Sampling:  72%|█████████████████████████████▌           |  ETA: 0:00:10Sampling:  72%|█████████████████████████████▋           |  ETA: 0:00:10Sampling:  73%|█████████████████████████████▉           |  ETA: 0:00:10Sampling:  73%|██████████████████████████████           |  ETA: 0:00:10Sampling:  74%|██████████████████████████████▎          |  ETA: 0:00:10Sampling:  74%|██████████████████████████████▍          |  ETA: 0:00:09Sampling:  75%|██████████████████████████████▋          |  ETA: 0:00:09Sampling:  75%|██████████████████████████████▊          |  ETA: 0:00:09Sampling:  76%|███████████████████████████████          |  ETA: 0:00:09Sampling:  76%|███████████████████████████████▏         |  ETA: 0:00:09Sampling:  76%|███████████████████████████████▍         |  ETA: 0:00:09Sampling:  77%|███████████████████████████████▊         |  ETA: 0:00:08Sampling:  78%|████████████████████████████████         |  ETA: 0:00:08Sampling:  78%|████████████████████████████████▏        |  ETA: 0:00:08Sampling:  79%|████████████████████████████████▎        |  ETA: 0:00:08Sampling:  79%|████████████████████████████████▌        |  ETA: 0:00:08Sampling:  80%|████████████████████████████████▊        |  ETA: 0:00:07Sampling:  80%|████████████████████████████████▉        |  ETA: 0:00:07Sampling:  81%|█████████████████████████████████▏       |  ETA: 0:00:07Sampling:  81%|█████████████████████████████████▎       |  ETA: 0:00:07Sampling:  82%|█████████████████████████████████▌       |  ETA: 0:00:07Sampling:  82%|█████████████████████████████████▋       |  ETA: 0:00:06Sampling:  83%|█████████████████████████████████▉       |  ETA: 0:00:06Sampling:  83%|██████████████████████████████████       |  ETA: 0:00:06Sampling:  84%|██████████████████████████████████▎      |  ETA: 0:00:06Sampling:  84%|██████████████████████████████████▌      |  ETA: 0:00:06Sampling:  84%|██████████████████████████████████▋      |  ETA: 0:00:06Sampling:  85%|██████████████████████████████████▊      |  ETA: 0:00:05Sampling:  86%|███████████████████████████████████▏     |  ETA: 0:00:05Sampling:  86%|███████████████████████████████████▍     |  ETA: 0:00:05Sampling:  87%|███████████████████████████████████▋     |  ETA: 0:00:05Sampling:  87%|███████████████████████████████████▊     |  ETA: 0:00:05Sampling:  88%|████████████████████████████████████     |  ETA: 0:00:04Sampling:  88%|████████████████████████████████████▏    |  ETA: 0:00:04Sampling:  89%|████████████████████████████████████▍    |  ETA: 0:00:04Sampling:  90%|████████████████████████████████████▊    |  ETA: 0:00:04Sampling:  90%|████████████████████████████████████▉    |  ETA: 0:00:04Sampling:  90%|█████████████████████████████████████▏   |  ETA: 0:00:03Sampling:  91%|█████████████████████████████████████▌   |  ETA: 0:00:03Sampling:  92%|█████████████████████████████████████▋   |  ETA: 0:00:03Sampling:  92%|█████████████████████████████████████▉   |  ETA: 0:00:03Sampling:  93%|██████████████████████████████████████   |  ETA: 0:00:03Sampling:  93%|██████████████████████████████████████▎  |  ETA: 0:00:02Sampling:  94%|██████████████████████████████████████▌  |  ETA: 0:00:02Sampling:  94%|██████████████████████████████████████▋  |  ETA: 0:00:02Sampling:  95%|██████████████████████████████████████▉  |  ETA: 0:00:02Sampling:  95%|███████████████████████████████████████  |  ETA: 0:00:02Sampling:  96%|███████████████████████████████████████▎ |  ETA: 0:00:02Sampling:  97%|███████████████████████████████████████▋ |  ETA: 0:00:01Sampling:  97%|███████████████████████████████████████▊ |  ETA: 0:00:01Sampling:  98%|████████████████████████████████████████ |  ETA: 0:00:01Sampling:  98%|████████████████████████████████████████▏|  ETA: 0:00:01Sampling:  98%|████████████████████████████████████████▍|  ETA: 0:00:01Sampling:  99%|████████████████████████████████████████▌|  ETA: 0:00:00Sampling:  99%|████████████████████████████████████████▊|  ETA: 0:00:00Sampling:  99%|████████████████████████████████████████▉|  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:35\n\n\n\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           b    0.0546    0.0070    0.0006   142.3672   246.8651    1.0063     ⋯\n           ā    0.0032    0.0055    0.0003   244.7927   360.4686    0.9995     ⋯\n        a[1]    0.0006    0.0003    0.0000   138.6061   241.4863    1.0037     ⋯\n        a[2]    0.0010    0.0005    0.0000   137.3028   231.4561    1.0049     ⋯\n        a[3]    0.0017    0.0007    0.0001   142.5674   228.3792    1.0042     ⋯\n           c    0.0068    0.0023    0.0002   169.0722   366.3223    0.9991     ⋯\n           k    2.8094    1.6466    0.1453   114.4244   208.0089    1.0187     ⋯\n                                                                1 column omitted\n\n\n\n\nPRECIS(DataFrame(chain4))\n\nrisk_factors4 = [mean(chain4[Symbol(\"a[$f]\")]) for f in 1:3]\n\n3-element Vector{Float64}:\n 0.0006251049456225291\n 0.000990412265130522\n 0.0016772235717042031\n\n\n\nlet data = data2\n\n    data_weight = data.exposures ./ sum(data.exposures)\n    data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n    color_i = data.risklevel\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        color=color_i,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        xlabel=\"age\",\n        ylim=(0.0, 0.25),\n        ylabel=\"mortality rate\",\n        title=\"Parametric Bayseian Mortality\"\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 100\n\n    ages = sort!(unique(data.att_age))\n    for r in 1:3\n        for i in 1:n\n            s = sample(chain4, 1)\n            a = only(s[Symbol(\"a[$r]\")])\n            b = only(s[:b])\n            k = only(s[:k])\n            c = 0\n            m = MortalityTables.MakehamBeard(; a, b, c, k)\n            if i == 1\n                plot!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"risk level $r\", alpha=0.2, color=r)\n            else\n                plot!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"\", alpha=0.2, color=r)\n            end\n        end\n    end\n    p\nend"
  },
  {
    "objectID": "posts/bayesian-claims-demo/index.html#predictions",
    "href": "posts/bayesian-claims-demo/index.html#predictions",
    "title": "Bayesian Markov-Chain-Monte-Carlo and Claims Data",
    "section": "Predictions",
    "text": "Predictions\nWe can generate predictive estimates by passing a vector of missing in place of the outcome variables and then calling predict.\nWe get a table of values where each row is the the prediction implied by the corresponding chain sample, and the columns are the predicted value for each of the outcomes in our original dataset.\n\npreds = predict(mortality4(data2, fill(missing, length(data2.deaths))), chain4)\n\n\nChains MCMC chain (1000×531×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = deaths[1], deaths[2], deaths[3], deaths[4], deaths[5], deaths[6], deaths[7], deaths[8], deaths[9], deaths[10], deaths[11], deaths[12], deaths[13], deaths[14], deaths[15], deaths[16], deaths[17], deaths[18], deaths[19], deaths[20], deaths[21], deaths[22], deaths[23], deaths[24], deaths[25], deaths[26], deaths[27], deaths[28], deaths[29], deaths[30], deaths[31], deaths[32], deaths[33], deaths[34], deaths[35], deaths[36], deaths[37], deaths[38], deaths[39], deaths[40], deaths[41], deaths[42], deaths[43], deaths[44], deaths[45], deaths[46], deaths[47], deaths[48], deaths[49], deaths[50], deaths[51], deaths[52], deaths[53], deaths[54], deaths[55], deaths[56], deaths[57], deaths[58], deaths[59], deaths[60], deaths[61], deaths[62], deaths[63], deaths[64], deaths[65], deaths[66], deaths[67], deaths[68], deaths[69], deaths[70], deaths[71], deaths[72], deaths[73], deaths[74], deaths[75], deaths[76], deaths[77], deaths[78], deaths[79], deaths[80], deaths[81], deaths[82], deaths[83], deaths[84], deaths[85], deaths[86], deaths[87], deaths[88], deaths[89], deaths[90], deaths[91], deaths[92], deaths[93], deaths[94], deaths[95], deaths[96], deaths[97], deaths[98], deaths[99], deaths[100], deaths[101], deaths[102], deaths[103], deaths[104], deaths[105], deaths[106], deaths[107], deaths[108], deaths[109], deaths[110], deaths[111], deaths[112], deaths[113], deaths[114], deaths[115], deaths[116], deaths[117], deaths[118], deaths[119], deaths[120], deaths[121], deaths[122], deaths[123], deaths[124], deaths[125], deaths[126], deaths[127], deaths[128], deaths[129], deaths[130], deaths[131], deaths[132], deaths[133], deaths[134], deaths[135], deaths[136], deaths[137], deaths[138], deaths[139], deaths[140], deaths[141], deaths[142], deaths[143], deaths[144], deaths[145], deaths[146], deaths[147], deaths[148], deaths[149], deaths[150], deaths[151], deaths[152], deaths[153], deaths[154], deaths[155], deaths[156], deaths[157], deaths[158], deaths[159], deaths[160], deaths[161], deaths[162], deaths[163], deaths[164], deaths[165], deaths[166], deaths[167], deaths[168], deaths[169], deaths[170], deaths[171], deaths[172], deaths[173], deaths[174], deaths[175], deaths[176], deaths[177], deaths[178], deaths[179], deaths[180], deaths[181], deaths[182], deaths[183], deaths[184], deaths[185], deaths[186], deaths[187], deaths[188], deaths[189], deaths[190], deaths[191], deaths[192], deaths[193], deaths[194], deaths[195], deaths[196], deaths[197], deaths[198], deaths[199], deaths[200], deaths[201], deaths[202], deaths[203], deaths[204], deaths[205], deaths[206], deaths[207], deaths[208], deaths[209], deaths[210], deaths[211], deaths[212], deaths[213], deaths[214], deaths[215], deaths[216], deaths[217], deaths[218], deaths[219], deaths[220], deaths[221], deaths[222], deaths[223], deaths[224], deaths[225], deaths[226], deaths[227], deaths[228], deaths[229], deaths[230], deaths[231], deaths[232], deaths[233], deaths[234], deaths[235], deaths[236], deaths[237], deaths[238], deaths[239], deaths[240], deaths[241], deaths[242], deaths[243], deaths[244], deaths[245], deaths[246], deaths[247], deaths[248], deaths[249], deaths[250], deaths[251], deaths[252], deaths[253], deaths[254], deaths[255], deaths[256], deaths[257], deaths[258], deaths[259], deaths[260], deaths[261], deaths[262], deaths[263], deaths[264], deaths[265], deaths[266], deaths[267], deaths[268], deaths[269], deaths[270], deaths[271], deaths[272], deaths[273], deaths[274], deaths[275], deaths[276], deaths[277], deaths[278], deaths[279], deaths[280], deaths[281], deaths[282], deaths[283], deaths[284], deaths[285], deaths[286], deaths[287], deaths[288], deaths[289], deaths[290], deaths[291], deaths[292], deaths[293], deaths[294], deaths[295], deaths[296], deaths[297], deaths[298], deaths[299], deaths[300], deaths[301], deaths[302], deaths[303], deaths[304], deaths[305], deaths[306], deaths[307], deaths[308], deaths[309], deaths[310], deaths[311], deaths[312], deaths[313], deaths[314], deaths[315], deaths[316], deaths[317], deaths[318], deaths[319], deaths[320], deaths[321], deaths[322], deaths[323], deaths[324], deaths[325], deaths[326], deaths[327], deaths[328], deaths[329], deaths[330], deaths[331], deaths[332], deaths[333], deaths[334], deaths[335], deaths[336], deaths[337], deaths[338], deaths[339], deaths[340], deaths[341], deaths[342], deaths[343], deaths[344], deaths[345], deaths[346], deaths[347], deaths[348], deaths[349], deaths[350], deaths[351], deaths[352], deaths[353], deaths[354], deaths[355], deaths[356], deaths[357], deaths[358], deaths[359], deaths[360], deaths[361], deaths[362], deaths[363], deaths[364], deaths[365], deaths[366], deaths[367], deaths[368], deaths[369], deaths[370], deaths[371], deaths[372], deaths[373], deaths[374], deaths[375], deaths[376], deaths[377], deaths[378], deaths[379], deaths[380], deaths[381], deaths[382], deaths[383], deaths[384], deaths[385], deaths[386], deaths[387], deaths[388], deaths[389], deaths[390], deaths[391], deaths[392], deaths[393], deaths[394], deaths[395], deaths[396], deaths[397], deaths[398], deaths[399], deaths[400], deaths[401], deaths[402], deaths[403], deaths[404], deaths[405], deaths[406], deaths[407], deaths[408], deaths[409], deaths[410], deaths[411], deaths[412], deaths[413], deaths[414], deaths[415], deaths[416], deaths[417], deaths[418], deaths[419], deaths[420], deaths[421], deaths[422], deaths[423], deaths[424], deaths[425], deaths[426], deaths[427], deaths[428], deaths[429], deaths[430], deaths[431], deaths[432], deaths[433], deaths[434], deaths[435], deaths[436], deaths[437], deaths[438], deaths[439], deaths[440], deaths[441], deaths[442], deaths[443], deaths[444], deaths[445], deaths[446], deaths[447], deaths[448], deaths[449], deaths[450], deaths[451], deaths[452], deaths[453], deaths[454], deaths[455], deaths[456], deaths[457], deaths[458], deaths[459], deaths[460], deaths[461], deaths[462], deaths[463], deaths[464], deaths[465], deaths[466], deaths[467], deaths[468], deaths[469], deaths[470], deaths[471], deaths[472], deaths[473], deaths[474], deaths[475], deaths[476], deaths[477], deaths[478], deaths[479], deaths[480], deaths[481], deaths[482], deaths[483], deaths[484], deaths[485], deaths[486], deaths[487], deaths[488], deaths[489], deaths[490], deaths[491], deaths[492], deaths[493], deaths[494], deaths[495], deaths[496], deaths[497], deaths[498], deaths[499], deaths[500], deaths[501], deaths[502], deaths[503], deaths[504], deaths[505], deaths[506], deaths[507], deaths[508], deaths[509], deaths[510], deaths[511], deaths[512], deaths[513], deaths[514], deaths[515], deaths[516], deaths[517], deaths[518], deaths[519], deaths[520], deaths[521], deaths[522], deaths[523], deaths[524], deaths[525], deaths[526], deaths[527], deaths[528], deaths[529], deaths[530], deaths[531]\ninternals         = \nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n   deaths[1]    8.5150    2.9069    0.0939    958.4763   1008.7639    0.9994   ⋯\n   deaths[2]   11.6930    3.4512    0.1055   1072.8987    944.2409    1.0000   ⋯\n   deaths[3]   14.9880    3.9562    0.1225   1048.3469    977.2964    0.9991   ⋯\n   deaths[4]    8.0010    2.8958    0.0951    926.2622    797.4456    1.0013   ⋯\n   deaths[5]   11.7660    3.3729    0.1069    975.5156    810.6114    0.9997   ⋯\n   deaths[6]   14.7840    3.8184    0.1248    932.4117    903.1870    1.0006   ⋯\n   deaths[7]    7.9830    2.9277    0.0927    985.8603    922.8455    1.0025   ⋯\n   deaths[8]   11.6140    3.4174    0.1094    973.3146    921.1675    1.0029   ⋯\n   deaths[9]   14.6820    3.9085    0.1275    930.6639    907.1682    0.9990   ⋯\n  deaths[10]    7.8740    2.9139    0.0891   1059.0651    958.9856    0.9999   ⋯\n  deaths[11]   11.1530    3.3655    0.1084    960.5418    894.2850    1.0015   ⋯\n  deaths[12]   13.8960    3.7610    0.1250    892.1812    914.3382    0.9990   ⋯\n  deaths[13]    7.6650    2.8829    0.0904   1022.3612    883.8790    1.0012   ⋯\n  deaths[14]   11.1400    3.3325    0.1078    945.1535    935.6795    1.0004   ⋯\n  deaths[15]   13.7080    3.6903    0.1108   1104.3195   1009.4330    0.9998   ⋯\n  deaths[16]    7.6550    2.7676    0.0897    946.8631   1036.7581    1.0032   ⋯\n  deaths[17]   11.0460    3.3288    0.1039   1033.3326    929.5685    1.0026   ⋯\n  deaths[18]   13.2610    3.6838    0.1276    812.4458    911.9378    1.0011   ⋯\n  deaths[19]    7.3600    2.6825    0.0829   1036.1341    961.6809    1.0000   ⋯\n  deaths[20]   10.7340    3.2714    0.1187    761.5744    924.5419    1.0014   ⋯\n  deaths[21]   12.3820    3.5616    0.1335    779.6126    668.9713    0.9994   ⋯\n  deaths[22]    7.0350    2.7399    0.0866    993.2216    778.8220    0.9993   ⋯\n  deaths[23]   10.2330    3.1621    0.1049    913.8614    901.2260    1.0002   ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮           ⋮          ⋮      ⋱\n                                                   1 column and 508 rows omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n   deaths[1]    3.0000    7.0000    8.0000   10.0000   15.0000\n   deaths[2]    6.0000    9.0000   12.0000   14.0000   19.0000\n   deaths[3]    8.0000   12.0000   15.0000   18.0000   23.0000\n   deaths[4]    3.0000    6.0000    8.0000   10.0000   14.0000\n   deaths[5]    6.0000    9.0000   12.0000   14.0000   19.0000\n   deaths[6]    8.0000   12.0000   15.0000   17.0000   22.0250\n   deaths[7]    3.0000    6.0000    8.0000   10.0000   14.0000\n   deaths[8]    5.0000    9.0000   11.0000   14.0000   18.0000\n   deaths[9]    7.0000   12.0000   15.0000   17.0000   22.0000\n  deaths[10]    3.0000    6.0000    8.0000   10.0000   14.0000\n  deaths[11]    5.0000    9.0000   11.0000   13.0000   18.0000\n  deaths[12]    7.0000   11.0000   14.0000   16.0000   22.0000\n  deaths[13]    3.0000    6.0000    7.5000    9.0000   14.0000\n  deaths[14]    5.0000    9.0000   11.0000   13.0000   18.0000\n  deaths[15]    7.0000   11.0000   13.0000   16.0000   21.0000\n  deaths[16]    3.0000    6.0000    8.0000    9.0000   13.0250\n  deaths[17]    5.0000    9.0000   11.0000   13.0000   18.0000\n  deaths[18]    7.0000   11.0000   13.0000   16.0000   21.0000\n  deaths[19]    3.0000    6.0000    7.0000    9.0000   13.0000\n  deaths[20]    5.0000    8.0000   11.0000   13.0000   17.0250\n  deaths[21]    6.0000   10.0000   12.0000   15.0000   20.0000\n  deaths[22]    2.0000    5.0000    7.0000    9.0000   13.0000\n  deaths[23]    4.0000    8.0000   10.0000   12.0000   17.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                508 rows omitted"
  },
  {
    "objectID": "posts/treasury-interactive/index.html",
    "href": "posts/treasury-interactive/index.html",
    "title": "US Treasury Comparison Tool",
    "section": "",
    "text": "The screenshot above shows a Pluto.jl notebook where you can select a pair of dates and find the change in the treasury curve’s zero rates."
  },
  {
    "objectID": "posts/treasury-interactive/index.html#instructions-to-run",
    "href": "posts/treasury-interactive/index.html#instructions-to-run",
    "title": "US Treasury Comparison Tool",
    "section": "Instructions to Run",
    "text": "Instructions to Run\nBecause JuliaActuary doesn’t have an active server to run this on, you have to run it locally. Assuming that you already have Julia installed but still need to install Pluto notebooks:\n\nOpen a Julia REPL and copy and paste the following:\n\n\n# install these dependencies\nimport Pkg; Pkg.add([\"Pluto\"]) \n\n# use and start Pluto\nusing Pluto; Pluto.run()\n\nIn the Pluto window that opens, enter this URL into the Open from file: box:\n\nhttps://raw.githubusercontent.com/JuliaActuary/Learn/master/USTreasuryComparison.jl"
  },
  {
    "objectID": "posts/cashflow-interactive/index.html",
    "href": "posts/cashflow-interactive/index.html",
    "title": "Interactive Cashflow Analysis with ActuaryUtilities.jl",
    "section": "",
    "text": "Scroll down below the sample video to see how to run it.\nThe recording above shows a Pluto.jl notebook demonstrating some of the basic cashflow-oriented features in ActuaryUtilities.jl and Yields.jl."
  },
  {
    "objectID": "posts/cashflow-interactive/index.html#instructions-to-run",
    "href": "posts/cashflow-interactive/index.html#instructions-to-run",
    "title": "Interactive Cashflow Analysis with ActuaryUtilities.jl",
    "section": "Instructions to Run",
    "text": "Instructions to Run\nBecause JuliaActuary doesn’t have an active server to run this on, you have to run it locally. Assuming that you already have Julia installed but still need to install Pluto notebooks:\n\nOpen a Julia REPL and copy and paste the following:\n\n# install these dependencies\nimport Pkg; Pkg.add([\"Pluto\"]) \n\n# use and start Pluto\nusing Pluto; Pluto.run()\n\nIn the Pluto window that opens, enter this URL into the Open from file: box:\n\nhttps://raw.githubusercontent.com/JuliaActuary/Learn/master/CashflowAnalysis.jl"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nInteractive Cashflow Analysis with ActuaryUtilities.jl\n\n\n\n\n\n\nactuaryutilities\n\n\nfinancemodels\n\n\nassets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversal Life Policy Account Mechanics as a Differential Equation\n\n\n\n\n\n\nmodeling\n\n\ndiffeq\n\n\nmortalitytables\n\n\nactuaryutilities\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUS Treasury Comparison Tool\n\n\n\n\n\n\nmodeling\n\n\nfinancemodels\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Markov-Chain-Monte-Carlo and Claims Data\n\n\n\n\n\n\nmortalitytables\n\n\nexposures\n\n\nexperience-analysis\n\n\ndataframes\n\n\ntutorial\n\n\nstatistics\n\n\nbayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested Projection Mechanics\n\n\n\n\n\n\nmodeling\n\n\nbenchmark\n\n\nactuaryutilities\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing MortaltiyTables.jl with DataFrames\n\n\n\n\n\n\nmortalitytables\n\n\ndataframes\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson approximation to Binomial\n\n\n\n\n\n\nmodeling\n\n\nstatistics\n\n\nexperience-analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian vs Limited Fluctuation Experience Analysis\n\n\n\n\n\n\nmodeling\n\n\nstatistics\n\n\nexperience-analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting Rate Data to Yield Curves\n\n\n\n\n\n\nmarket\n\n\nyieldcurves\n\n\nfinancemodels\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting survival data with MortaltityTables.jl\n\n\n\n\n\n\nsurvival\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive AAA Economic Scenario Generator\n\n\n\n\n\n\nmodeling\n\n\nscenario-generator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic claims projections demo\n\n\n\n\n\n\nmodeling\n\n\nbenchmark\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExposure Calculation with ExperienceAnalysis.jl\n\n\n\n\n\n\nmortalitytables\n\n\nexposures\n\n\nexperience-analysis\n\n\ndataframes\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplicating the AAA equtity generator\n\n\n\n\n\n\nmodeling\n\n\nscenario-generator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Mortality Comparison Tool\n\n\n\n\n\n\nexperience-analysis\n\n\nmortalitytables\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "community.html",
    "href": "community.html",
    "title": "Learn",
    "section": "",
    "text": "Ask questions or suggest ideas\n\n\n\nIf you have other ideas or questions, join the JuliaActuary Github Discussions. Or come say hello on the community Zulip or Slack #actuary channel. We welcome all actuarial and related disciplines!\n\n\n\nYou can also access help text when using the packages in the REPL by activating help mode, e.g.:\njulia&gt; ? survival\n    survival(mortality_vector,to_age)\n    survival(mortality_vector,from_age,to_age)\n\n\n  Returns the survival through attained age to_age. The start of the \n  calculation is either the start of the vector, or attained age `from_age` \n  and `to_age` need to be Integers. \n\n  Add a DeathDistribution as the last argument to handle floating point \n  and non-whole ages:\n\n    survival(mortality_vector,to_age,::DeathDistribution)\n    survival(mortality_vector,from_age,to_age,::DeathDistribution)\n\n\n  If given a negative to_age, it will return 1.0. Aside from simplifying the code, \n  this makes sense as for something to exist in order to decrement in the first place, \n  it must have existed and survived to the point of being able to be decremented.\n\n  Examples\n  ≡≡≡≡≡≡≡≡≡≡\n\n  julia&gt; qs = UltimateMortality([0.1,0.3,0.6,1]);\n\n  julia&gt; survival(qs,0)\n  1.0\n  julia&gt; survival(qs,1)\n  0.9\n\n  julia&gt; survival(qs,1,1)\n  1.0\n  julia&gt; survival(qs,1,2)\n  0.7\n\n  julia&gt; survival(qs,0.5,Uniform())\n  0.95"
  },
  {
    "objectID": "community.html#get-help",
    "href": "community.html#get-help",
    "title": "Learn",
    "section": "",
    "text": "Ask questions or suggest ideas\n\n\n\nIf you have other ideas or questions, join the JuliaActuary Github Discussions. Or come say hello on the community Zulip or Slack #actuary channel. We welcome all actuarial and related disciplines!\n\n\n\nYou can also access help text when using the packages in the REPL by activating help mode, e.g.:\njulia&gt; ? survival\n    survival(mortality_vector,to_age)\n    survival(mortality_vector,from_age,to_age)\n\n\n  Returns the survival through attained age to_age. The start of the \n  calculation is either the start of the vector, or attained age `from_age` \n  and `to_age` need to be Integers. \n\n  Add a DeathDistribution as the last argument to handle floating point \n  and non-whole ages:\n\n    survival(mortality_vector,to_age,::DeathDistribution)\n    survival(mortality_vector,from_age,to_age,::DeathDistribution)\n\n\n  If given a negative to_age, it will return 1.0. Aside from simplifying the code, \n  this makes sense as for something to exist in order to decrement in the first place, \n  it must have existed and survived to the point of being able to be decremented.\n\n  Examples\n  ≡≡≡≡≡≡≡≡≡≡\n\n  julia&gt; qs = UltimateMortality([0.1,0.3,0.6,1]);\n\n  julia&gt; survival(qs,0)\n  1.0\n  julia&gt; survival(qs,1)\n  0.9\n\n  julia&gt; survival(qs,1,1)\n  1.0\n  julia&gt; survival(qs,1,2)\n  0.7\n\n  julia&gt; survival(qs,0.5,Uniform())\n  0.95"
  },
  {
    "objectID": "community.html#contributing",
    "href": "community.html#contributing",
    "title": "Learn",
    "section": "Contributing",
    "text": "Contributing\n\nContribute code or report issues.\n\n&lt;mark&gt;Thank you&lt;/mark&gt; for your interest in modern actuarial solutions, no matter how you participate in the community.&lt;/p&gt;\n\nPull Requests\nJuliaActuary is open source; you are free to modify, use, or change your copy of the code - but if you make enhancements please consider opening a pull request (basic walkthrough here). Beginners are welcome and we can help with your first pull request!\n\n\nIssues\nIf you find issues, please open an issue on the relevant package’s repository and we will try and address it as soon as possible.\n\n\nProject Board\nSee the Good first Issues project board on Github for simple, self-contained ways to contribute such as adding small new features, improving the documentation, or writing up a tutorial on how to do something simple!\n\n\nCreating Examples, Tutorials, or Blog Posts\nThis site is a basic Quarto website. New content can be added by creating a pull request with a new page in the posts folder. A blog post is distinguished from the other examples content by the simple virtue of having the blog category field in the topmatter of the .qmd file."
  },
  {
    "objectID": "community.html#other-inquiries",
    "href": "community.html#other-inquiries",
    "title": "Learn",
    "section": "Other Inquiries",
    "text": "Other Inquiries\nFor more directed inquires, please send email to inquiry@JuliaActuary.org."
  },
  {
    "objectID": "community.html#share",
    "href": "community.html#share",
    "title": "Learn",
    "section": "Share",
    "text": "Share\nFollow JuliaActuary on LinkedIn for updates and to share with colleagues!"
  },
  {
    "objectID": "posts/finance-models-announcement/index.html",
    "href": "posts/finance-models-announcement/index.html",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "",
    "text": "Yields.jl has evolved into FinanceModels.jl. The benefits are:\nThis blog post describes the conceptual overview and motivation for the change."
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#finance-models-overview",
    "href": "posts/finance-models-announcement/index.html#finance-models-overview",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "Finance Models Overview",
    "text": "Finance Models Overview\nFinanceModels.jl provides a set of composable contracts, models, and functions that allow for modeling of both simple and complex financial instruments. The resulting models, such as discount rates or term structures, can then be used across the JuliaActuary ecosystem to perform actuarial and financial analysis.\n\n\n\nA conceptual sketch of FinanceModels.jl"
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#cashflow---a-fundamental-financial-type",
    "href": "posts/finance-models-announcement/index.html#cashflow---a-fundamental-financial-type",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "1. Cashflow - a fundamental financial type",
    "text": "1. Cashflow - a fundamental financial type\nSay you wanted to model a contract that paid quarterly payments, and those payments occurred starting 15 days from the valuation date (first payment time = 15/365 = 0.057)\nPreviously, you had two options:\n\nChoose a discrete timestep to model (e.g. monthly, quarterly, annual) and then lump the cashflows into those timesteps. E.g. with monthly timesteps of a unit payment of our contract, it might look like: [1,0,0,1,0,0...]\nKeep track of two vectors: one for the payment and one for the times. In this case, that might look like: cfs = [1,1,...];times = [0.057, 0.307...]\n\nThe former has inaccuracies due to the simplified timing and logical complication related to mapping the contracts natural periodicity into an arbitrary modeling choice. The latter becomes unwieldy and fails to take advantage of Julia’s type system.\nThe new solution: Cashflows. Our example above would become: [Cashflow(1,0.057), Cashflow(1,0.307),...]"
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#contracts---a-composable-way-to-represent-financial-instruments",
    "href": "posts/finance-models-announcement/index.html#contracts---a-composable-way-to-represent-financial-instruments",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "2. Contracts - A composable way to represent financial instruments",
    "text": "2. Contracts - A composable way to represent financial instruments\nContracts are a composable way to represent financial instruments. They are, in essence, anything that is a collection of cashflows. Contracts can be combined to represent more complex instruments. For example, a bond can be represented as a collection of cashflows that correspond to the coupon payments and the principal repayment.\nExamples:\n\na Cashflow\nBonds:\n\nBond.Fixed, Bond.Floating\n\nOptions:\n\nOption.EuroCall and Option.EuroPut\n\nCompositional contracts:\n\nForwardto represent an instrument that is relative to a forward point in time.\nComposite to represent the combination of two other instruments.\n\n\nIn the future, this notion may be extended to liabilities (e.g. insurance policies in LifeContingencies.jl)\n\nCreating a new Contract\nA contract is anything that creates a vector of Cashflows when collected. For example, let’s create a bond which only pays down principle and offers no coupons.\nusing FinanceModels,FinanceCore\n\n# Transducers is used to provide a more powerful, composable way to construct collections than the basic iteration interface\nusing Transducers: __foldl__, @next, complete\n\n\"\"\"\nA bond which pays down its par (one unit) in equal payments. \n\"\"\"\nstruct PrincipalOnlyBond{F&lt;:FinanceCore.Frequency} &lt;: FinanceModels.Bond.AbstractBond\n    frequency::F\n    maturity::Float64\nend\n\n# We extend the interface to say what should happen as the bond is projected\n# There's two parts to customize:\n# 1. any initialization or state to keep track of\n# 2. The loop where we decide what gets returned at each timestep\nfunction Transducers.__foldl__(rf, val, p::Projection{C,M,K}) where {C&lt;:PrincipalOnlyBond,M,K}\n    # initialization stuff\n    b = p.contract # the contract within a projection\n    ts = Bond.coupon_times(b) # works since it's a FinanceModels.Bond.AbstractBond with a frequency and maturity\n    pmt = 1 / length(ts)\n\n    for t in ts\n        # the loop which returns a value\n        cf = Cashflow(pmt, t)\n        val = @next(rf, val, cf) # the value to return is the last argument\n    end\n    return complete(rf, val)\nend\nThat’s it! then we can use this contract to fitting models, create projections, quotes, etc. Here we simply collect the bond into an array of cashflows:\njulia&gt; PrincipalOnlyBond(Periodic(2),5.) |&gt; collect\n10-element Vector{Cashflow{Float64, Float64}}:\n Cashflow{Float64, Float64}(0.1, 0.5)\n Cashflow{Float64, Float64}(0.1, 1.0)\n Cashflow{Float64, Float64}(0.1, 1.5)\n Cashflow{Float64, Float64}(0.1, 2.0)\n Cashflow{Float64, Float64}(0.1, 2.5)\n Cashflow{Float64, Float64}(0.1, 3.0)\n Cashflow{Float64, Float64}(0.1, 3.5)\n Cashflow{Float64, Float64}(0.1, 4.0)\n Cashflow{Float64, Float64}(0.1, 4.5)\n Cashflow{Float64, Float64}(0.1, 5.0)\nNote that all contracts in FinanceModels.jl are currently unit contracts in that they assume a unit par value. Scale assets down to unit values before constructing the default contracts.\n\nMore complex Contracts\nWhen the cashflow depends on a model. An example of this is a floating bond where the coupon paid depends on a view of forward rates. See Section 6 - Projections for how this is handled."
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#quotes---the-observed-price-we-need-to-fit-a-model-to",
    "href": "posts/finance-models-announcement/index.html#quotes---the-observed-price-we-need-to-fit-a-model-to",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "3. Quotes - The observed price we need to fit a model to",
    "text": "3. Quotes - The observed price we need to fit a model to\nQuotes are the observed prices that we need to fit a model to. They represent the market prices of financial instruments, such as bonds or swaps. In the context of the package, a quote is defined as a pair of a contract and a price.\nFor example, a par yield bond paying a 4% coupon (paid as 2% twice per annum) implies a price at par (i.e. 1.0):\njulia&gt; ParYield(Periodic(0.04,2),10)\nQuote{Float64, FinanceModels.Bond.Fixed{Periodic, Float64, Int64}}(\n1.0, \nFinanceModels.Bond.Fixed{Periodic, Float64, Int64}(0.040000000000000036, Periodic(2), 10))\n\nA number of convenience functions are included to construct a Quote:\n\nZCBPrice and ZCBYield\nParYield\nCMTYield\nOISYield\nForwardYields"
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#models---not-just-yield-curves-anymore",
    "href": "posts/finance-models-announcement/index.html#models---not-just-yield-curves-anymore",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "4. Models - Not just yield curves anymore",
    "text": "4. Models - Not just yield curves anymore\n\nYield Curves: all of Yields.jl yield models are included in the initial FinanceModels.jl release\nEquities and Options: The initial release includes BlackScholesMerton option pricing and one can use constant or spline volatility models\nOthers more to come in the future\n\n\nCreating a new model\nHere we’ll do a complete implementation of a yield curve model where the discount rate is approximated by a straight line (often called an AB line from the y=ax+b formula.\n using FinanceModels, FinanceCore\n using AccessibleOptimization \n using IntervalSets\n \nstruct ABDiscountLine{A} &lt;: FinanceModels.Yield.AbstractYieldModel\n    a::A\n    b::A\nend\n\nABDiscountLine() = ABDiscountLine(0.,0.)\n\nfunction FinanceCore.discount(m::ABDiscountLine,t)\n    #discount rate is approximated by a straight line, floored at 0.0 and capped at 1.0\n    clamp(m.a*t + m.b, 0.0,1.0) \nend\n\n\n# `@optic` indicates what in our model variables needs to be updated (from AccessibleOptimization.jl)\n# `-1.0 .. 1.0` says to bound the search from negative to positive one (from IntervalSets.jl)\nFinanceModels.__default_optic(m::ABDiscountLine) = OptArgs([\n    @optic(_.a) =&gt; -1.0 .. 1.0,\n    @optic(_.b) =&gt; -1.0 .. 1.0,\n]...)\n\nquotes = ZCBPrice([0.9, 0.8, 0.7,0.6])\n\nm = fit(ABDiscountLine(),quotes)\nNow, m is a model like any of the other yield curve models provided and can be used in that context. For example, calculating the price of the bonds contained within our quotes where we indeed recover the prices for our contrived example:\njulia&gt; map(q -&gt; pv(m,q.instrument),quotes) \n4-element Vector{Float64}:\n 0.9\n 0.8\n 0.7\n 0.6"
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#fit---the-standardized-api-for-all-models-quotes-and-methods",
    "href": "posts/finance-models-announcement/index.html#fit---the-standardized-api-for-all-models-quotes-and-methods",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "5. fit - The standardized API for all models, quotes, and methods",
    "text": "5. fit - The standardized API for all models, quotes, and methods\n       Model                                                               Method\n          |                                                                   |\n    |------------|                                                     |---------------|\nfit(Spline.Cubic(), CMTYield.([0.04,0.05,0.055,0.06,0055],[1,2,3,4,5]), Fit.Bootstrap())\n                    |-------------------------------------------------|\n                                              |\n                                              Quotes\n\nModel could be Spline.Linear(), Yield.NelsonSiegelSvensson(), Equity.BlackScholesMerton(...), etc.\nQuote could be CMTYields, ParYields, Option.Eurocall, etc.\nMethod could be Fit.Loss(x-&gt;x^2), Fit.Loss(x-&gt;abs(x)), Fit.Bootstrap(), etc.\n\nThe benefit of this versus the old Yields.jl API is:\n\nWithout a generic fit method, no obvious way to expose different curve construction methods (e.g. choice of model and method)\nThe fit is extensible. Users or other packages could define their own Models, Quotes, or Methods and integrate into the JuliaActuary ecosystem.\nThe fit formulation is very generic: the required methods are minimal to integrate in order to extend the functionality.\n\n\nCustomizing model fitting\nModel fitting can be customized:\n\nThe loss function (least squares, absolute difference, etc.) via the third argument to fit:\n\ne.g.fit(ABDiscountLine(), quotes, FIt.Loss(x -&gt; abs(x))\nthe default is Fit.Loss(x-&gt;x^2)\n\nthe optimization algorithm by defining a method FinanceModels.__default_optim__(m::ABDiscountLine) = OptimizationOptimJL.Newton()\n\nyou may need to change the __default_optic to be unbounded (simply omit the =&gt; and subsequent bounds)\nThe default is OptimizationMetaheuristics.ECA()\n\nThe general algorithm can be customized by creating a new method for fit:\n\nfunction FinanceModels.fit(m::ABDiscountLine, quotes, ...)\n   # custom code for fitting your model here\nend\n\nAs an example, the splines (Spline.Linear(), Spline.Cubic(),…) are defined to use bootstrap by default: fit(mod0::Spline.BSpline, quotes, method::Fit.Bootstrap)\n\n\n\nUsing models without fitting\nWhile many of the examples show models being fit to observed prices, you can skip that step in practice if you want to define an assumed valuation model that does not intend to calibrate market prices."
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#projections",
    "href": "posts/finance-models-announcement/index.html#projections",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "6. Projections",
    "text": "6. Projections\nA Projection is a generic way to work with various data that you can project forward. For example, getting the series of cashflows associated with a contract.\nWhat is a Projection?\nstruct Projection{C,M,K} &lt;: AbstractProjection\n    contract::C    # the contract (or set of contracts) we want to project\n    model::M       # the model that defines how the contract will behave\n    kind::K           # what kind of projection do we want? only cashflows? \nend\ncontract is obvious, so let’s talk more about the second two:\n\nmodel is the same kind of thing we discussed above. Some contracts (e.g. a floating rate bond). We can still decompose a floating rate bond into a set of cashflows, but we need a model.\n\nThere are also projections which don’t need a model (e.g. fixed bonds) and for that there’s the generic NullModel()\n\nkind defines what we’ll return from the projection.\n\nCashflowProjection() says we just want a Cashflow[...] vector\n… but if we wanted to extend this such that we got a vector containing cashflows, capital factors, default rates, etc we could define a new projection type (e.g. we might call the above AssetDetailProjection()\nAs of the time of announcement, only CashflowProjection() is defined by FinanceModels.jl\n\n\n\nContracts that depend on the model (or multiple models)\nFor example, the cashflows you generate for a floating rate bond is the current reference rate. Or maybe you have a stochastic volatility model and want to project forward option values. This type of dependency is handled like this:\n\ndefine model as a relation that maps a key to a model. E.g. a Dict(\"SOFR\" =&gt; NelsonSiegelSvensson(...))\nwhen defining the logic for the reducible collection/foldl, you can reference the Projection.model by the associated key.\n\nHere’s how a floating bond is implemented:\nThe contract struct. The key would be “SOFR” in our example above.\nstruct Floating{F&lt;:FinanceCore.Frequency,N&lt;:Real,M&lt;:Timepoint,K} &lt;: AbstractBond\n    coupon_rate::N # coupon_rate / frequency is the actual payment amount\n    frequency::F\n    maturity::M\n    key::K\nend\nAnd how we can reference the associated model when projecting that contract. This is very similar to the definition of __foldl__ for our PrincipalOnlyBond, except we are paying a coupon and referencing the scenario rate.\n@inline function Transducers.__foldl__(rf, val, p::Projection{C,M,K}) where {C&lt;:Bond.Floating,M,K}\n    b = p.contract\n    ts = Bond.coupon_times(b)\n    for t in ts\n        freq = b.frequency # e.g. `Periodic(2)`\n        freq_scalar = freq.frequency  # the 2 from `Periodic(2)`\n\n        # get the rate from the current time to next payment \n        # out of the model and convert it to the contract's periodicity\n        model = p.model[b.key]\n        reference_rate = rate(freq(forward(model, t, t + 1 / freq_scalar)))\n        coup = (reference_rate + b.coupon_rate) / freq_scalar\n        amt = if t == last(ts)\n            1.0 + coup\n        else\n            coup\n        end\n        cf = Cashflow(amt, t)\n        val = @next(rf, val, cf)\n    end\n    return complete(rf, val)\nend"
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#projectionkinds",
    "href": "posts/finance-models-announcement/index.html#projectionkinds",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "7. ProjectionKinds",
    "text": "7. ProjectionKinds\nWhile CashflowProjection is the most common (and the only one built into the initial release of FinanceModels), a Projection can be created which handles different kinds of outputs in the same manner as projecting just basic cashflows. For example, you may want to output an amortization schedule, or a financial statement, or an account value roll-forward. The Projection is able to handle these custom outputs by dispatching on the third element in a Projection.\nLet’s extend the example of a principle-only bond from section 2 above. Our goal is to create a basic amortization schedule which shows the payment made and outstanding balance.\nFirst, we create a new subtype of ProjectionKind:\nstruct AmortizationSchedule &lt;: FinanceModels.ProjectionKind\nend\nAnd then define the loop for the amortization schedule output:\n# note the dispatch on `AmortizationSchedule` in the next line\nfunction Transducers.__foldl__(rf, val, p::Projection{C,M,K}) where {C&lt;:PrincipalOnlyBond,M,K&lt;:AmortizationSchedule}\n    # initialization stuff\n    b = p.contract # the contract within a projection\n    ts = Bond.coupon_times(b) # works since it's a FinanceModels.Bond.AbstractBond with a frequency and maturity\n    pmt = 1 / length(ts)\n    balance = 1.0\n    for t in ts\n        # the loop which returns a tuple of the relevant data\n        balance -= pmt\n        result = (time=t,payment=pmt,outstanding=balance)\n        val = @next(rf, val, result) # the value to return is the last argument\n    end\n    return complete(rf, val)\nend\nWe can now define the projection:\njulia&gt; p = Projection(\n           PrincipalOnlyBond(Periodic(2),5.),  # our contract\n           NullModel(),                       # the projection doesn't need a model, so use the null model\n           AmortizationSchedule(),            # specify the amortization schedule output\n           );\n\nAnd then collect the values:\njulia&gt; collect(p)\n10-element Vector{NamedTuple{(:time, :payment, :outstanding), Tuple{Float64, Float64, Float64}}}:\n (time = 0.5, payment = 0.1, outstanding = 0.9)\n (time = 1.0, payment = 0.1, outstanding = 0.8)\n (time = 1.5, payment = 0.1, outstnding = 0.7000000000000001)\n (time = 2.0, payment = 0.1, outstanding = 0.6000000000000001)\n (time = 2.5, payment = 0.1, outstanding = 0.5000000000000001)\n (time = 3.0, payment = 0.1, outstanding = 0.40000000000000013)\n (time = 3.5, payment = 0.1, outstanding = 0.30000000000000016)\n (time = 4.0, payment = 0.1, outstanding = 0.20000000000000015)\n (time = 4.5, payment = 0.1, outstanding = 0.10000000000000014)\n (time = 5.0, payment = 0.1, outstanding = 1.3877787807814457e-16)"
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#development-benefits",
    "href": "posts/finance-models-announcement/index.html#development-benefits",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "Development Benefits",
    "text": "Development Benefits\nIn addition to the more composable code for the end-user, the package itself has been able to be simplified. Compared to Yields.jl, the lines of source code have been reduced by 30% while the number of lines of documentation has increased by over 20%."
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#migration-guide",
    "href": "posts/finance-models-announcement/index.html#migration-guide",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "Migration Guide",
    "text": "Migration Guide\nFor those looking to upgrade from Yields (v3.x.x) to FinanceModels (v4+), there is a migration guide here. Associated packages ActuaryUtilities.jl and FinanceCore.jl had major version releases for minor breaking changes where most code should remain unaffected (FinanceCore is not intended to be user-facing).\nSome tutorials or examples on the site may still use Yields.jl - that’s okay as they will still work given Julia’s strong degree of reproducibility and dependency management tools. Please open an issue on the JuliaActuary.org repository if you have trouble with any of the old example code."
  },
  {
    "objectID": "posts/finance-models-announcement/index.html#conclusion",
    "href": "posts/finance-models-announcement/index.html#conclusion",
    "title": "FinanceModels.jl - Evolving the JuliaActuary Ecosystem",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve now defined two assets that can work seamlessly with projecting cashflows, fitting models, and determining valuations :)\nFinanceModel.jl should provide the basis for a performant and composable design to facilitate further development and use by actuaries and other financial professionals."
  },
  {
    "objectID": "posts/policy-diffeq/index.html",
    "href": "posts/policy-diffeq/index.html",
    "title": "Universal Life Policy Account Mechanics as a Differential Equation",
    "section": "",
    "text": "This demonstrates an example of defining an universal life policy value roll like a discrete differential equation.\nIt then uses the SciML DifferentialEquations package to “solve” the policy projection given a single point, but also to see how the policy projection behaves under different premium and interest rate conditions.\nThe first time this is run, it will download and precompile some large packages.\n\nusing Dates\nusing MortalityTables\nusing DifferentialEquations\nusing Plots\nusing ColorSchemes # for Turbo colors, which emphasize readability\nusing ActuaryUtilities\nusing LaTeXStrings\n\nLet’s use the 2001 CSO table as the basis for cost of insurance charges:\n\ncso = MortalityTables.table(\"2001 CSO Super Preferred Select and Ultimate - Male Nonsmoker, ANB\")\n\nMortalityTable (CSO/CET):\n   Name:\n       2001 CSO Super Preferred Select and Ultimate - Male Nonsmoker, ANB\n   Fields: \n       (:select, :ultimate, :metadata)\n   Provider:\n       American Academy of Actuaries\n   mort.SOA.org ID:\n       1076\n   mort.SOA.org link:\n       https://mort.soa.org/ViewTable.aspx?&TableIdentity=1076\n   Description:\n       2001 Commissioners Standard Ordinary (CSO) Super Preferred Select and Ultimate Table – Male Nonsmoker. Basis: Age Nearest Birthday. Minimum Select Age: 0. Maximum Select Age: 99. Minimum Ultimate Age: 16. Maximum Ultimate Age: 120\n\n\nNext, policy mechanics are coded. It’s essentially a discrete differential equation, so it leverages DifferentialEquations.jl\nThe projection is coded in the Discrete DifferentialEquation format:\n\\[u_{n+1} = f(u,p,t_{n+1})\\]\nIn the code below, this translates to:\n\nu is the state of the system. We will track three variables to represent the state:\n\nstate[1] is the account value\nstate[2] is the premium paid\nstate[3] is the policy duration\n\np are the parameters of the system.\nt is the time, which will represent days since policy issuance\n\n\nfunction policy_projection(state, p, t)\n    # grab the state from the inputs\n    av = state[1]\n\n    # calculated variables\n    cur_date = p.issue_date + Day(t)\n    dur = duration(p.issue_date, cur_date)\n    att_age = p.issue_age + dur - 1\n\n    # lapse if AV &lt;= 0 \n    lapsed = (av &lt;= 0.0) & (t &gt; 1)\n\n    if !lapsed\n\n        monthly_coi_rate = (1 - (1 - p.mort_assump[att_age])^(1 / 12))\n\n        ## Periodic Policy elements\n\n        # annual events\n        if Dates.monthday(cur_date) == Dates.monthday(p.issue_date) ||\n           cur_date == p.issue_date + Day(1) # OR first issue date\n\n            premium = p.annual_prem\n        else\n            premium = 0.0\n        end\n\n        # monthly_events\n        if Dates.day(cur_date) == Dates.day(p.issue_date)\n            coi = max((p.face - av) * monthly_coi_rate, 0.0)\n        else\n            coi = 0.0\n        end\n\n        # daily events\n        int(av) = av * ((1 + p.int_rate)^(1 / 360) - 1.0)\n\n\n\n        # av\n        new_av = max(0.0, av - coi + premium + int(av - coi))\n\n        # new state\n        return [new_av, premium, dur] # AV, Prem, Dur\n\n    else\n        # new state\n        return [0.0, 0.0, dur] # AV, Prem, Dur\n\n    end\n\n\nend\n\npolicy_projection (generic function with 1 method)\n\n\nThe following function will create a named tuple of parameters given a varying prem (premium) and int (credit rate).\n\nparams(prem, int) = (\n    int_rate=int,\n    issue_date=Date(2010, 1, 1),\n    face=1e6,\n    issue_age=25,\n    mort_assump=MortalityTables.table(\"2001 CSO Super Preferred Select and Ultimate - Male Nonsmoker, ANB\").ultimate,\n    projection_years=75,\n    annual_prem=prem,\n)\n\nparams (generic function with 1 method)"
  },
  {
    "objectID": "posts/policy-diffeq/index.html#introduction",
    "href": "posts/policy-diffeq/index.html#introduction",
    "title": "Universal Life Policy Account Mechanics as a Differential Equation",
    "section": "",
    "text": "This demonstrates an example of defining an universal life policy value roll like a discrete differential equation.\nIt then uses the SciML DifferentialEquations package to “solve” the policy projection given a single point, but also to see how the policy projection behaves under different premium and interest rate conditions.\nThe first time this is run, it will download and precompile some large packages.\n\nusing Dates\nusing MortalityTables\nusing DifferentialEquations\nusing Plots\nusing ColorSchemes # for Turbo colors, which emphasize readability\nusing ActuaryUtilities\nusing LaTeXStrings\n\nLet’s use the 2001 CSO table as the basis for cost of insurance charges:\n\ncso = MortalityTables.table(\"2001 CSO Super Preferred Select and Ultimate - Male Nonsmoker, ANB\")\n\nMortalityTable (CSO/CET):\n   Name:\n       2001 CSO Super Preferred Select and Ultimate - Male Nonsmoker, ANB\n   Fields: \n       (:select, :ultimate, :metadata)\n   Provider:\n       American Academy of Actuaries\n   mort.SOA.org ID:\n       1076\n   mort.SOA.org link:\n       https://mort.soa.org/ViewTable.aspx?&TableIdentity=1076\n   Description:\n       2001 Commissioners Standard Ordinary (CSO) Super Preferred Select and Ultimate Table – Male Nonsmoker. Basis: Age Nearest Birthday. Minimum Select Age: 0. Maximum Select Age: 99. Minimum Ultimate Age: 16. Maximum Ultimate Age: 120\n\n\nNext, policy mechanics are coded. It’s essentially a discrete differential equation, so it leverages DifferentialEquations.jl\nThe projection is coded in the Discrete DifferentialEquation format:\n\\[u_{n+1} = f(u,p,t_{n+1})\\]\nIn the code below, this translates to:\n\nu is the state of the system. We will track three variables to represent the state:\n\nstate[1] is the account value\nstate[2] is the premium paid\nstate[3] is the policy duration\n\np are the parameters of the system.\nt is the time, which will represent days since policy issuance\n\n\nfunction policy_projection(state, p, t)\n    # grab the state from the inputs\n    av = state[1]\n\n    # calculated variables\n    cur_date = p.issue_date + Day(t)\n    dur = duration(p.issue_date, cur_date)\n    att_age = p.issue_age + dur - 1\n\n    # lapse if AV &lt;= 0 \n    lapsed = (av &lt;= 0.0) & (t &gt; 1)\n\n    if !lapsed\n\n        monthly_coi_rate = (1 - (1 - p.mort_assump[att_age])^(1 / 12))\n\n        ## Periodic Policy elements\n\n        # annual events\n        if Dates.monthday(cur_date) == Dates.monthday(p.issue_date) ||\n           cur_date == p.issue_date + Day(1) # OR first issue date\n\n            premium = p.annual_prem\n        else\n            premium = 0.0\n        end\n\n        # monthly_events\n        if Dates.day(cur_date) == Dates.day(p.issue_date)\n            coi = max((p.face - av) * monthly_coi_rate, 0.0)\n        else\n            coi = 0.0\n        end\n\n        # daily events\n        int(av) = av * ((1 + p.int_rate)^(1 / 360) - 1.0)\n\n\n\n        # av\n        new_av = max(0.0, av - coi + premium + int(av - coi))\n\n        # new state\n        return [new_av, premium, dur] # AV, Prem, Dur\n\n    else\n        # new state\n        return [0.0, 0.0, dur] # AV, Prem, Dur\n\n    end\n\n\nend\n\npolicy_projection (generic function with 1 method)\n\n\nThe following function will create a named tuple of parameters given a varying prem (premium) and int (credit rate).\n\nparams(prem, int) = (\n    int_rate=int,\n    issue_date=Date(2010, 1, 1),\n    face=1e6,\n    issue_age=25,\n    mort_assump=MortalityTables.table(\"2001 CSO Super Preferred Select and Ultimate - Male Nonsmoker, ANB\").ultimate,\n    projection_years=75,\n    annual_prem=prem,\n)\n\nparams (generic function with 1 method)"
  },
  {
    "objectID": "posts/policy-diffeq/index.html#runing-the-system",
    "href": "posts/policy-diffeq/index.html#runing-the-system",
    "title": "Universal Life Policy Account Mechanics as a Differential Equation",
    "section": "Runing the system",
    "text": "Runing the system\nThis results in the following plot. The tracked output variables u1 and u2 represent the two vars that we tracked above: account value and cumulative premium.\n\nbegin\n    p = params(\n        8000.0, # 8,000 annual premium\n        0.08    # 8% interest\n    )\n\n    # calculate the number of days to project\n    projection_end_date = p.issue_date + Year(p.projection_years)\n    days_to_project = Dates.value(projection_end_date - p.issue_date)\n\n    # the [0.0,..] are the initial conditions for the tracked variables\n    prob = DiscreteProblem(policy_projection, [0.0, 0.0, 0], (0, days_to_project), p)\n    proj = solve(prob, FunctionMap())\n\n    plot(proj)\nend"
  },
  {
    "objectID": "posts/policy-diffeq/index.html#moving-up-the-ladder-of-abstraction",
    "href": "posts/policy-diffeq/index.html#moving-up-the-ladder-of-abstraction",
    "title": "Universal Life Policy Account Mechanics as a Differential Equation",
    "section": "Moving up the ladder of abstraction",
    "text": "Moving up the ladder of abstraction\nAn excellent way to understand the behavior of a model is to move up the ladder of abstraction. Below, we will see what happens to the projection at varying levels of credit rates and annual premiums.\nmd”The full range of simulated outcomes can take a couple of minutes to run:\n\nbegin\n    prem_range = 1000.0:100.0:9000.0\n    int_range = 0.02:0.0025:0.08\n\n    function ending_av(ann_prem, int, days_to_project)\n        p = params(ann_prem, int)\n        prob = DiscreteProblem(policy_projection, [0.0, 0.0, 0], (0, days_to_project), p)\n        proj = solve(prob, FunctionMap())\n        end_av = proj[end][1]\n        if end_av == 0.0\n            lapse_time = findfirst(isequal(0.0), proj[1, 2:end])\n        else\n            lapse_time = length(proj)\n        end\n        duration = proj[3, lapse_time]\n        end_age = p.issue_age + duration - 1.0\n\n        return end_av, end_age\n\n    end\n\n    end_age = zeros(length(prem_range), length(int_range))\n    end_av = zeros(length(prem_range), length(int_range))\n\n    # loop through each projection we did and fill our ranges with the ending AV and ending age\n    for (i, vp) in enumerate(prem_range)\n        for (j, vi) in enumerate(int_range)\n            end_av[i, j], end_age[i, j] = ending_av(vp, vi, days_to_project)\n        end\n    end\nend\n\nNow let’s plot the result. Not surprising, interest has a huge effect on the policy projection. Premium is also a major influence.\nOne thing that’s remarkable is how going from 2000 premium to just ~2200 of premium results in about a $5m difference at 8% interest. The power of compound interest!\n\nviz = plot(layout=2) # side by side plot\n\n# \ncontour!(viz[1], int_range,\n    prem_range,\n    end_av ./ 1e6, # scale to millions for readability\n    contour_labels=true,\n    c=cgrad(ColorSchemes.turbo.colors),\n    fill=true,\n    title=\"AV at age 100 (M)\",\n    ylabel=L\"Annual Premium (\\$)\",\n    xlabel=\"credit rate\")\n\ncontour!(viz[2], int_range,\n    prem_range,\n    end_age, contour_labels=true,\n    c=cgrad(ColorSchemes.turbo.colors),\n    fill=true,\n    yaxis=false,\n    title=\"Age at Lapse\",\n    xlabel=\"credit rate\"\n)\n\nannotate!(viz[2], (0.055, 7000, Plots.text(\"Doesn't lapse \\nbefore age 100\", 8, :white, :center)))"
  },
  {
    "objectID": "posts/policy-diffeq/index.html#conclusion",
    "href": "posts/policy-diffeq/index.html#conclusion",
    "title": "Universal Life Policy Account Mechanics as a Differential Equation",
    "section": "Conclusion",
    "text": "Conclusion\nThis shows how universal life mechanics are a dynamic system. the growth/decay is governed by two competing feedback loops:\n\nGrowth: the force of interest lets the balance grow exponentially over long periods of time\nDecay: low balances increase the net amount at risk and the resulting COI charges."
  },
  {
    "objectID": "posts/policy-diffeq/index.html#endnotes",
    "href": "posts/policy-diffeq/index.html#endnotes",
    "title": "Universal Life Policy Account Mechanics as a Differential Equation",
    "section": "Endnotes",
    "text": "Endnotes\nThis is not meant to represent any particular insurance product, nor fully replicate typical account mechanics."
  },
  {
    "objectID": "posts/life-modeling-problem/index.html",
    "href": "posts/life-modeling-problem/index.html",
    "title": "The Life Modeling Problem: A Comparison of Julia, Rust, Python, and R",
    "section": "",
    "text": "!! Note that this article is a draft\nNote: This is an extended discussion of the results from one of the items on the Benchmarks page.\nIn the ActuarialOpenSource GitHub organization, a discussion began of the “Life Modeling Problem” (LMP) in actuarial science.\nI think the “Life Modeling Problem” has the following attributes:\nThe following discussion will get a little bit technical, but I think there are a few key takeaways:\nTo skip the background and the discussion, click here to jump to the benchmarks."
  },
  {
    "objectID": "posts/life-modeling-problem/index.html#the-life-modeling-problem",
    "href": "posts/life-modeling-problem/index.html#the-life-modeling-problem",
    "title": "The Life Modeling Problem: A Comparison of Julia, Rust, Python, and R",
    "section": "The Life Modeling Problem",
    "text": "The Life Modeling Problem\nInspired by the discussion in the ActuarialOpenSource GitHub community discussion, folks started submitted solutions to what someone referred to as the “Life Modeling Problem”. This user submitted a short snippet for consideration of a representative problem.\nMy take on the characteristics are that modeling life actuarial science problems breaks down to the following items:\n\nCalculations are recursive in nature\nComputationally intensive calculations\nPerformance matters given large volumes of data to process\nReadability and usability aids in controls and risk\n\n\nRecursive calculations\nMany actuarial formulas are recursive in nature. Reserves are defined “prospectively” or “retrospectively”. Algorithmically, this means that intra-seriatim calculations (ie account value growth, survivorship) are not amenable to parallelism but iter-seriatim calculations would be (ie calculating multiple policy trajectories simultaneously).\n\n\nComputationally intensive\nModeling is incredibly computationally complex due to the volume of data needed to process. For example, CUNA Mutual disclosed that they spin up 50 servers with 20 cores a couple of days per month to do the calculations.\n\n\nProcessing volume\nThere’s a cottage industry devoted to inforce compression and model simplifications to get runtime and budgets down to a reasonable level. However, as the capacity for computing has grown, the company and regulatory demands have grown. E.g in the US reserving has transition from net-premium reserve to integrated ALM (CFT) to deterministic scenarios sets (CFT w NY7 + others) to truly stochastic (Stochastic PBR). “What Intel giveth, the NAIC taketh away.”[^1].\nSo again, performance matters!\n\n\nReadability and Expressiveness\nActuaries, even the 10x Actuary, aren’t pure computer scientists and computational efficiency has never been so critical that they sacrifice everything else to get it. So the industry never turned to the 40-year king of performance computation, Fortran. The syntax is very “close to the machine”. I’s a bit rough to read to anyone not well versed.\nInterestingly, APL took off and was one of the dominant languages used by actuaries before the advent of vendor-supplied modeling solutions.\nCounting the occurrences of a string looks like this in APL:\ncsubs←{0=x←⊃⍸⍺⍷⍵:0 ⋄ 1+⍺∇(¯1+x+⍴⍺)↓⍵}\nwhereas in Fortran it would be:\nfunction countsubstring(s1, s2) result(c)\n  character(*), intent(in) :: s1, s2\n  integer :: c, p, posn\n \n  c = 0\n  if(len(s2) == 0) return\n  p = 1\n  do \n    posn = index(s1(p:), s2)\n    if(posn == 0) return\n    c = c + 1\n    p = p + posn + len(s2) - 1\n  end do\nend function\nMaybe more readable to the modern eye than APL, but many actuaries would still recognize what’s going on with the first code example.\nWhy did APL take off for Actuaries and not APL? I think expressiveness and the ability to have a language inspired by mathematical notation were attractive. It’s not pleasant to write a lot of boiler-plate simply to achieve a simple objective.\nWhat is boiler-plate? It’s writing a lot of code supporting the main idea, but straying from a simple mathematical formulation. For example, in high performance object-oriented languages (C#/C++/Java), the idiomatic code might involve a special class. See, for example this C# code to count substrings:\nusing System;\n \nclass SubStringTestClass\n{\n   public static int CountSubStrings(this string testString, string testSubstring)\n   {\n        int count = 0;\n \n        if (testString.Contains(testSubstring))\n        {\n            for (int i = 0; i &lt; testString.Length; i++)\n            {\n                if (testString.Substring(i).Length &gt;= testSubstring.Length)\n                {\n                    bool equals = testString.Substring(i, testSubstring.Length).Equals(testSubstring);\n                    if (equals)\n                    {\n                        count++;\n                        i += testSubstring.Length - 1;\n                    }\n                }\n            }\n        }\n        return count;\n   }\n}\nThe takeaway from this point, though, is that there is a natural draw towards more expressive, powerful languages than less expressive languages, especially when dealing with mathematically based ideas. So high expressiveness is something we want from a language that solves the LMP."
  },
  {
    "objectID": "posts/life-modeling-problem/index.html#benchmarks",
    "href": "posts/life-modeling-problem/index.html#benchmarks",
    "title": "The Life Modeling Problem: A Comparison of Julia, Rust, Python, and R",
    "section": "Benchmarks",
    "text": "Benchmarks\nAfter the original user submitted a proposal, others chimed in and submitted versions in their favorite languages. I have collected those versions, and run them on a consistent set of hardware[^3].\nSome “submissions” were excluded because they involved an entirely different approach, such as memoizing the function calls[^2].\n\n\n\n\nTable 1: Benchmarks for the Life Modeling Problem in nanoseconds (lower times are better).\n\n\n\n18×6 DataFrame\n\n\n\nRow\nlang\nalgorithm\nfunction_name\nmedian\nmean\nrelative_mean\n\n\n\nString15\nString15\nString15\nFloat64?\nFloat64\nFloat64\n\n\n\n\n1\nJulia\nAccumulator\nnpv9\n6.388\n6.375\n1.0\n\n\n2\nRust\nAccumulator\nnpv3\n7.0\n7.0\n1.09804\n\n\n3\nJulia\nAccumulator\nnpv8\n7.372\n7.375\n1.15686\n\n\n4\nJulia\nAccumulator\nnpv7\n7.92\n7.917\n1.24188\n\n\n5\nJulia\nAccumulator\nnpv6\n9.037\n9.009\n1.41318\n\n\n6\nJulia\nAccumulator\nnpv4\n10.764\n10.761\n1.688\n\n\n7\nJulia\nAccumulator\nnpv5\n11.49\n11.469\n1.79906\n\n\n8\nRust\nAccumulator\nnpv2\n14.0\n14.0\n2.19608\n\n\n9\nJulia\nAccumulator\nnpv3\n14.507\n14.487\n2.27247\n\n\n10\nRust\nAccumulator\nnpv1\n22.0\n22.0\n3.45098\n\n\n11\nJulia\nVectorized\nnpv2\n235.758\n218.391\n34.2574\n\n\n12\nJulia\nVectorized\nnpv1\n235.322\n228.198\n35.7958\n\n\n13\nPython (Numba)\nAccumulator\nnpv_numba\nmissing\n626.0\n98.1961\n\n\n14\nPython\nAccumulator\nnpv_loop\nmissing\n2314.0\n362.98\n\n\n15\nPython (NumPy)\nVectorized\nnpv\nmissing\n14261.0\n2237.02\n\n\n16\nR\nVectorized\nnpv base\n4264.0\n46617.0\n7312.47\n\n\n17\nR\nAccumulator\nnpv_loop\n4346.0\n62275.7\n9768.74\n\n\n18\nR (data.table)\nVectorized\nnpv\n770554.0\n8.42767e5\n1.32199e5\n\n\n\n\n\n\n\n\n\nTo aid in visualizing results with such vast different orders of magnitude, this graph includes a physical length comparison to serve as a reference. The computation time is represented by the distance that light travels in the time for the computation to complete (comparing a nanosecond to one foot length goes at least back to Admiral Grace Hopper).\n\n\n\n\nTable 2: Benchmarks for the Life Modeling Problem in nanoseconds (lower times are better).\n\n\n\n18×6 DataFrame\n\n\n\nRow\nlang\nalgorithm\nfunction_name\nmedian\nmean\nrelative_mean\n\n\n\nString15\nString15\nString15\nFloat64?\nFloat64\nFloat64\n\n\n\n\n1\nJulia\nAccumulator\nnpv9\n6.388\n6.375\n1.0\n\n\n2\nRust\nAccumulator\nnpv3\n7.0\n7.0\n1.09804\n\n\n3\nJulia\nAccumulator\nnpv8\n7.372\n7.375\n1.15686\n\n\n4\nJulia\nAccumulator\nnpv7\n7.92\n7.917\n1.24188\n\n\n5\nJulia\nAccumulator\nnpv6\n9.037\n9.009\n1.41318\n\n\n6\nJulia\nAccumulator\nnpv4\n10.764\n10.761\n1.688\n\n\n7\nJulia\nAccumulator\nnpv5\n11.49\n11.469\n1.79906\n\n\n8\nRust\nAccumulator\nnpv2\n14.0\n14.0\n2.19608\n\n\n9\nJulia\nAccumulator\nnpv3\n14.507\n14.487\n2.27247\n\n\n10\nRust\nAccumulator\nnpv1\n22.0\n22.0\n3.45098\n\n\n11\nJulia\nVectorized\nnpv2\n235.758\n218.391\n34.2574\n\n\n12\nJulia\nVectorized\nnpv1\n235.322\n228.198\n35.7958\n\n\n13\nPython (Numba)\nAccumulator\nnpv_numba\nmissing\n626.0\n98.1961\n\n\n14\nPython\nAccumulator\nnpv_loop\nmissing\n2314.0\n362.98\n\n\n15\nPython (NumPy)\nVectorized\nnpv\nmissing\n14261.0\n2237.02\n\n\n16\nR\nVectorized\nnpv base\n4264.0\n46617.0\n7312.47\n\n\n17\nR\nAccumulator\nnpv_loop\n4346.0\n62275.7\n9768.74\n\n\n18\nR (data.table)\nVectorized\nnpv\n770554.0\n8.42767e5\n1.32199e5\n\n\n\n\n\n\n\n\n\nTo aid in visualizing results with such vast different orders of magnitude, this graph includes a physical length comparison to serve as a reference. The computation time is represented by the distance that light travels in the time for the computation to complete (comparing a nanosecond to one foot length goes at least back to Admiral Grace Hopper).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "posts/life-modeling-problem/index.html#benchmark-discussion",
    "href": "posts/life-modeling-problem/index.html#benchmark-discussion",
    "title": "The Life Modeling Problem: A Comparison of Julia, Rust, Python, and R",
    "section": "Benchmark Discussion",
    "text": "Benchmark Discussion\n\nTakeaway #1\n\nThere are a lot of ways to accomplish the same task and and that’s good enough in most cases\n\nAll of the submissions and algorithms above worked, and fast enough that it gave an answer in very little time. And much of the time, the volume of data to process is small enough that it doesn’t matter.\nBut remember the CUNA Mutual example from above: Let’s say that CUNA’s runtime is already as fast as it can be, and index it to the fastest result in the benchmarks below. The difference between the fastest “couple of days” run and the slowest would be over 721 years. So it’s important to use tools and approaches that are performant for actuarial work.\nSo for little one-off tasks it doesn’t make a big difference what tool or algorithm is used. More often than not, your one-off calculations or checks will be done fast enough that it’s not important to be picky. But if wanting to scale your work to a broader application within your company or the industry, I think it’s important to be performance-minded[^4].\n\n\nTakeaway #2\n\n“If all you have is a dataframe, everything looks like a join”\n\nI’ve seen this several times in practice. Where a stacked dataframe of mortality rates is joined up with policy data in a complicated series of %&gt;%s (pipes), inner_joins and mutates.\nDon’t get me wrong, I think code is often still a better approach than spreadsheets[^5].\nHowever, like the old proverb that “if all you have is a hammer, everything looks like a nail” - sometimes the tool you have just isn’t right for the job. That’s the lesson of the R data.table result above. Even with the fastest dataframe implementation in R, it vastly trails other submissions.\n\nAlgorithm choice\nGetting more refined about the approach, the other thing that is very obvious is that for this recursive type calculation, it’s much more efficient to write a for loop (the Accumulator approach) in every language except for R (where it wants everything to be a vector or dataframe).\nThe difference hints at some computational aspects related to arrays that I will touch upon when discussing code examples below. The point for now is that you should consider using tools that give you the flexibility to approach problems in different ways.\n\n\n\nTakeaway #3\n\nPerformance, flexibility, readability: pick one, two, or three depending on the language\n\nThis section ranges from objective (performance metrics) to subjective (my take on flexibility and readability). My opinions are based on using R heavily for several years ~2011-2015, Python for ~2015-2018, and then primarily switched to Julia in ~2018. I have a lot of experience with VBA, and moderate experience with Javascript, with some educational/introductory background in C, List/Racket, Mathematica, Haskell, and Java.\n\nR\n\nPerformance\nR was the slowest all-around.\n\n\nFlexibility\nR scores well here - non-standard evaluation lets you, essentially, inspect the written code without evaluating it. This is a nice feature that enables a lot of creativity and pleasantries (like ggplot knowing what to label the axes without you telling it).\nR works in notebook environments and from a REPL.\nOne negative about R’s flexibility is the fact that the language is GPL licensed, meaning that there are quite a few restrictions. For example, if you distribute an application that relies on R (e.g. it becomes part of your sales platform distributed to agents) you would need to be able to provide the source code for your application to the said users[^6]. The other languages discussed on this page have much more permissive licenses.\n\n\nReadability\nR reads pretty easily, with very little boiler plate and terse syntax:\n\n# via Houstonwp\nq &lt;- c(0.001,0.002,0.003,0.003,0.004,0.004,0.005,0.007,0.009,0.011)\nw &lt;- c(0.05,0.07,0.08,0.10,0.14,0.20,0.20,0.20,0.10,0.04)\nP &lt;- 100\nS &lt;- 25000\nr &lt;- 0.02\n\nbase_r_npv &lt;- function(q,w,P,S,r) {\n  inforce &lt;- c(1,head(cumprod(1-q-w), -1))\n  ncf &lt;- inforce * P - inforce * q * S\n  d &lt;- (1/(1+r)) ^ seq_along(ncf)\n  sum(ncf * d)\n}\n\nbase_r_npv(q,w,P,S,r)\n#&gt; [1] 50.32483\nmicrobenchmark::microbenchmark(base_r_npv(q,w,P,S,r))\nR is the oldest of the languages commonly used in actuarial contexts and carries a lot of that weight for better or worse: over several decades, it’s accumulated a sizeable community but is left with a number of rough edges. The data scientist Evan Patterson has written a nice summary of “the good, the bad, and the ugly” of R.\n\n\n\nRust\nRust is a newer, statically compiled language designed for performance and safety (in the don’t let your program do memory management mistakes that crash the computer).\n\nPerformance\nRust scores well here, coming second only to Julia.\n\n\nFlexibility\nRust is statically compiled (you write script, have computer run script, see results). It doesn’t have a REPL or ability to be used in an interactive way (e.g. notebook environments).\n\n\nReadability\nI really like the explicit function contract: you give it various floating point (f64) vectors and numbers, and it returns a float: -&gt; f64.\nOther than that it’s pretty straightforward but definitely more verbose than any of the others.\n#![feature(test)]\nextern crate test;\nuse test::Bencher;\n\n// Via Paddy Horan\npub fn npv(mortality_rates: &Vec&lt;f64&gt;, lapse_rates: &Vec&lt;f64&gt;, interest_rate: f64, sum_assured: f64, premium: f64, init_pols: f64, term: Option&lt;usize&gt;) -&gt; f64 {\n\n    let term = term.unwrap_or_else(|| mortality_rates.len());\n    let mut result = 0.0;\n    let mut inforce = init_pols;\n    let v: f64 = 1.0 / (1.0 + interest_rate);\n\n    for (t, (q, w)) in mortality_rates.iter().zip(lapse_rates).enumerate() {\n        let no_deaths = if t &lt; term {inforce * q} else {0.0};\n        let no_lapses = if t &lt; term {inforce * w} else {0.0};\n        let premiums = inforce * premium;\n        let claims = no_deaths * sum_assured;\n        let net_cashflow = premiums - claims;\n        result += net_cashflow * v.powi(t as i32);\n        inforce = inforce - no_deaths - no_lapses;\n    }\n\n    result\n}\n#[bench]\nfn bench_xor_1000_ints(b: &mut Bencher) {\n\nlet q: Vec&lt;f64&gt; = vec![0.001,0.002,0.003,0.003,0.004,0.004,0.005,0.007,0.009,0.011];\nlet w: Vec&lt;f64&gt; = vec![0.05,0.07,0.08,0.10,0.14,0.20,0.20,0.20,0.10,0.04];\nlet p: f64 = 100.0;\nlet s: f64 = 25000.0;\nlet r: f64 = 0.02;\n    b.iter(|| {\n        // use `test::black_box` to prevent compiler optimizations from disregarding\n        // unused values\n        test::black_box(npv(&q,&w,r,s,p,1.0,Some(10)));\n    });\n}\n\n\n\nPython\n\nPerformance\nWith NumPy, Python was the second fastest Vectorized approach and 3rd place for the Accumulator loop, both cases were still more than an order of magnitude slower than the next place.\n\n\nFlexibility\nPython wins large points for interactive usage in the REPL, notebooks, and wide variety of environments that support running Python code. However, within the language itself I have to deduct points for the ease of inspecting/evaluating code.\nWhat I mean by that, is that if you look at the code example below, in order to test the code you have to turn it into string and then call the timeit function to read and parse the string. In none of the other tested languages was that sort of boiler-plate required.\nPython scores partial points for meta-programming: decorators (@ syntax) is syntactic sugar for macro-like modifications to functions, but Python metaprogramming is fundamentally limited by the language design.\nPython has perhaps the most robust ecosystem of all the languages discussed here, but in many ways its limiting: once you get deep into an ecosystem (e.g. NumPy), you are sort of at the mercy of package developers to ensure that the packages are compatible. As a key example, many common types and data structures are not shareable between libraries: there are efforts to standardize data types/classes for better compatibility across the Python ecosystem, but may require fundamental changes to the language or ecosystem to accomplish.\nOn the subject of Python packages: environment and package management in Python is notoriously painful:\n\n\n\nPython XKCD\n\n\n\n\nReadability\nOne of Python’s seminal features is the pleasant syntax, though opinions differ as to whether the indentation should matter to how your program runs.\nimport timeit\nsetup='''\nimport numpy as np\nq = np.array([0.001,0.002,0.003,0.003,0.004,0.004,0.005,0.007,0.009,0.011])\nw = np.array([0.05,0.07,0.08,0.10,0.14,0.20,0.20,0.20,0.10,0.04])\nP = 100\nS = 25000\nr = 0.02\ndef npv(q,w,P,S,r):\n    decrements = np.cumprod(1-q-w)\n    inforce = np.empty_like(decrements)\n    inforce[:1] = 1\n    inforce[1:] = decrements[:-1]\n    ncf = inforce * P - inforce * q * S\n    t = np.arange(np.size(q))\n    d = np.power(1/(1+r), t)\n    return np.sum(ncf * d)\n'''\nbenchmark = '''npv(q,w,P,S,r)'''\n\nprint(timeit.timeit(stmt=benchmark,setup=setup,number = 1000000))\nTo do non standard things like benchmarking (or The benchmarking setup with Python’s timeit is definitely the most painful, needing to wrap the whole thing in a string. And then only get a single number result, without normalizing for the number of runs is very annoying.\n\n\n\nJulia\n\nPerformance\nThe fastest language with both algorithms.\n\n\nFlexibility\nAvailable in a variety of environments, include the standard interactive ones like the REPL and notebooks. One key differentiator is the reactive notebook environment, Pluto.jl where notebook cells understand and interact with one-another.\nJulia packages are also notoriously cross-functional, so unlike Python (e.g. NumPy) or R (e.g. Tidyverse), tightly coupled specialty-ecosystems have not evolved in Julia.\nJulia is MIT licensed, as are many of the community packages (including JuliaActuary’s). This license is very permissive and is likely to cause the least issue compared with other licenses discussed on this page.\nThe ability to introspect code is one of Julia’s superpowers\n\n\nReadability\nJulia scores well here, but gets dinged in my mind for a couple of things:\n\nall of those dots!\nthe weird @benchmark and dollar signs ($s)\n\nThe former is actually a very powerful concept/tool called broadcasting. Kind of like R (where everything is a vector and will combine in vector-like ways). Julia lets you both worlds: really effective scalars and highly efficient vector operations. Once you know what it does, it’s hard to think of a shorter/more concise way to express it than the dot (.).\nThe latter, @benchmark is a way to get Julia to work with the code itself, again kind of like R does. @benchmark is a macro that will run a really comprehensive and informative benchmarking set on the code given.\nThe Vectorized version:\nusing BenchmarkTools\n\nq = [0.001,0.002,0.003,0.003,0.004,0.004,0.005,0.007,0.009,0.011]\nw = [0.05,0.07,0.08,0.10,0.14,0.20,0.20,0.20,0.10,0.04]\nP = 100\nS = 25000\nr = 0.02\n\nfunction npv1(q,w,P,S,r) \n    inforce = [1.; cumprod(1 .- q .- w)[1:end-1]] \n    ncf = inforce .* P .- inforce .* q .* S\n    d = (1 ./ (1 + r)) .^ (1:length(ncf))\n    return sum(ncf .* d)\nend\n\n@benchmark npv($q,$w,$P,$S,$r)\nAnd the Accumulator version:\nfunction npv5(q,w,P,S,r,term=nothing)\n    term = term === nothing ? length(q) : term\n    inforce = 1.0\n    result = 0.0\n    v = (1 / ( 1 + r))\n    v_t = v\n    for (q,w,_) in zip(q,w,1:term)\n        result += inforce * (P - S * q) * v_t\n        inforce -= inforce * q + inforce * w\n        v_t *= v\n    end\n    return result\nend"
  },
  {
    "objectID": "posts/life-modeling-problem/index.html#more-flexibility-more-performance-from-julia",
    "href": "posts/life-modeling-problem/index.html#more-flexibility-more-performance-from-julia",
    "title": "The Life Modeling Problem: A Comparison of Julia, Rust, Python, and R",
    "section": "More flexibility, more performance from Julia",
    "text": "More flexibility, more performance from Julia\nI wanted to go a little bit deeper and show how 1) Julia just runs fast even if your not explicitly focused on performance. But for where it really matters, you can go even deeper. This is a little advanced, but I think it can be useful to introduce some basics as to why some languages and approaches are going to be fundamentally slower than others.\nNotes: The accumulator approach vectors and allocations talk about stack/heap? pick a faster approach and explain why it’s even faster."
  },
  {
    "objectID": "posts/life-modeling-problem/index.html#colophon",
    "href": "posts/life-modeling-problem/index.html#colophon",
    "title": "The Life Modeling Problem: A Comparison of Julia, Rust, Python, and R",
    "section": "Colophon",
    "text": "Colophon\n\nCode\nAll of the benchmarked code can be found in the JuliaActuary Learn repository. Please file an issue or submit a PR request there for issues/suggestions.\n\n\nHardware\nMacBook Air (M1, 2020)\n\n\nSoftware\nAll languages/libraries are Mac M1 native unless otherwise noted\n\nJulia\nJulia Version 1.7.0-DEV.938\nCommit 2b4c088ee7* (2021-04-16 20:37 UTC)\nPlatform Info:\n  OS: macOS (arm64-apple-darwin20.3.0)\n  CPU: Apple M1\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-11.0.1 (ORCJIT, cyclone)\n\n\nRust\n1.53.0-nightly (b0c818c5e 2021-04-16)\n\n\nPython\nPython 3.9.4 (default, Apr  4 2021, 17:42:23) \n[Clang 12.0.0 (clang-1200.0.32.29)] on darwin"
  },
  {
    "objectID": "posts/life-modeling-problem/index.html#footnotes",
    "href": "posts/life-modeling-problem/index.html#footnotes",
    "title": "The Life Modeling Problem: A Comparison of Julia, Rust, Python, and R",
    "section": "Footnotes",
    "text": "Footnotes\n[^1] A take on Andy and Bill’s law\n[^2] If benchmarking memoization, it’s essentially benchmarking how long it takes to perform hashing in a language. While interesting, especially in the context of incremental computing, it’s not the core issue at hand. Incremental computing libraries exist for all of the modern languages discussed here.\n[^3] Note that not all languages have both a mean and median result in their benchmarking libraries. Mean is a better representation for a garbage-collected modern language, because sometimes the computation just takes longer than the median result. Where the mean is not available in the graph below, median is substituted.\n[^4] Don’t prematurely optimize. But in the long run avoid, re-writing your code in a faster language too many times!\n[^5] I’ve seen 50+ line, irregular Excel formulas. To Nick: it probably started out as a good idea but it was a beast to understand and modify! At least with code you can look at the code with variable names and syntax highlighting! Comments, if you are lucky!\n[^6] This is not legal advice, consult a lawyer for more details."
  },
  {
    "objectID": "posts/bayesian-intro/index.html",
    "href": "posts/bayesian-intro/index.html",
    "title": "Getting Started with Julia for Actuaries",
    "section": "",
    "text": "The previously published article titled Julia for Actuaries gave a longer introduction to why Julia works so well in actuarial workflows. In summary: Julia’s attributes are “evident in its pragmatic, productivity-focused design choices, pleasant syntax, rich ecosystem, thriving communities, and its ability to be both very general purpose and power cutting edge computing”.\nThis is the second in the following trilogy of articles:"
  },
  {
    "objectID": "posts/bayesian-intro/index.html#reminder-of-why-julia",
    "href": "posts/bayesian-intro/index.html#reminder-of-why-julia",
    "title": "Getting Started with Julia for Actuaries",
    "section": "Reminder of “Why Julia?”",
    "text": "Reminder of “Why Julia?”\nIn the 2021 Stack Overflow Survey, Julia was the 5th most loved language. This was ahead of other languages commonly used in actuarial contexts, such as Python (6th), R (28th), C++ (25th), Matlab (36th), or VBA (37th).\nThere are three main reasons to consider using Julia:\n\nThe language itself offers expressiveness, pleasant syntax, and less boilerplate than many alternatives. Multiple dispatch is a programming paradigm that is an evolution of object-oriented approaches that’s more amenable to a wide range of programming styles, including functional and vectorized approaches. It affords Julia code a high level of composability and is what makes the Julia ecosystem so powerful.\nHigh performant Julia code, instead of needing libraries written in C/Cython/etc. For lots of problems, especially “toy” problems as you learn a language, the speed of Matlab/Python/R is fast enough. However, in real usage, particularly actuarial problems, you might find that when you need the performance, it’s too late1\nThe language, tooling, and ecosystem is very modern, mature, and powerful. A built-in package manager, packages that work together without needing to know about each other, differentiable programming, meta-programming (macros), first-class GPU/parallel support, and wide range of packages relevant to actuarial workloads.\n\nMore detail of why Julia works so well in actuarial contexts was discussed in the prior article, Julia for Actuaries. The rest of this article is going to focus on getting oriented to using Julia, and the next article in the series will introduce actuarial packages available."
  },
  {
    "objectID": "posts/bayesian-intro/index.html#the-language-itself",
    "href": "posts/bayesian-intro/index.html#the-language-itself",
    "title": "Getting Started with Julia for Actuaries",
    "section": "The language itself",
    "text": "The language itself\nJulia is a high level language, with syntax that should feel familiar to someone coming from R, Python, or Matlab. This article is too short for a true introduction to the language. Fortunately, there’s a ton of great references online, such as:\n\nMatlab-Python-Julia Cheatsheet\nLearn Julia in Y minutes\nJulia Official Documentation\n\nJulia code is compiled on-the-fly, generating efficient code for the specific data that you are currently working with. This is kind of an in-between of a fully interpreted language (like pure Python or R) and a complied language like C++ which must compile everything in advance.\nBorn of a desire to have the niceties of high level languages with the performance of low level compiled code, there are many built-in data structures and functions related to numerical computing like that used in finance, insurance, and statistics.\nIf you just want a quick introduction for beginners, Julia For Data Science is a great resource with easy, digestible tutorials. If you want a ground-up introduction, the e-book Think Julia starts simple and builds up. If you prefer to learn-by-example: the interactive, free, online course Introduction to Computational Thinking by MIT will have you working on everything from data science to climate modeling.\nLastly, JuliaAcademy.com has a number of free courses that introduce the language via data science, machine learning, or “for nervous beginners.”\n\nSpotlight on Features\nA full introduction to the language is beyond the scope of this article. There are two features we’d like to highlight here, which exemplify powerful features not available in other languages. These might sound a little technical at first, but as Paul Graham describes in his essay, Beating the Average, it is hard to see what you might be missing until you get experience with it.\n\nMultiple Dispatch\nMultiple dispatch is the term for deciding which function to call based on the combination of arguments. Taking an example pointed out by data scientist Brian Groenke2 of a simple ordinary least squares implementation:\nols(x,y) = inv(x'x)x'y\n\n# alternatively, one can write this using matrix division:\nols(x,y) = x \\ y\n“This very naive implementation already works for any appropriately sized x and y, including the multivariate case where x is a matrix or multiple-target case where y is a matrix. Furthermore, if x and y have special types [e.g diagonal or sparse matrices], we get the potentially optimized implementations [of the different combinations] for free.”\nIn a language without multiple dispatch, the alternative would be to:\n\ndefine ols for every combination of types you might encounter\nattach a method to a class with every combination of second argument type\n\nThe statistician Josh Day wrote an entire blog post about how multiple dispatch boosts one’s productivity, allows for less code, and more time spent on solving the actual problem at hand.\n\n\nMeta-programming and Macros\nMeta-programming is the essentially the ability to program the language itself, and macros are one of the tools that provide this ability. In the Paul Graham essay mentioned above, macros are an example of the competitive advantage conferred by a more powerful language.\nFor example, when you see @benchmark present_value(0.05, [10, 10, 10]) in Julia, the @benchmark is a macro (starts with @). It modifies the written code to wrap present_value in setup, timing, and summary code before returning the result of present_value. There is an example of this later in the article.\nMost other languages don’t have macros, and it means that it’s hard to ‘hook into’ code in a safe way. For example, benchmarking can involve a lot of boilerplate code just to setup, time, and summarize the results (such as the timeit library in Python)3.\nNote that using macros is quite prevalent when coding in Julia, however writing macros is more advanced usage and beyond the scope of a “getting-started” guide."
  },
  {
    "objectID": "posts/bayesian-intro/index.html#installation-and-tooling",
    "href": "posts/bayesian-intro/index.html#installation-and-tooling",
    "title": "Getting Started with Julia for Actuaries",
    "section": "Installation and Tooling",
    "text": "Installation and Tooling\n\nInstallation\nJulia is open source and can be downloaded from JuliaLang.org and is available for all major operating systems. After you download and install, then you have Julia installed and can access the REPL, or Read-Eval-Print-Loop, which can run complete programs or function as powerful day-to-day calculator. However, many people find it more comfortable to work in a text editor or IDE (Integrated Development Environment).\nIf you are looking for managed installations with a curated set of packages for use within an organization, there are ways to self-host package repositories and otherwise administratively manage packages. Julia Computing offers managed support with enterprise solutions, including push-button cloud compute capabilities.\n\n\nPackage Management\nJulia comes with Pkg, a built-in package manger. With it, you can install packages, pin certain versions, recreate environments with the same set of dependencies, and upgrade/remove/develop packages easily. It’s one of the things that just works and makes Julia stand out versus alternative languages that don’t have a de-facto way of managing or installing packages.\nPackage installation is accomplished interactively in the REPL or executing commands.\n\nIn the REPL, you can change to the Package Management Mode by hitting ] and, e.g., add DataFrames CSV to install the two packages. Hit [backspace] to exit that mode in the REPL.\nThe same operation without changing REPL modes would be: using Pkg; Pkg.add([\"DataFrames\", \"CSV\"])\n\nRelated to packages, are environments which are a self-contained workspaces for your code. This lets you install only packages that are relevant to the current work. It also lets you ‘remember’ the exact set of packages and versions that you used. In fact, you can share the environment with others, and it will be able to recreate the same environment as when you ran the code. This is accomplished via a Project.toml file, which tracks the direct dependencies you’ve added, along with details about your project like its version number. The Manifest.toml tracks the entire dependency tree.\nReproducibility via the environment tools above is a really key aspect that will ensure Julia code is consistent across time and users, which is important for financial controls.\n\n\nEditors\nBecause Julia is very extensible and amenable to analysis of its own code, you can typically find plugins for whatever tool you prefer to write code in. A few examples:\n\nVisual Studio Code\nVisual Studio Code is a free editor from Microsoft. There’s a full-featured Julia plugin available, which will help with auto-completion, warnings, and other code hints that you might find in a dedicated editor (e.g. PyCharm or RStudio). Like those tools, you can view plots, search documentation, show datasets, debug, and manage version control.\n\n\nNotebooks\nNotebooks are typically more interactive environments than text editors - you can write code in cells and see the results side-by-side.\nThe most popular notebook tool is Jupyter (“Julia, Python, R”). It is widely used and fits in well with exploratory data analysis or other interactive workflows. It can be installed by adding the IJulia.jl package.\nPluto.jl is a newer tool, which adds reactivity and interactivity. It is also more amenable to version control than Jupyter notebooks because notebooks are saved as plain Julia scripts. Pluto is unique to Julia because of the language’s ability to introspect and analyze dependencies in its own code. Pluto also has built-in package/environment management, meaning that Pluto notebooks contains all the code needed to reproduce results (as long as Julia and Pluto are installed)."
  },
  {
    "objectID": "posts/bayesian-intro/index.html#a-whirlwind-tour-of-general-purpose-packages",
    "href": "posts/bayesian-intro/index.html#a-whirlwind-tour-of-general-purpose-packages",
    "title": "Getting Started with Julia for Actuaries",
    "section": "A Whirlwind Tour of General-Purpose Packages",
    "text": "A Whirlwind Tour of General-Purpose Packages\nThe Julia ecosystem favors composability and interoperability, enabled by multiple dispatch. In other words, because it’s easy to automatically specialize functionality based on the type of data being used, there’s much less need to bundle a lot of features within a single package.\nAs you’ll see, Julia packages tend to be less vertically integrated because it’s easier to pass data around. Counterexamples of this in Python and R:\n\nNumpy-compatible packages that are designed to work with a subset of numerically fast libraries in Python\nspecial functions in Pandas to read CSV, JSON, database connections, etc.\nThe Tidyverse in R has a tightly coupled set of packages that works well together but has limitations with some other R packages\n\nJulia is not perfect in this regard, but it’s neat to see how frequently things just work. It’s not magic, but because of Julia features outside the scope of this article it’s easy for package developers (and you!) to do this.\nJulia also has language-level support for documentation, so packages can follow a consistent style of help-text and have the docs be auto-generated into web pages available locally or online.\nThe following highlighted packages were chosen for their relevance to typical actuarial work, with a bias towards those used regularly by the authors. This is a small sampling of the over 6000 registered Julia Packages4\n\nData\nJulia offers a rich data ecosystem with a multitude of available packages. Perhaps at the center of the data ecosystem are CSV.jl and DataFrames.jl. CSV.jl is for reading and writing files text files (namely CSVs) and offers top-class read and write performance. DataFrames.jl is a mature package for working with dataframes, comparable to Pandas or dplyr.\nOther notable packages include ODBC.jl, which lets you connect to any database (given you have the right drivers installed), and Arrow.jl which implements the Apache Arrow standard in Julia.\nWorth mentioning also is Dates, a built-in package making date manipulation straightforward and robust.\nCheck out JuliaData org for more packages and information.\n\n\nPlotting\nPlots.jl is a meta-package providing an interface to consistently work with several plotting backends, depending if you are trying to emphasize interactivity on the web or print-quality output. You can very easily add animations or change almost any feature of a plot.\nStatsPlots.jl extends Plots.jl with a focus on data visualization and compatibility with dataframes.\nMakie.jl supports GPU-accelerated plotting and can create very rich, beautiful visualizations, but it’s main downside is that it has not yet been optimized to minimize the time-to-first-plot.\n\n\nStatistics\nJulia has first-class support for missing values, which follows the rules of three-valued logic so other packages don’t need to do anything special to incorporate missing values.\nStatsBase.jl and Distributions.jl are essentials for a range of statistics functions and probability distributions respectively.\nOthers include:\n\nTuring.jl, a probabilistic programming (Bayesian statistics) library, which is outstanding in its combination of clear model syntax with performance.\nGLM.jl for any type of linear modeling (mimicking R’s glm functionality).\nLsqFit.jl for fitting data to non-linear models.\nMultivariateStats.jl for multivariate statistics, such as PCA.\n\nYou can find more packages and learn about them here.\n\n\nMachine Learning\nFlux, Gen, Knet, and MLJ are all very popular machine learning libraries. There are also packages for PyTorch, Tensorflow, and SciKitML available. One advantage for users is that the Julia packages are written in Julia, so it can be easier to adapt or see what’s going on in the entire stack. In contrast to this design, PyTorch and Tensorflow are built primarily with C++.\nAnother advantage is that the Julia libraries can use automatic differentiation to optimize on a wider range of data and functions than those built into libraries in other languages.\n\n\nDifferentiable Programming\nSensitivity testing is very common in actuarial workflows: essentially, it’s understanding the change in one variable in relation to another. In other words, the derivative!\nJulia has unique capabilities where almost across the entire language and ecosystem, you can take the derivative of entire functions or scripts. For example, the following is real Julia code to automatically calculate the sensitivity of the ending account value with respect to the inputs:\njulia&gt; using Zygote\n\njulia&gt; function policy_av(pol)\n    COIs = [0.00319, 0.00345, 0.0038, 0.00419, 0.0047, 0.00532]\n    av = 0.0\n    for (i,coi) in enumerate(COIs)\n        av += av * pol.credit_rate\n        av += pol.annual_premium\n        av -= pol.face * coi\n    end\n    return av                # return the final account value\nend\n\njulia&gt; pol = (annual_premium = 1000, face = 100_000, credit_rate = 0.05);\n\njulia&gt; policy_av(pol)        # the ending account value\n4048.08\n\njulia&gt; policy_av'(pol)       # the derivative of the account value with respect to the inputs\n(annual_premium = 6.802, face = -0.0275, credit_rate = 10972.52)\nWhen executing the code above, Julia isn’t just adding a small amount and calculating the finite difference. Differentiation is applied to entire programs through extensive use of basic derivatives and the chain rule. Automatic differentiation, has uses in optimization, machine learning, sensitivity testing, and risk analysis. You can read more about Julia’s autodiff ecosystem here.\n\n\nUtilities\nThere are also a lot of quality-of-life packages, like Revise.jl which lets you edit code on the fly without needing to re-run entire scripts.\nBenchmarkTools.jl makes it incredibly easy to benchmark your code - simply add @benchmark in front of what you want to test, and you will be presented with detailed statistics. For example:\njulia&gt; using ActuaryUtilities, BenchmarkTools\n\njulia&gt; @benchmark present_value(0.05,[10,10,10])\n\nBenchmarkTools.Trial: 10000 samples with 994 evaluations.\n Range (min … max):  33.492 ns … 829.015 ns  ┊ GC (min … max): 0.00% … 95.40%\n Time  (median):     34.708 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   36.599 ns ±  33.686 ns  ┊ GC (mean ± σ):  4.40% ±  4.55%\n\n  ▁▃▆▆▆██▇▄▃▂         ▁                                        ▂\n  █████████████▆▆▇█▇████▇██▇█▇█▇▇▆▆▅▅▅▅▅▄▅▄▄▅▅▅▅▄▄▁▅▄▄▅▄▄▅▅▆▅▆ █\n  33.5 ns       Histogram: log(frequency) by time      45.6 ns &lt;\n\n Memory estimate: 112 bytes, allocs estimate: 1.\n\nTest is a built-in package for performing testsets, while Documenter.jl will build high-quality documentation based on your inline documentation.\nClipData.jl lets you copy and paste from spreadsheets to Julia sessions.\n\n\nOther packages\nJulia is a general-purpose language, so you will find packages for web development, graphics, game development, audio production, and much more. You can explore packages (and their dependencies) at https://juliahub.com/.\n\n\nActuarial packages\nSaving the best for last, the next article in the series will dive deeper into actuarial packages, such as those published by JuliaActuary for easy mortality table manipulation, common actuarial functions, financial math, and experience analysis."
  },
  {
    "objectID": "posts/bayesian-intro/index.html#getting-help",
    "href": "posts/bayesian-intro/index.html#getting-help",
    "title": "Getting Started with Julia for Actuaries",
    "section": "Getting Help",
    "text": "Getting Help\nAside from the usual StackOverflow, there is a community page with links to the Discourse forum, Slack, and Zulip. The latter two have a dedicated #actuary channel."
  },
  {
    "objectID": "posts/bayesian-intro/index.html#summary",
    "href": "posts/bayesian-intro/index.html#summary",
    "title": "Getting Started with Julia for Actuaries",
    "section": "Summary",
    "text": "Summary\nThis article introduced Julia, getting setup with running and editing, and pointed toward a number of general-purpose features and packages useful to actuaries. The next article in the series will focus on the range of actuarial-specific packages available."
  },
  {
    "objectID": "posts/bayesian-intro/index.html#footnotes",
    "href": "posts/bayesian-intro/index.html#footnotes",
    "title": "Getting Started with Julia for Actuaries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://ocw.mit.edu/courses/mathematics/18-335j-introduction-to-numerical-methods-spring-2019/week-1/Julia-intro.pdf↩︎\nhttps://discourse.julialang.org/t/claim-false-julia-isnt-multiple-dispatch-but-overloading/42370/114↩︎\nPerhaps benchmarking isn’t the best example because of the ‘magic’ %timeit keyword in Jupyter. However, the documentation for IPython itself reveals the limitations: “To Jupyter users: Magics are specific to and provided by the IPython kernel. Whether Magics are available on a kernel is a decision that is made by the kernel developer on a per-kernel basis. To work properly, Magics must use a syntax element which is not valid in the underlying language.”↩︎\nAs of July 2021.↩︎"
  },
  {
    "objectID": "posts/julia-for-actuaries/index.html",
    "href": "posts/julia-for-actuaries/index.html",
    "title": "Julia for Actuaries",
    "section": "",
    "text": "I have suggested that actuaries who are competent coders will differentiate both themselves and the companies they work for. Coding ability will be useful no matter what tools you utilize every day (e.g., Python/R/C++/etc. and associated packages) and all of those tools and communities contribute to moving actuarial processes out of the Spreadsheet Age.\nThere’s a newer programming language called Julia, and in this article, I’d like to motivate why Julia is worth considering for actuarial work."
  },
  {
    "objectID": "posts/julia-for-actuaries/index.html#julia-overview",
    "href": "posts/julia-for-actuaries/index.html#julia-overview",
    "title": "Julia for Actuaries",
    "section": "Julia Overview",
    "text": "Julia Overview\nJulia is relatively new1, and it shows. It is evident in its pragmatic, productivity-focused design choices, pleasant syntax, rich ecosystem, thriving communities, and its ability to be both very general purpose and power cutting edge computing.\nWith Julia: math-heavy code looks like math; it’s easy to pick up, and quick-to-prototype. Packages are well-integrated, with excellent visualization libraries and pragmatic design choices.\nJulia’s popularity continues to grow across many fields and there’s a growing body of online references and tutorials, videos, and print media to learn from.\nLarge financial services organizations have already started realizing gains: BlackRock’s Aladdin portfolio modeling, the Federal Reserve’s economic simulations, and Aviva’s Solvency II-compliant modeling. The last of these has a great talk on YouTube by Aviva’s Tim Thornham, which showcases an on-the-ground view of what difference the right choice of technology and programming language can make. Moving from their vendor-supplied modeling solution was 1000x faster, took 1/10 the amount of code, and was implemented 10x faster2.\nThe language is not just great for data science — but also modeling, ETL, visualizations, package control/version management, machine learning, string manipulation, web-backends, and many other use cases."
  },
  {
    "objectID": "posts/julia-for-actuaries/index.html#for-the-actuary",
    "href": "posts/julia-for-actuaries/index.html#for-the-actuary",
    "title": "Julia for Actuaries",
    "section": "For the Actuary",
    "text": "For the Actuary\nJulia is well suited for actuarial work: easy to read and write and very performant for large amounts of data/modeling.\n\nExpressiveness and Syntax\nExpressiveness is the manner in which and scope of ideas and concepts that can be represented in a programming language. Syntax refers to how the code looks on the screen and its readability.\nIn a language with high expressiveness and pleasant syntax, you:\n\nGo from idea in your head to final product faster.\nEncapsulate concepts naturally and write concise functions.\nCompose functions and data naturally.\nFocus on the end-goal instead of fighting the tools.\n\nExpressiveness can be hard to explain, but perhaps two short examples will illustrate.\n\nExample: Retention Analysis\nThis is a really simple example relating Cessions, Policys, and Lives to do simple retention analysis.\nFirst, let’s define our data:\n\n# Define our data structures\nstruct Life\n  policies\nend\n\nstruct Policy\n  face\n  cessions\n end\n\nstruct Cession\n  ceded\nend\nNow to calculate amounts retained. First, let’s say what retention means for a Policy:\n# define retention\nfunction retained(pol::Policy)\n  pol.face - sum(cession.ceded for cession in pol.cessions)\nend\nAnd then what retention means for a Life:\nfunction retained(l::Life)\n  sum(retained(policy) for policy in life.policies)\nend\nIt’s almost exactly how you’d specify it English. No joins, no boilerplate, no fiddling with complicated syntax. You can express ideas and concepts the way that you think of them, not, for example, as a series of dataframe joins or as row/column coordinates on a spreadsheet.\nWe defined retained and adapted it to mean related, but different things depending on the specific context. That is, we didn’t have to define retained_life(...) and retained_pol(...) because Julia can be dispatch based on what you give it. This is, as some would call it, unreasonably effective.\nLet’s use the above code in practice then. First, create two policies with two and one cessions respectively:\n\n\n\n\n\n\nNote\n\n\n\nThe julia&gt; syntax indicates that we’ve moved into Julia’s interactive mode (REPL mode):\n\n\njulia&gt; pol_1 = Policy( 1000, [ Cession(100), Cession(500)] )\njulia&gt; pol_2 = Policy( 2500, [ Cession(1000) ] )\nThen, create a life, which has the two policies:\njulia&gt; life = Life([pol_1, pol_2])\nAnd calculate the associated retention:\njulia&gt; retained(pol_1)\n400\njulia&gt; retained(life)\n1900\nAnd for the last trick, something called “broadcasting”, which automatically vectorizes any function you write, no need to write loops or create if statements to handle a single vs repeated case:\njulia&gt; retained.(life.policies) # retained amount for each policy\n[400 ,  1500]\n\n\nExample: Random Sampling\nAs another motivating example showcasing multiple dispatch, here’s random sampling in Julia, R, and Python.\nWe generate 100:\n\nUniform random numbers\nStandard normal random numbers\nBernoulli random number\nRandom samples with a given set\n\n\nA comparison of random sampling between Julia, R, and Python\n\n\n\n\n\n\n\nJulia\nR\nPython\n\n\n\n\nusing Distributions\n\nrand(100)\nrand(Normal(), 100)\nrand(Bernoulli(0.5), 100)\nrand([\"Preferred\",\"Standard\"], 100)\nrunif(100)\nrnorm(100)\nrbern(100, 0.5)\nsample(c(\"Preferred\",\"Standard\"),\n100, replace=TRUE)\nimport scipy.stats as sps\nimport numpy as np\n\n\nsps.uniform.rvs(size=100)\nsps.norm.rvs(size=100)\nsps.bernoulli.rvs(p=0.5,size=100)\nnp.random.choice([\"Preferred\",\"Standard\"],\nsize=100)\n\n\n\nBy understanding the different types of things passed to rand(), it maintains the same syntax across a variety of different scenarios. We could define rand(Cession) and have it generate a random Cession like we used above.\n\n\n\nThe Speed\nAs the journal Nature said, “Come for the Syntax, Stay for the Speed”.\nRecall the Solvency II compliance which ran 1000x faster than the prior vendor solution mentioned earlier: what does it mean to be 1000x faster at something? It’s the difference between something taking 10 seconds instead of 3 hours — or 1 hour instead of 42 days.\nWhat analysis would you like to do if it took less time? A stochastic analysis of life-level claims? Machine learning with your experience data? Daily valuation instead of quarterly?\nSpeaking from experience, speed is not just great for production time improvements. During development, it’s really helpful too. When building something, I can see that I messed something up in a couple of seconds instead of 20 minutes. The build, test, fix, iteration cycle goes faster this way.\nAdmittedly, most workflows don’t see a 1000x speedup, but 10x to 1000x is a very common range of speed differences vs R or Python or MATLAB.\nSometimes you will see less of a speed difference; R and Python have already circumvented this and written much core code in low-level languages. This is an example of what’s called the “two-language” problem where the language productive to write in isn’t very fast. For example, more than half of R packages use C/C++/Fortran and core packages in Python like Pandas, PyTorch, NumPy, SciPy, etc. do this too.\nWithin the bounds of the optimized R/Python libraries, you can leverage this work. Extending it can be difficult: what if you have a custom retention management system running on millions of policies every night?\nJulia packages you are using are almost always written in pure Julia: you can see what’s going on, learn from them, or even contribute a package of your own!\n\n\nMore of Julia’s benefits\nJulia is easy to write, learn, and be productive in:\n\nIt’s free and open-source\n\nVery permissive licenses, facilitating the use in commercial environments (same with most packages)\n\nLarge and growing set of available packages\nWrite how you like because it’s multi-paradigm: vectorizable (R), object-oriented (Python), functional (Lisp), or detail-oriented (C)\nBuilt-in package manager, documentation, and testing-library\nJupyter Notebook support (it’s in the name! Julia-Python-R)\nMany small, nice things that add up:\n\nUnicode characters like α or β\nNice display of arrays\nSimple anonymous function syntax\nWide range of text editor support\nFirst-class support for missing values across the entire language\nLiterate programming support (like R-Markdown)\n\nBuilt-in Dates package that makes working with dates pleasant\nAbility to directly call and use R and Python code/packages with the PyCall and RCall packages\nError messages are helpful and tell you what line the error came from, not just the type of error\nDebugger functionality so you can step through your code line by line\n\nFor power-users, advanced features are easily accessible: parallel programming, broadcasting, types, interfaces, metaprogramming, and more.\nThese are some of the things that make Julia one of the world’s most loved languages on the StackOverflow Developer Survey.\nFor those who are enterprise-minded: in addition to the liberal licensing mentioned above, there are professional products from organizations like Julia Computing that provide hands-on support, training, IT governance solutions, behind-the-firewall package management, and deployment/scaling assistance.\n\n\nThe Tradeoff\nJulia is fast because it’s compiled, unlike R and Python where (loosely speaking) the computer just reads one line at a time. Julia compiles code “just-in-time”: right before you use a function for the first time, it will take a moment to pre-process the code section for the machine. Subsequent calls don’t need to be re-compiled and are very fast.\nA hypothetical example: running 10,000 stochastic projections where Julia needs to precompile but then runs each 10x faster:\n\nJulia runs in 2 minutes: the first projection takes 1 second to compile and run, but each 9,999 remaining projections only take 10ms.\nPython runs in 17 minutes: 100ms of a second for each computation.\n\nTypically, the compilation is very fast (milliseconds), but in the most complicated cases it can be several seconds. One of these is the “time-to-first-plot” issue because it’s the most common one users encounter: super-flexible plotting libraries have a lot of things to pre-compile. So in the case of plotting, it can take several seconds to display the first plot after starting Julia, but then it’s remarkably quick and easy to create an animation of your model results. The time-to-first plot is a solvable problem that’s receiving a lot of attention from the core developers and will get better with future Julia releases.\nFor users working with a lot of data or complex calculations (like actuaries!), the runtime speedup is worth a few seconds at the start.\n\n\nPackage Ecosystem\nUsing packages as dependencies in your project is assisted by Julia’ bundled package manager.\nFor each project, you can track the exact set of dependencies and replicate the code/process on another machine or another time. In R or Python, dependency management is notoriously difficult and it’s one of the things that the Julia creators wanted to fix from the start.\nPackages can be one of the thousands of publicly available, or private packages hosted internally behind a firewall.\nAnother powerful aspect of the package ecosystem is that due to the language design, packages can be combined/extended in ways that are difficult for other common languages. This means that Julia packages often interop without any additional coordination.\nFor example, packages that operate on data tables work without issue together in Julia. In R/Python, many features tend to come bundled in a giant singular package like Python’s Pandas which has Input/Output, Date manipulation, plotting, resampling, and more. There’s a new Consortium for Python Data API Standards which seeks to harmonize the different packages in Python to make them more consistent (R’s Tidyverse plays a similar role in coordinating their subset of the package ecosystem).\nIn Julia, packages tend to be more plug-and-play. For example, every time you want to load a CSV you might not want to transform the data into a dataframe (maybe you want a matrix or a plot instead). To load data into a dataframe, in Julia the practice is to use both the CSV and DataFrames packages, which help separate concerns. Some users may prefer the Python/R approach of less modular but more all-inclusive packages.\nSome highlighted/recommended packages:\n\nActuarial Specific (part of the JuliaActuary.org umbrella3)\n\nMortalityTables – Common tables and parametric models with survivorship calculations\nActuaryUtilities – Robust and fast calculations for common functions\nLifeContingencies - Insurance, annuity, premium, and reserve maths.\n\nData Science and Statistics\n\nDataFrames – Work with datasets – similar to R’s data.table and able to handle much larger datasets than Python’s Pandas4\nDistributions – Common and exotic statistical distributions\nGLM – Generalized Linear Models\nTuring – Bayesian statistics like STAN\nGen – Probabilistic programming\nOnlineStats – Single-pass algorithms for real-time/large data\nCSV – The fastest CSV reader\nODBC – Database Connections\nDates – Robust date types and functions\n\nMachine Learning\n\nFlux – Elegant, GPU-powered ML\nKnet – Deep learning framework\nMLJ – ML models\n\nNotebooks\n\nIJulia – the Julia kernel for Jupyter notebooks\nPluto – reactive/interactive notebooks that address some of the biggest complaints with Jupyter\n\nVisualization\n\nPlots – Powerful but user-friendly plots and animations\nQueryverse – Tidyverse-like data manipulation and plotting\n\nDashboards\n\nPlot.ly Dash\n\nMiscellaneous\n\nOptim – Uni/Multivariate function optimization\nLinearAlgebra – Built-in library for working with arrays/matrices\nJuMP – Linear, Nonlinear, and other advanced optimization\nCUDA – GPU programming made easier\nRevise – Edit code while you work on it\n\nInteroperability\n\nPyCall – use existing Python code/libraries inside Julia\nRCall – use existing R code/libraries inside Julia\n\nWeb\n\nHTTP – Core web utilities\nGenie – Full application framework\nFranklin – Flexible Static Site Generator\n\nDocumentation\n\nWeave/Literate – Literate programming like RMarkdown\nDocumenter – Write your documentation as comments to your code and produce full docpages\n\n\nAnd finally, some general resources to get started:\n\nJuliaLang.org, the home site with the downloads to get started and links to learning resources.\nJuliaHub indexes open-source Julia packages and makes the entire ecosystem and documentation searchable from one place.\nJuliaAcademy, which has free short courses in Data Science, Introduction to Julia, DataFrames.jl, Machine Learning, and more.\nData Wrangling with DataFrames Cheat Sheet\nLearn Julia in Y minutes, a great quick-start if you are already comfortable with coding.\nThink Julia, a free e-book (or paid print edition) book which introduces programming from the start and teaches you valuable ways of thinking.\nDesign Patterns and Best Practices, a book that will help you as you transition from smaller, one-off scripts to designing larger packages and projects."
  },
  {
    "objectID": "posts/julia-for-actuaries/index.html#conclusion",
    "href": "posts/julia-for-actuaries/index.html#conclusion",
    "title": "Julia for Actuaries",
    "section": "Conclusion",
    "text": "Conclusion\nLooking at other great tools like R and Python, it can be difficult to summarize a single reason to motivate a switch to Julia, but hopefully this article piqued an interest to try it for your next project.\nThat said, Julia shouldn’t be the only tool in your tool-kit. SQL will remain an important way to interact with databases. R and Python aren’t going anywhere in the short term and will always offer a different perspective on things!\nIn an earlier article, I talked about becoming a 10x Actuary which meant being proficient in the language of computers so that you could build and implement great things. In a large way, the choice of tools and paradigms shape your focus. Productivity is one aspect, expressiveness is another, speed one more. There are many reasons to think about what tools you use and trying out different ones is probably the best way to find what works best for you.\nIt is said that you cannot fully conceptualize something unless your language has a word for it. Similar to spoken language, you may find that breaking out of spreadsheet coordinates (and even a dataframe-centric view of the world) reveals different questions to ask and enables innovated ways to solve problems. In this way, you reward your intellect while building more meaningful and relevant models and analysis."
  },
  {
    "objectID": "posts/julia-for-actuaries/index.html#footnotes",
    "href": "posts/julia-for-actuaries/index.html#footnotes",
    "title": "Julia for Actuaries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPython first appeared in 1990. R is an implementation of S, which was created in 1976, though depending on when you want to place the start of an independent R project varies (1993, 1995, and 2000 are alternate dates). The history of these languages is long and substantial changes have occurred since these dates.↩︎\nAviva Case Study↩︎\nThe author of this article contributes to JuliaActuary.↩︎\nH20 AI Benchmark↩︎"
  },
  {
    "objectID": "posts/poisson-approximation/index.html",
    "href": "posts/poisson-approximation/index.html",
    "title": "Poisson approximation to Binomial",
    "section": "",
    "text": "This notebook explores using the Poissson approximation to the Binomial. This can be useful for a number of reasons:\nWe will look at the approximation across a range of parameters q (the probabilty a binomial event occurs) and N (the number of chances for the event to occur, or “exposures”).\nUnder certain conditions, the Poisson distribution can approximate the Binomial, where the average number of events, λ=N*q.\nWhat are those conditions? According to Wikipedia:\nIt’s not really that simple, and arguably a bit restrictive as this analysis will show."
  },
  {
    "objectID": "posts/poisson-approximation/index.html#approach",
    "href": "posts/poisson-approximation/index.html#approach",
    "title": "Poisson approximation to Binomial",
    "section": "Approach",
    "text": "Approach\nWe will use Julia and Turing.jl to simulate the posterior distribution. Our prior for the parameter $q$ will be ${Uniform} = Beta(1,1)$.\nWe will sample from the posterior using the No-U-Turn (NUTS) sampler, and aggregate the results of the chains over $(trials) trials of simulated outcomes for the given $q$ and $N$.\nThis is overkill for a toy problem where we could just model the parameters themselves, but it demonstrates using Bayesian MCMC techniques in a simple, exploratory fashion.\nWe begin by importing the relevant packages:\n\nusing Turing\nusing CairoMakie\nusing StatsBase\nusing MCMCChains\nusing DataFrames\nusing ThreadsX\nusing Logging\nusing Markdown\n\n““”"
  },
  {
    "objectID": "posts/poisson-approximation/index.html#define-the-models",
    "href": "posts/poisson-approximation/index.html#define-the-models",
    "title": "Poisson approximation to Binomial",
    "section": "Define the Models",
    "text": "Define the Models\n\n@model function poisson(N,n_events) \n    q ~ Beta(1,1)\n    \n    n_events ~ Poisson(q*N)\nend\n\n@model function binom(N,n_events) \n    q ~ Beta(1,1)\n    \n    n_events ~ Binomial(N,q)\nend\n\nbinom (generic function with 2 methods)"
  },
  {
    "objectID": "posts/poisson-approximation/index.html#simulation-parameters",
    "href": "posts/poisson-approximation/index.html#simulation-parameters",
    "title": "Poisson approximation to Binomial",
    "section": "Simulation Parameters",
    "text": "Simulation Parameters\n\ntrials = 30\n\n# true probabilities\nqs = [0.05,0.25,0.5,0.75,0.95]\n\n# number of observations\nNs = [10,25,50,100,250,500,1000,5000]\n\n# combine q and N into joint model points\nmodel_points =  [(;q,N) for q in qs, N in Ns]\n\n5×8 Matrix{@NamedTuple{q::Float64, N::Int64}}:\n (q = 0.05, N = 10)  (q = 0.05, N = 25)  …  (q = 0.05, N = 5000)\n (q = 0.25, N = 10)  (q = 0.25, N = 25)     (q = 0.25, N = 5000)\n (q = 0.5, N = 10)   (q = 0.5, N = 25)      (q = 0.5, N = 5000)\n (q = 0.75, N = 10)  (q = 0.75, N = 25)     (q = 0.75, N = 5000)\n (q = 0.95, N = 10)  (q = 0.95, N = 25)     (q = 0.95, N = 5000)"
  },
  {
    "objectID": "posts/poisson-approximation/index.html#sample-from-the-posterior",
    "href": "posts/poisson-approximation/index.html#sample-from-the-posterior",
    "title": "Poisson approximation to Binomial",
    "section": "Sample from the Posterior",
    "text": "Sample from the Posterior\nThis is a collection of samples (chains) from Markov chains that are sampled in proportion to the posterior density. If this is new to you, I highly recommend the book Statistical Rethinking:\n\nLogging.disable_logging(Logging.Warn); #Disable warning logs to improve sampling time\n\nbpchains = map(model_points) do mp\n    \n    ThreadsX.map(1:trials) do i\n        claims = sum(rand() &lt; mp.q for _ in 1:mp.N)\n        bc = sample(binom(mp.N,claims), NUTS(), 500)\n        pc = sample(poisson(mp.N,claims), NUTS(), 500)\n\n        (;bc,pc)\n    end\nend\n\n5×8 Matrix{Vector{@NamedTuple{bc::Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, @NamedTuple{parameters::Vector{Symbol}, internals::Vector{Symbol}}, @NamedTuple{varname_to_symbol::OrderedDict{AbstractPPL.VarName, Symbol}, start_time::Float64, stop_time::Float64}}, pc::Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, @NamedTuple{parameters::Vector{Symbol}, internals::Vector{Symbol}}, @NamedTuple{varname_to_symbol::OrderedDict{AbstractPPL.VarName, Symbol}, start_time::Float64, stop_time::Float64}}}}}:\n [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]  …  [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]\n [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]     [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]\n [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]     [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]\n [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]     [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]\n [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]     [(bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))  …  (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3})), (bc = MCMC chain (500×13×1 Array{Float64, 3}), pc = MCMC chain (500×13×1 Array{Float64, 3}))]"
  },
  {
    "objectID": "posts/poisson-approximation/index.html#results",
    "href": "posts/poisson-approximation/index.html#results",
    "title": "Poisson approximation to Binomial",
    "section": "Results",
    "text": "Results\nThe results indicate that the Poisson is a good fit when $q$ is small, where “small” depends on N, but in general it seems to provide a good fit in less restrictive cases than the “rule of thumb” quoted below. E.g. go ahead and use the Poisson approximation when you’ve got enough expsoures even if $q$ is well above $0.10$. The Poisson approximation also isn’t terrible when $N$ is as low as 10 as long as $q$ is very small (e.g. $&lt;0.05$ ).\nThe fit remains poor when $q &gt;&gt; 0.5$ and when $N$ is small.\n\nVisualization\nThis visualization shows the aggregated posterior distribution across all of the trials and model points. The darker shaded band indicates the middle 50% of the posterior:\n\nfunction plot_band_under!(ax,plot,y,low,high,label=\"\")\n    function points(plot)\n        pts = plot.plots[2].converted[1][]\n        [p[1] for p in pts], [p[2] for p in pts]\n    end    \n    xs′, ys′ = points(plot)\n\n    filt = findall(x-&gt; (x ≥ low) && (x ≤ high),xs′)\n    typeof(xs′[filt]), typeof(fill(y,length(filt))), typeof(ys′[filt])\n    b = band!(\n        ax,\n        Float64.(xs′[filt]), \n        Float64.(fill(y * 1.0, length(filt))), \n        Float64.(ys′[filt]),\n        color=(plot.color.val, 0.9),transparency=true,shading = false) # 0.25 alpha\n    translate!(b,0,0,5)\nend\n\n\nlet\n    f = Figure(resolution=(1280,960))\n    a = Any # outer variable to set as axis for to grab legend\n    \n    for (i,N) in enumerate(reverse(Ns))\n        ax = Axis(f[i+1,1:10],\n        xticks=qs,\n        ylabel=\"N=$N\"\n        )\n        a = ax\n        xlims!(0,1)\n        model_idx = findall(x-&gt;x.N == N,model_points)\n\n        for n in model_idx\n            c = bpchains[n]\n            q = model_points[n].q\n            y = model_points[n].N\n            bpoints = vcat([x.bc[\"q\"][:] for x in c]...)\n            ppoints = vcat([x.pc[\"q\"][:] for x in c]...)\n            bqtls = quantile(bpoints,[.25,.75])\n            pqtls = quantile(ppoints,[.25,.75])\n            j = density!(\n                bpoints,\n                # linewidth=10,\n                strokewidth = 1, \n                strokecolor = (:grey30,0.6),\n                label=\"Binomial\",\n                color=(:red,.25),\n            )\n            plot_band_under!(ax,j,0,bqtls[1],bqtls[2],\"Binomial\")\n        \n\n            j = density!(\n                ppoints, \n                color=(:blue,.25),\n                # linewidth=10,\n                strokewidth = 1, \n                strokecolor = (:grey30,0.6),\n                label=\"Poisson\",\n            )\n            plot_band_under!(ax,j,0,pqtls[1],pqtls[2],\"Poisson\")\n\n            hideydecorations!(ax,label=false)\n        end\n\n        scatter!(\n            qs,\n            zeros(length(qs)),\n            marker = :vline,\n            markersize=20,\n            color=:grey30,\n            label=\"actual value\"\n            )\n    end\n\n    Legend(f[1,5],a,unique=true,orientation=:horizontal)\n    f\nend"
  },
  {
    "objectID": "posts/fitting-yield-curves/index.html",
    "href": "posts/fitting-yield-curves/index.html",
    "title": "Fitting Rate Data to Yield Curves",
    "section": "",
    "text": "Given rates and maturities, we can fit the yield curves with different techniques.\nBelow, we specify that the rates should be interpreted as Continuously compounded zero rates:\nusing FinanceModels\nusing Plots\nrates = Continuous.([0.01, 0.01, 0.03, 0.05, 0.07, 0.16, 0.35, 0.92, 1.40, 1.74, 2.31, 2.41] ./ 100)\nmats = [1 / 12, 2 / 12, 3 / 12, 6 / 12, 1, 2, 3, 5, 7, 10, 20, 30]\n\n12-element Vector{Float64}:\n  0.08333333333333333\n  0.16666666666666666\n  0.25\n  0.5\n  1.0\n  2.0\n  3.0\n  5.0\n  7.0\n 10.0\n 20.0\n 30.0\nThe above rates and associated maturities represent prices of zero coupon bonds, which we use as the financial instrument that we will fit the curve to:\nquotes = ZCBYield.(rates, mats)\n\n12-element Vector{Quote{Float64, Cashflow{Float64, Float64}}}:\n Quote{Float64, Cashflow{Float64, Float64}}(0.9999916667013888, Cashflow{Float64, Float64}(1.0, 0.08333333333333333))\n Quote{Float64, Cashflow{Float64, Float64}}(0.9999833334722215, Cashflow{Float64, Float64}(1.0, 0.16666666666666666))\n Quote{Float64, Cashflow{Float64, Float64}}(0.9999250028124297, Cashflow{Float64, Float64}(1.0, 0.25))\n Quote{Float64, Cashflow{Float64, Float64}}(0.999750031247396, Cashflow{Float64, Float64}(1.0, 0.5))\n Quote{Float64, Cashflow{Float64, Float64}}(0.9993002449428433, Cashflow{Float64, Float64}(1.0, 1.0))\n Quote{Float64, Cashflow{Float64, Float64}}(0.9968051145430329, Cashflow{Float64, Float64}(1.0, 2.0))\n Quote{Float64, Cashflow{Float64, Float64}}(0.9895549325678993, Cashflow{Float64, Float64}(1.0, 3.0))\n Quote{Float64, Cashflow{Float64, Float64}}(0.9550419621907147, Cashflow{Float64, Float64}(1.0, 5.0))\n Quote{Float64, Cashflow{Float64, Float64}}(0.9066489037539209, Cashflow{Float64, Float64}(1.0, 7.0))\n Quote{Float64, Cashflow{Float64, Float64}}(0.8402968976584314, Cashflow{Float64, Float64}(1.0, 10.0))\n Quote{Float64, Cashflow{Float64, Float64}}(0.6300223399419124, Cashflow{Float64, Float64}(1.0, 20.0))\n Quote{Float64, Cashflow{Float64, Float64}}(0.4852941873885002, Cashflow{Float64, Float64}(1.0, 30.0))\nFitting is then calling fit along with the desired curve construction technique. Here are several variants:\nns = fit(Yield.NelsonSiegel(), quotes);\nnss = fit(Yield.NelsonSiegelSvensson(), quotes);\nsw = fit(Yield.SmithWilson(ufr=0.05, α=0.1), quotes);\nbl = fit(Spline.Linear(), quotes, Fit.Bootstrap());\nbq = fit(Spline.Quadratic(), quotes, Fit.Bootstrap());\nbc = fit(Spline.Cubic(), quotes, Fit.Bootstrap());\nThat’s it! We’ve fit the rates using six different techniques. These can now be used in a variety of ways, such as calculating the present_value, duration, or convexity of different cashflows if you imported ActuaryUtilities.jl”"
  },
  {
    "objectID": "posts/fitting-yield-curves/index.html#visualizing-the-results",
    "href": "posts/fitting-yield-curves/index.html#visualizing-the-results",
    "title": "Fitting Rate Data to Yield Curves",
    "section": "Visualizing the results",
    "text": "Visualizing the results\n\n\"\"\"\nA helper function to plot the given curve onto the given plot figure\n\"\"\"\nfunction curveplot!(plot_fig, curve; label=\"\", alpha=alpha)\n    maturities = 0.25:0.25:40\n    f(x) = rate(zero(curve, x))\n\n    plot!(plot_fig, maturities, f, label=label, line=3, alpha=alpha)\nend\n\n\"\"\" \na function to plot the curves, given different alpha transparency for each of the lines (used when creating animiation)\n\"\"\"\nfunction p(alpha=[1, 1, 1, 1, 1, 1])\n    theme(:wong2)\n    p = plot(legend=:topleft, xlabel=\"Tenor\", ylabel=\"Continuous yield\", grid=false)\n    scatter!(\n        mats,\n        rate.(Continuous().(rates)),\n        label=\"Given zero rates\"\n    )\n\n    curveplot!(p, bc, label=\"Bootstrap (Cubic)\", alpha=alpha[1])\n    curveplot!(p, bq, label=\"Bootstrap (Quadratic)\", alpha=alpha[2])\n    curveplot!(p, bl, label=\"Bootstrap (Linear)\", alpha=alpha[3])\n    curveplot!(p, ns, label=\"NelsonSiegel\", alpha=alpha[4])\n    curveplot!(p, nss, label=\"NelsonSiegelSvensson\", alpha=alpha[5])\n    curveplot!(p, sw, label=\"SmithWilson\", alpha=alpha[6])\n    lens!([0, 3.5], [0.0, 0.0045], inset=(1, bbox(0.5, 0.5, 0.4, 0.4)))\n\n\nend\np()\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd an animated version:\n\nanim = let\n    a = [1, 0.25, 0.25, 0.25, 0.25, 0.25]\n    anim = @animate for i in 1:6\n        a = circshift(a, 1) # shift the transparency\n        p(a)\n    end\n    anim\nend\n\ngif(anim, \"anim_fps2.gif\", fps=2)"
  },
  {
    "objectID": "posts/fitting-survival-data/index.html",
    "href": "posts/fitting-survival-data/index.html",
    "title": "Fitting survival data with MortaltityTables.jl",
    "section": "",
    "text": "begin\n    using LsqFit\n    using MortalityTables\n    using Plots\n    using Distributions\n    using Optim\n    using DataFrames\n    using Survival\nend\nThis tutorial is via PharmCat on Github (this link has similar code with comments in Russian and English)."
  },
  {
    "objectID": "posts/fitting-survival-data/index.html#fitting-a-weibull-survival-curve",
    "href": "posts/fitting-survival-data/index.html#fitting-a-weibull-survival-curve",
    "title": "Fitting survival data with MortaltityTables.jl",
    "section": "Fitting a Weibull survival curve",
    "text": "Fitting a Weibull survival curve\nSample data:\n\ndata = let\n    survival = [0.99, 0.98, 0.95, 0.9, 0.8, 0.65, 0.5, 0.38, 0.25, 0.2, 0.1, 0.05, 0.02, 0.01]\n    times = 1:length(survival)\n    DataFrame(; times, survival)\nend\n\n14×2 DataFrame\n\n\n\nRow\ntimes\nsurvival\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1\n0.99\n\n\n2\n2\n0.98\n\n\n3\n3\n0.95\n\n\n4\n4\n0.9\n\n\n5\n5\n0.8\n\n\n6\n6\n0.65\n\n\n7\n7\n0.5\n\n\n8\n8\n0.38\n\n\n9\n9\n0.25\n\n\n10\n10\n0.2\n\n\n11\n11\n0.1\n\n\n12\n12\n0.05\n\n\n13\n13\n0.02\n\n\n14\n14\n0.01\n\n\n\n\n\n\nVisualizing the data:\n\nplt = plot(data.times, data.survival, label=\"observed survival proportion\", xlabel=\"time\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefine the two-parameter Weibull model:\n\nx: array of independent variables\np: array of model parameters\n\nmodel(x, p) will accept the full data set as the first argument x. This means that we need to write our model function so it applies the model to the full dataset. We use @. to apply (“broadcast”) the calculations across all rows.\n\n@. model1(x, p) = survival(MortalityTables.Weibull(; m=p[1], σ=p[2]), x)\n\nmodel1 (generic function with 1 method)\n\n\n\nFitting the Model\nAnd fit the model with LsqFit.jl:\n\nfit1 = curve_fit(model1, data.times, data.survival, [1.0, 1.0])\n\nplot!(plt, data.times, model1(data.times, fit1.param), label=\"fitted model\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood estimation\nGenerate 100 sample datapoints:\n\nt = rand(Weibull(fit1.param[2], fit1.param[1]), 100)\n\n100-element Vector{Float64}:\n  8.825787337622568\n 16.569618848181783\n 12.956834421711289\n  6.555400926197694\n  8.504194949892344\n  8.713067857978032\n  6.902907682735063\n  2.9238737618887756\n  6.220224938590163\n  5.637392847808691\n  6.566266756942628\n  3.4227977121519046\n  9.072382858601902\n  ⋮\n 12.427306670864814\n 13.020181642791094\n  8.937824930348103\n  7.384760880970262\n  8.62992521964653\n  9.064473129442108\n  7.784665870395106\n  8.063042056800445\n  3.4419321614824834\n 10.237415171326523\n  6.022490383277525\n  7.869905502329432\n\n\n\n\nWithout Censored Data”\n\n#No censored data\nfit_mle(Weibull, t)\n\nWeibull{Float64}(α=2.624123641855999, θ=8.249520319926305)\n\n\n\n\nWith Censored Data\nPick some arbitrary observations to censor:\n\nc = collect(trues(100))\nc[[1, 3, 7, 9]] .= false\n\n4-element view(::Vector{Bool}, [1, 3, 7, 9]) with eltype Bool:\n 0\n 0\n 0\n 0\n\n\n\n#ML function\nsurvmle(x) = begin\n    ml = 0.0\n    for i = 1:length(t)\n        if c[i]\n            ml += logpdf(Weibull(x[2], x[1]), t[i]) #if not censored log(f(x))\n        else\n            ml += logccdf(Weibull(x[2], x[1]), t[i]) #if censored log(1-F)\n        end\n    end\n    -ml\nend\n\nopt = Optim.optimize(\n    survmle,          # function to optimize\n    [1.0, 1.0], # lower bound\n    [15.0, 15.0],            # upper bound\n    [3.0, 3.0]          # initial guess\n)\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     2.447550e+02\n\n * Found with\n    Algorithm:     Fminbox with L-BFGS\n\n * Convergence measures\n    |x - x'|               = 9.89e-08 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.13e-08 ≰ 0.0e+00\n    |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n    |g(x)|                 = 2.72e-09 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    4\n    f(x) calls:    53\n    ∇f(x) calls:   53\n\n\nThe solution converges to similar values as the function generating the synthetic data:\n\nOptim.minimizer(opt)\n\n2-element Vector{Float64}:\n 8.36026550865313\n 2.5856602163415103"
  },
  {
    "objectID": "posts/fitting-survival-data/index.html#fitting-kaplan-meier",
    "href": "posts/fitting-survival-data/index.html#fitting-kaplan-meier",
    "title": "Fitting survival data with MortaltityTables.jl",
    "section": "Fitting Kaplan Meier",
    "text": "Fitting Kaplan Meier\nKaplanMeier comes from Survival.jl.\n\n#t- time vector;c - censored events vector\nkm = fit(Survival.KaplanMeier, t, c)\n\nplt2 = plot(km.events.time, km.survival; labels=\"Empirical\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@. model(x, p) = survival(MortalityTables.Weibull(; m=p[1], σ=p[2]), x)\n\nmfit = LsqFit.curve_fit(model, km.events.time, km.survival, [2.0, 2.0])\n\nplot!(plt2, km.events.time, model(km.events.time, mfit.param), labels=\"Theoretical\")"
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html",
    "href": "posts/getting-started-for-actuaries/index.html",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "",
    "text": "One of the first probabilistic theorems everyone learns is Bayes’ Theorem, but that theorem is conspicuously absent from most applications and practice. The reason for this is that outside of trivial introductory examples (“you tested positive for a disease…”) is that Bayes’ Theorem becomes intractably complicated to calculate the posterior distribution. Modern advances in computing power, algorithms, and open-source libraries have made it possible to start applying the most powerful theorem to much more complex problems.\nThe advantage of this is that actuaries can now apply these techniques to problems in a principled and flexible way to understand uncertainty better than we have before by explicitly looking at the posterior distribution of the parameters in our model. This would serve to enhance risk management via moving away from singular estimates of model parameters to a culture of considering the parameter distribution of modeled risk."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#what-is-modern-bayesian-statistics",
    "href": "posts/getting-started-for-actuaries/index.html#what-is-modern-bayesian-statistics",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "What is Modern Bayesian Statistics?",
    "text": "What is Modern Bayesian Statistics?\nA Bayesian statistical model has four main components to focus on:\n\nPrior encoding assumptions about the random variables related to the problem at hand, before conditioning on the data.\nA Model that defines how the random variables give rise to the observed outcome.\nData that we use to update our prior assumptions.\nPosterior distributions of our random variables, conditioned on the observed data and our model\n\nHaving defined the first two components and collected our data, the workflow involves computationally sampling the posterior distribution, often using a technique called Markov Chain Monte-Carlo (MCMC). The result is a series of values that are sampled statistically from the posterior distribution."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#advantages-of-the-bayesian-approach",
    "href": "posts/getting-started-for-actuaries/index.html#advantages-of-the-bayesian-approach",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Advantages of the Bayesian Approach",
    "text": "Advantages of the Bayesian Approach\nThe main advantages of this approach over traditional actuarial techniques are:\n\nFocus on distributions rather than point estimates of the posterior’s mean or mode. We are often interested in the distribution of the parameters and a focus on a single parameter estimate will understate the risk distribution.\nModel flexibility. A Bayesian model can be as simple as an ordinary linear regression, but as complex as modeling a full insurance mechanics.\nSimpler mental model. Fundamentally, Bayes’ theorem could be distilled down to an approach where you count the ways that things could occur and update the probabilities accordingly.\nExplicit Assumptions.: Enumerating the random variables in your model and explicitly parameterizing prior assumptions avoids ambiguity of the assumptions inside the statistical model."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#challenges-with-the-bayesian-approach",
    "href": "posts/getting-started-for-actuaries/index.html#challenges-with-the-bayesian-approach",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Challenges with the Bayesian Approach",
    "text": "Challenges with the Bayesian Approach\nWith the Bayesian approach, there are a handful of things that are challenging. Many of the listed items are not unique to the Bayesian approach, but there are different facets of the issues that arise.\n\nModel Construction. One must be thoughtful about the model and how variables interact. However, with the flexibility of modeling, you can apply (actuarial) science to makes better models!\nModel Diagnostics. Instead of R^2 values, there are unique diagnostics that one must monitor to ensure that the posterior sampling worked as intended.\nModel Complexity and Size of Data. The sampling algorithms are computationally intensive - as the amount of data grows and model complexity grows, the runtime demands cluster computing.\nModel Representation. The statistical derivation of the posterior can only reflect the complexity of the world as defined by your model. A Bayesian model won’t automatically infer all possible real-world relationships and constraints."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#why-now",
    "href": "posts/getting-started-for-actuaries/index.html#why-now",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Why Now?",
    "text": "Why Now?\nThere are both philosophical and practical reasons why Bayesian analysis is rapidly changing the statistical landscape.\nPhilosophically, one of the main reasons why Bayesian thinking is appealing is its ability to provide a straightforward interpretation of statistical conclusions.\nFor example, when estimating an unknown quantity, a Bayesian probability interval can be directly understood as having a high probability of containing that quantity. In contrast, a frequentist confidence interval is typically interpreted only in the context of a series of similar inferences that could be made in repeated practice. In recent years, there has been a growing emphasis on interval estimation rather than hypothesis testing in applied statistics. This shift has strengthened the Bayesian perspective since it is likely that many users of standard confidence intervals intuitively interpret them in a manner consistent with Bayesian thinking.\nAnother meaningful way to understand the contrast between Bayesian and frequentist approaches is through the lens of decision theory, specifically how each view treats the concept of randomness. This perspective pertains to whether you regard the data being random or the parameters being random.\nFrequentist statistics treats parameters as fixed and unknown, and the data as random — this is reflective of the view that data you collect is but one realization of an infinitely repeatable random process. Consequently, frequentist procedures, like hypothesis testing or confidence intervals, are generally based on the idea of long-run frequency or repeatable sampling.\nConversely, Bayesian statistics turns this on its head by treating the data as fixed — after all, once you’ve collected your data, it’s no longer random but a fixed observed quantity. Parameters, which are unknown, are treated as random variables. The Bayesian approach then allows us to use probability to quantify our uncertainty about these parameters.\nThe Bayesian approach tends to align more closely with our intuitive way of reasoning about problems. Often, you are given specific data and you want to understand what that particular set of data tells you about the world. You’re likely less interested in what might happen if you had infinite data, but rather in drawing the best conclusions you can from the data you do have.\nPractically, recent advances in computational power, algorithm development, and open-source libraries have enabled practitioners to adapt the Bayesian workflow.\nDeriving the posterior distribution is analytically intractable so computational methods must be used. Advances in raw computing power only in the 1990’s made non-trivial Bayesian analysis possible, and recent advances in algorithms have made the computations more efficient. For example, one of the most popular algorithms, NUTS, was only published in the 2010’s.\nMany problems require the use of compute clusters to manage runtime, but if there is any place to invest in understanding posterior probability distributions, it’s insurance companies trying to manage risk!\nMoreover, the availability of open-source libraries, such as Turing.jl, PyMC3, and Stan provide access to the core routines in an accessible interface."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#subjectivity-of-the-priors",
    "href": "posts/getting-started-for-actuaries/index.html#subjectivity-of-the-priors",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Subjectivity of the Priors?",
    "text": "Subjectivity of the Priors?\nThere are two ways one might react to subjectivity in a Bayesian context: It’s a feature that should be embraced or it’s a flaw that should be avoided.\n\nSubjectivity as a Feature\nA Bayesian approach to defining a statistical model is an approach that allows for explicitly incorporating actuarial judgment. Encoding assumptions into a Bayesian model forces the actuary to be explicit about otherwise fuzzy predilections. The explicit assumption is also more amenable to productive debate about its merits and biases than an implicit judgmental override.\n\n\nSubjectivity as a Flaw\nSubjectivity is inherent in all useful statistical methods. Subjectivity in traditional approaches include how the data was collected, which hypothesis to test, what significant levels to use, and assumptions about the data-generating processes.\nIn fact, the “objective” approach to null hypothesis testing is so prone to abuse and misinterpretation that in 2016, the American Statistical Association issued a statement intended to steer statistical analysis into a “post p&lt;0.05 era.” That “p&lt;0.05” approach is embedded in most traditional approaches to actuarial credibility1 and therefore should be similarly reconsidered.\n\n\nMaximum Entropy Distributions\nFurther, when assigning a prior assumption to a random variable, there are mathematically most conservative choices to pull from. These are called Maximum Entropy Distributions (MED) and it can be shown that for certain minimal constraints these are the information-theoretic least informative choices. Least informative means that the prior will have the least influence on the resulting posterior distribution.\nFor example, if all you know is that the mean of a random process is positive, then the Exponential Distribution is your MED. If you know that a mean and variance must exist for the process, then the Normal distribution is your MED. If you know nothing at all, you can use a Uniform distribution for the possible values."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#bayesian-versus-machine-learning",
    "href": "posts/getting-started-for-actuaries/index.html#bayesian-versus-machine-learning",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Bayesian Versus Machine Learning",
    "text": "Bayesian Versus Machine Learning\nMachine learning (ML) is fully compatible with Bayesian analysis - one can derive posterior distributions for the ML parameters like any other statistical model and the combination of approaches may be fruitful in practice.\nHowever, to the extent that actuaries have leaned on ML approaches due to the shortcomings of traditional actuarial approaches, Bayesian modeling may provide an attractive alternative without resorting to notoriously finicky and difficult-to-explain ML models. The Bayesian framework provides an explainable model and offers several analytic extensions beyond the scope of this introductory article:\n\nCausal Modeling: Identifying not just correlated relationships, but causal ones, in contexts where a traditional experiment is unavailable.\nBayes Action: Optimizing a parameter for, e.g., a CTE95 level instead of a parameter mean.\nInformation Criterion: Principled techniques to compare model fit and complexity.\nMissing data: Mechanisms to handle the different kinds of missing data.\nModel averaging: Posteriors can be combined from different models to synthesize different approaches."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#implications-for-risk-management",
    "href": "posts/getting-started-for-actuaries/index.html#implications-for-risk-management",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Implications for Risk Management",
    "text": "Implications for Risk Management\nLike Bayes’ Formula itself, another aspect of actuarial literature that is taught but often glossed over in practice is the difference between process risk (volatility), parameter risk, and model formulation risk. Often when performing analysis that relies on stochastic result, in practice only process/volatility risk is assessed.\nBayesian statistics provides the tools to help actuaries address parameter risk and model formulation. The posterior distribution of parameters derived is consistent with the observed data and modeled relationships. This posterior distribution of parameters can then be run as an additional dimension to the risk analysis.\nAdditionally, best practices include skepticism of the model construction itself, and testing different formulation of the modeled relationships and variable combinations to identify models which are best fit for purpose. Tools such as Information Criterion, posterior predictive checks, Bayes factors, and other statistical diagnostics can inform the actuary about tradeoffs between different choices of model."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#paving-the-way-forward-for-actuaries",
    "href": "posts/getting-started-for-actuaries/index.html#paving-the-way-forward-for-actuaries",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Paving the Way Forward for Actuaries",
    "text": "Paving the Way Forward for Actuaries\nBayesian approaches to statistical problems are rapidly changing the professional statistical field. To the extent that the actuarial profession incorporates statistical procedures we should consider adopting the same practices. The benefits of this are a better understanding of the distribution of risks, results that are more interpretable and explainable, and techniques that can be applied to a wider range of problems. The combination of these things would serve to enhance actuarial best practices related to understanding and communicating about risk.\nFor actuaries interested in learning more, there are number of available resources to be found. Textbooks recommended by the author are:\n\nStatistical Rethinking (McElreath)\nBayes Rules! (Johnson, Ott, Dogucu)\nBayesian Data Analysis (Gelman, et. al.)\n\nAdditionally, the author has published a few examples of Bayesian analysis in an actuarial context on JuliaActuary.org."
  },
  {
    "objectID": "posts/getting-started-for-actuaries/index.html#footnotes",
    "href": "posts/getting-started-for-actuaries/index.html#footnotes",
    "title": "Modern Bayesian Statistics for Actuaries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the approach discussed here is much more encompassing than the Bühlmann-Straub Bayesian approach described in the actuarial literature.↩︎"
  },
  {
    "objectID": "posts/coding-the-future/index.html",
    "href": "posts/coding-the-future/index.html",
    "title": "Coding the Future",
    "section": "",
    "text": "“…the insurance business is perhaps the purest example of an ‘information-based’ industry - that is, an industry whose sole activity consists of gathering, processing, and distributing information.” - Martin Campbell-Kelly, writing about the Prudential in the Victorian Era.1"
  },
  {
    "objectID": "posts/coding-the-future/index.html#the-insurance-industry-yesterday-today-and-tomorrow",
    "href": "posts/coding-the-future/index.html#the-insurance-industry-yesterday-today-and-tomorrow",
    "title": "Coding the Future",
    "section": "The insurance industry: yesterday, today, and tomorrow",
    "text": "The insurance industry: yesterday, today, and tomorrow\nIt might be odd to say that technology and its use in insurance is on a one-hundred-year cycle, but that seems to be the case.\n130 years ago, actuaries crowded into a room at a meeting of the Actuarial Society of America to watch a demonstration that would revolutionize the industry: Herman Hollerith’s tabulating punch card machine2.\nFor the next half-century, the increasing automation — from tabulating machines to early-adopting mainframes and computers — was a critical competitive differentiator. Companies like Prudential, MetLife, and others partnered with technology companies in the development of hardware and software3.\nThe dramatic embodiment of this information-driven cycle was portrayed in the infamous Billion Dollar Bubble movie, which showcased the power and abstraction of the computer to commit millions of dollars of fraud by creating and maintaining fake insurance policies.\nThe movie also starts to hint at the oscillation away from the technological-competitive focus of insurance companies. I argue that the focus on technology was lost over the last 50 years with the rise of Wall Street finance, investment-oriented life insurance, industry consolidation, and the explosion of financial structuring like derivatives, reserve financing, or other advanced forms of reinsurance.\nValue-add came from the C-Suite, not from the underlying business processes, operations, and analysis. The result is, e.g., ever-more complicated reinsurance treaties layered into mainframes and admin systems older than most of the actuaries interfacing with them.\nThe pace of strategic value-add isn’t slowing, though it must stretch further (in complexity and risk) to find comparable opportunities as the past. Having more agile, data-oriented operations enables companies to be able to react to and implement those opportunities. Technological value-add can improve a company’s bottom line through lower expenses and higher top-line growth, but often with a more favorable risk profile than some of the “strategic” opportunities.\nToday, there is a trend reverting back to technological value-creation and is evident across many traditional sectors. Tesla claims that it’s a technology company; Amazon is the #1 product retailer because of its vehement focus on internal information sharing4; Airlines are so dependent on their systems that the skies become quieter on the rare occasion that their computers give way.\nWhy is it, that companies that are so involved in things (cars, shopping) and physical services (flights) are so much more focused on improving their technological operations than insurance companies whose very focus is ‘information-based’? The market has rewarded those who have prioritized their internal technological solutions.\nCommoditized investing services and low yield environments have reduced insurance companies’ comparative advantage to “manage money”. Yield compression and the explosion of consumer-oriented investment services means a more competitive focus on the ability to manage the entire policy lifecycle efficiently (digitally), perform more real-time analysis of experience and risk management, and handle the growing product and regulatory complexity.\nThese are problems that have technological solutions and are waiting for insurance company adoption.\nCompanies that treat data like coordinates on a grid (spreadsheets) will get left behind. Two main hurdles have prevented technology companies from breaking into insurance:\n\nHigh regulatory barriers to entry, and\nDifficulty in selling complex insurance products without traditional distribution.\n\nOnce those two walls are breached, traditional insurance companies without a strong technology core will struggle to keep up. The key to thriving is not just adding “developers” to an organization; it’s going to be getting domain experts like actuaries to be an integral part of the technology transformation."
  },
  {
    "objectID": "posts/coding-the-future/index.html#whats-coding-got-to-do-with-this",
    "href": "posts/coding-the-future/index.html#whats-coding-got-to-do-with-this",
    "title": "Coding the Future",
    "section": "What’s coding got to do with this?",
    "text": "What’s coding got to do with this?\nEverything. Programming is the optimal way to interact between the computer and actuary — and importantly between computer and computer. Programming is the actionable expression of ideas, math, analysis, and information. Think of programming as the 21st-century leap in the actuary’s toolkit, just as spreadsheets were in the preceding 40 years. Versus a spreadsheet-oriented workflow:\n\nMore natural automation of, and between processes\nBetter reproducibility\nScaling to fit any size dataset and workload\nStatistics and machine learning capabilities\nAdvanced visualizations to garner new views into your data\n\nThis list isn’t comprehensive and some benefits are subtle — when you are code-oriented instead of spreadsheet-oriented, you tend to want to structure your data in a portable and shareable way. For example, relying more on data warehouses instead of email attachments. This, in turn, enables data discovery and insights that otherwise wouldn’t be there. Investing in a code-oriented workflow is playing the long-game.\nThe actuary of the future needs to have coding as one of their core skills. Already today, the advances of business processes, insurance products, and financial ingenuity are written with lines of code — not spreadsheets. Not being able to code necessarily means that you are following what others are doing today.\nIt’s commonly accepted now that to gather insights from your data, you need to know how to code. Similar to your data, your business architecture, modeling needs, and product peculiarities are often better suited to customized solutions. Why stop at data science when learning how to solve problems with a computer?"
  },
  {
    "objectID": "posts/coding-the-future/index.html#the-10x-actuary",
    "href": "posts/coding-the-future/index.html#the-10x-actuary",
    "title": "Coding the Future",
    "section": "The 10x Actuary",
    "text": "The 10x Actuary\nAs we swing back to a technological focus, we do not leave the finance-driven complexity behind. The increasingly complex business needs will highlight a large productivity difference between an actuary who can code and one who can’t — simply because the former can react, create, synthesize, and model faster than the latter. From the efficiency of transforming administration extracts, summarizing and aggregating valuation output, to analyzing claims data in ways that spreadsheets simply can’t handle, you can become a “10x Actuary”5.\nFlipping switches in a graphical user interface versus being able to build models is the difference between having a surface-level familiarity and having full command over the analysis and the concepts involved — with the flexibility to do what your software can’t.\nYour current software might be able to perform the first layer of analysis but be at a loss when you want to visualize, perform sensitivity analysis, statistics, stochastic analysis, or process automation. Things that, when done programmatically, are often just a few lines of additional code.\nDo I advocate dropping the license for your software vendor? No, not yet anyway. But the ability to supplement and break out of the modeling box has been an increasingly important part of most actuaries’ work.\nAdditionally, code-based solutions can leverage the entire-technology sector’s progress to solve problems that are hard otherwise: scalability, data workflows, integration across functional areas, version control and versioning, model change governance, reproducibility, and more.\n30-40 years ago, there were no vendor-supplied modeling solutions and so you had no choice but to build models internally. This shifted with the advent of vendor-supplied modeling solutions. Today, it’s never been better for companies to leverage open source to support their custom modeling, risk analysis/monitoring, and reporting workflows."
  },
  {
    "objectID": "posts/coding-the-future/index.html#risk-governance",
    "href": "posts/coding-the-future/index.html#risk-governance",
    "title": "Coding the Future",
    "section": "Risk Governance",
    "text": "Risk Governance\nCode-based workflows are highly conducive to risk governance frameworks as well. If a modern software project has all of the following benefits, then why not a modern insurance product and associated processes?\n\nAccess control and approval processes\nVersion control, version management, and reproducibility\nContinuous testing and validation of results\nOpen and transparent design\nMinimization of manual overrides, intervention, and opportunity for user error\nAutomated trending analysis, system metrics, and summary statistics\nContinuously updated, integrated, and self-generating documentation\nIntegration with other business processes through a formal boundary (e.g. via an API)\nTools to manage collaboration in parallel and in sequence"
  },
  {
    "objectID": "posts/coding-the-future/index.html#managing-and-leading-the-transformation",
    "href": "posts/coding-the-future/index.html#managing-and-leading-the-transformation",
    "title": "Coding the Future",
    "section": "Managing and Leading the Transformation",
    "text": "Managing and Leading the Transformation\nThe ability to understand the concepts, capabilities, challenges, and lingo is not a dichotomy, it’s a spectrum. Most actuaries, even at fairly high levels, are still often involved in analytical work. Still above that, it’s difficult to lead something that you don’t understand.\nConversely, the skill and practice of coding enhances managerial capabilities. When you are really skilled at pulling apart a problem or process into its constituent parts and designing optimal solutions; that’s a core attribute of leadership: having the vision of where the organization should be instead of thinking about where it is now.\nNor is the skillset described here limiting in any other aspect of career development any more than mathematical ability, project collaboration, or financial acumen — just to name a few."
  },
  {
    "objectID": "posts/coding-the-future/index.html#outlook",
    "href": "posts/coding-the-future/index.html#outlook",
    "title": "Coding the Future",
    "section": "Outlook",
    "text": "Outlook\nIt will increasingly be essential for companies to modernize to remain competitive. That modernization isn’t built with big black-box software packages; it will be with domain experts who can translate the expertise into new forms of analysis - doing it faster and more robustly than the competition.\nSpaceX doesn’t just hire rocket scientists - they hire rocket scientists who code.\nBe an actuary who codes."
  },
  {
    "objectID": "posts/coding-the-future/index.html#future-articles",
    "href": "posts/coding-the-future/index.html#future-articles",
    "title": "Coding the Future",
    "section": "Future Articles",
    "text": "Future Articles\nThe forthcoming series of articles will help illustrate what this can look like in practice: examining the business case, acquainting with ways of interacting with problems outside of spreadsheets and dataframes, and approaching work in a way that removes the boring parts and focuses on the concepts and insights.\nThe next article in this series will discuss what tools enable an actuary to deliver on the vision outlined in this article. In particular, it will highlight the Julia programming language, a tool well positioned to enable actuaries to develop the analysis, systems, and models of tomorrow."
  },
  {
    "objectID": "posts/coding-the-future/index.html#footnotes",
    "href": "posts/coding-the-future/index.html#footnotes",
    "title": "Coding the Future",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCo-evolution of Information Processing Technology and Use: Interaction Between the Life Insurance and Tabulating Industries↩︎\nCo-evolution of Information Processing Technology and Use: Interaction Between the Life Insurance and Tabulating Industries↩︎\nFrom Tabulators to Early Computers in the U.S. Life Insurance Industry↩︎\nHave you had your Bezos moment? What you can learn from Amazon↩︎\nThe 10x [Rockstar] developer is NOT a myth↩︎"
  },
  {
    "objectID": "posts/academy-generator/index.html",
    "href": "posts/academy-generator/index.html",
    "title": "Replicating the AAA equtity generator",
    "section": "",
    "text": "using CairoMakie\nusing ColorSchemes\nusing Distributions\nusing LabelledArrays\nusing Random\nThis notebook replicates the model and parameters for the real world equity generator described in this AAA 2005 reference paper."
  },
  {
    "objectID": "posts/academy-generator/index.html#stochastic-log-volatility-model",
    "href": "posts/academy-generator/index.html#stochastic-log-volatility-model",
    "title": "Replicating the AAA equtity generator",
    "section": "Stochastic Log Volatility Model",
    "text": "Stochastic Log Volatility Model\nNote that the @. and other broadcasting (. symbol) allows us to operate on multiple funds at once.\n\nfunction v(v_prior,params,Zₜ) \n    (;σ_v, σ_m,σ_p,σ⃰,ϕ,τ) = params\n    \n    v_m = log.(σ_m)\n    v_p = log.(σ_p)\n    v⃰ = log.(σ⃰)\n\n    # vol are the odd values in the random array\n    ṽ =  @. min(v_p, (1 - ϕ) * v_prior + ϕ * log(τ) ) + σ_v * Zₜ[[1,3,5,7]]\n    \n    v = @. max(v_m, min(v⃰,ṽ))\n\n    return v\nend\n\nfunction scenario(params,Z;months=1200)\n    (;σ_v,σ_0, ρ,A,B,C) = params\n\n    n_funds = size(params,2)\n    \n    #initilize/pre-allocate\n    Zₜ = rand(Z)\n    v_t = log.(σ_0)\n    σ_t = zeros(n_funds)\n    μ_t = zeros(n_funds)\n    \n    log_returns = map(1:months) do t\n        Zₜ = rand!(Z,Zₜ)\n        v_t .= v(v_t,params,Zₜ)\n\n        σ_t .= exp.(v_t)\n\n        @. μ_t =  A + B * σ_t + C * (σ_t)^2\n\n        # return are the even values in the random array\n        log_return = @. μ_t / 12 + σ_t / sqrt(12) * Zₜ[[2,4,6,8]]\n    end\n\n    # convert vector of vector to matrix\n    reduce(hcat,log_returns)\nend\n\nscenario (generic function with 1 method)"
  },
  {
    "objectID": "posts/academy-generator/index.html#model-parameters",
    "href": "posts/academy-generator/index.html#model-parameters",
    "title": "Replicating the AAA equtity generator",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n# use a labelled array for easy reference of the parameters \nparams = @LArray [\n    0.12515 0.14506 0.16341 0.20201     # τ\n    0.35229 0.41676 0.3632 0.35277      # ϕ\n    0.32645 0.32634 0.35789 0.34302     # σ_v\n    -0.2488 -0.1572 -0.2756 -0.2843     # ρ\n    0.055 0.055 0.055 0.055             # A\n    0.56 0.466 0.67 0.715               # B\n    -0.9 -0.9 -0.95 -1.0                # C\n    0.1476 0.1688 0.2049 0.2496         # σ_0\n    0.0305 0.0354 0.0403 0.0492         # σ_m\n    0.3 0.3 0.4 0.55                    # σ_p\n    0.7988 0.4519 0.9463 1.1387         # σ⃰\n] ( \n    # define the regions each label refers to\n    τ = (1,:),\n    ϕ = (2,:),\n    σ_v = (3,:),\n    ρ = (4,:),\n    A = (5,:),\n    B = (6,:),\n    C = (7,:),\n    σ_0 = (8,:),\n    σ_m = (9,:),\n    σ_p = (10,:),\n    σ⃰ = (11,:)\n)\n\n11×4 LArray{Float64, 2, Matrix{Float64}, (τ = (1, Colon()), ϕ = (2, Colon()), σ_v = (3, Colon()), ρ = (4, Colon()), A = (5, Colon()), B = (6, Colon()), C = (7, Colon()), σ_0 = (8, Colon()), σ_m = (9, Colon()), σ_p = (10, Colon()), σ⃰ = (11, Colon()))}:\n   :τ =&gt; 0.12515  …    :τ =&gt; 0.20201\n   :ϕ =&gt; 0.35229       :ϕ =&gt; 0.35277\n :σ_v =&gt; 0.32645     :σ_v =&gt; 0.34302\n   :ρ =&gt; -0.2488       :ρ =&gt; -0.2843\n   :A =&gt; 0.055         :A =&gt; 0.055\n   :B =&gt; 0.56     …    :B =&gt; 0.715\n   :C =&gt; -0.9          :C =&gt; -1.0\n :σ_0 =&gt; 0.1476      :σ_0 =&gt; 0.2496\n :σ_m =&gt; 0.0305      :σ_m =&gt; 0.0492\n :σ_p =&gt; 0.3         :σ_p =&gt; 0.55\n   :σ⃰ =&gt; 0.7988   …    :σ⃰ =&gt; 1.1387\n\n\n\nThe Multivariate normal and covariance matrix\n\n# 11 columns because it's got the bond returns in it\ncov_matrix = [\n    1.000   -0.249  0.318   -0.082  0.625   -0.169  0.309   -0.183  0.023   0.075   0.080;\n    -0.249  1.000   -0.046  0.630   -0.123  0.829   -0.136  0.665   -0.120  0.192   0.393;\n    0.318   -0.046  1.000   -0.157  0.259   -0.050  0.236   -0.074  -0.066  0.034   0.044;\n    -0.082  0.630   -0.157  1.000   -0.063  0.515   -0.098  0.558   -0.105  0.130   0.234;\n    0.625   -0.123  0.259   -0.063  1.000   -0.276  0.377   -0.180  0.034   0.028   0.054;\n    -0.169  0.829   -0.050  0.515   -0.276  1.000   -0.142  0.649   -0.106  0.067   0.267;\n    0.309   -0.136  0.236   -0.098  0.377   -0.142  1.000   -0.284  0.026   0.006   0.045;\n    -0.183  0.665   -0.074  0.558   -0.180  0.649   -0.284  1.000   0.034   -0.091  -0.002;\n    0.023   -0.120  -0.066  -0.105  0.034   -0.106  0.026   0.034   1.000   0.047   -0.028;\n    0.075   0.192   0.034   0.130   0.028   0.067   0.006   -0.091  0.047   1.000   0.697;\n    0.080   0.393   0.044   0.234   0.054   0.267   0.045   -0.002  -0.028  0.697   1.000;\n]\n\n    Z = MvNormal(\n        zeros(11), #means for return and volatility\n        cov_matrix # covariance matrix\n        # full covariance matrix in AAA Excel workook on Parameters tab\n    )\n\nFullNormal(\ndim: 11\nμ: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nΣ: [1.0 -0.249 … 0.075 0.08; -0.249 1.0 … 0.192 0.393; … ; 0.075 0.192 … 1.0 0.697; 0.08 0.393 … 0.697 1.0]\n)"
  },
  {
    "objectID": "posts/academy-generator/index.html#scenarios-and-validation",
    "href": "posts/academy-generator/index.html#scenarios-and-validation",
    "title": "Replicating the AAA equtity generator",
    "section": "Scenarios and validation",
    "text": "Scenarios and validation\n\nA single scenario\n\nx = scenario(params,Z;months=1200)\n\n4×1200 Matrix{Float64}:\n 0.0752418  -0.000684304  0.0803535  …   0.000348079  -0.00459584\n 0.0253968   0.0235449    0.0841353      0.0127518    -0.0323876\n 0.0893632   0.0144473    0.0872761     -0.0191581     0.00114621\n 0.138261    0.0343143    0.0945716     -0.0176436    -0.029718\n\n\n\n\nValidation of summary statistics\nThe summary statistics expected (per paper Table 8):\n\nμ ≈ [0.0060, 0.0062, 0.0063, 0.0065]\nσ ≈ [0.0436, 0.0492, 0.0590, 0.0724]\n\nThese computed values match very closely:\n\n# generate 1000 scenarios \nscens = [scenario(params,Z) for _ in 1:1000];\n\nlet\n    # compute summary statistics\n    μ = vec(mean(mean(x,dims=2) for x in scens))\n    σ = vec(mean(std(x,dims=2) for x in scens))\n    (;μ,σ)\nend\n\n(μ = [0.005996167539900355, 0.006165861874564148, 0.006254386906381312, 0.006366151507437672], σ = [0.0436367414351783, 0.04920555060632976, 0.05914308496454576, 0.0724382319579281])"
  },
  {
    "objectID": "posts/academy-generator/index.html#plotting-some-scenarios",
    "href": "posts/academy-generator/index.html#plotting-some-scenarios",
    "title": "Replicating the AAA equtity generator",
    "section": "Plotting some scenarios",
    "text": "Plotting some scenarios\n\nlet \n    f = Figure()\n    n = 25\n    colors = ColorSchemes.Johnson\n    ax = Axis(f[1,1],yscale=log10,ylabel=\"index value\",\n        title=\"$n realizations of 4 correlated equity funds per AAA ESG\")\n    for s in scens[1:n]\n        for i in 1:4\n        lines!(ax,cumprod(exp.(s[i,:])), color=(colors[i],0.3),label=\"fund $i\")\n        end\n    end\n    axislegend(ax,unique=true,position=:lt)\n    f\nend\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/We6MY/src/scenes.jl:227"
  },
  {
    "objectID": "posts/academy-generator/index.html#see-also",
    "href": "posts/academy-generator/index.html#see-also",
    "title": "Replicating the AAA equtity generator",
    "section": "See also",
    "text": "See also\nInteractive AAA Economic Scenario Generator"
  },
  {
    "objectID": "benchmarks.html",
    "href": "benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "Inspired by the discussion in the ActuarialOpenSource GitHub community discussion, folks started submitted solutions to what someone referred to as the “Life Modeling Problem”. This user submitted a short snippet for consideration of a representative problem.\n\n\nAfter the original user submitted a proposal, others chimed in and submitted versions in their favorite languages. I have collected those versions, and run them on a consistent set of hardware.\nSome submissions were excluded because from the benchmarks they involved an entirely different approach, such as memoizing the function calls[^1].\n\n\n[ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n\n\n\n\nTable 1: Benchmarks for the Life Modeling Problem in nanoseconds (lower times are better).\n\n\n\n18×6 DataFrame\n\n\n\nRow\nlang\nalgorithm\nfunction_name\nmedian\nmean\nrelative_mean\n\n\n\nString15\nString15\nString15\nFloat64?\nFloat64\nFloat64\n\n\n\n\n1\nJulia\nAccumulator\nnpv9\n6.388\n6.375\n1.0\n\n\n2\nRust\nAccumulator\nnpv3\n7.0\n7.0\n1.09804\n\n\n3\nJulia\nAccumulator\nnpv8\n7.372\n7.375\n1.15686\n\n\n4\nJulia\nAccumulator\nnpv7\n7.92\n7.917\n1.24188\n\n\n5\nJulia\nAccumulator\nnpv6\n9.037\n9.009\n1.41318\n\n\n6\nJulia\nAccumulator\nnpv4\n10.764\n10.761\n1.688\n\n\n7\nJulia\nAccumulator\nnpv5\n11.49\n11.469\n1.79906\n\n\n8\nRust\nAccumulator\nnpv2\n14.0\n14.0\n2.19608\n\n\n9\nJulia\nAccumulator\nnpv3\n14.507\n14.487\n2.27247\n\n\n10\nRust\nAccumulator\nnpv1\n22.0\n22.0\n3.45098\n\n\n11\nJulia\nVectorized\nnpv2\n235.758\n218.391\n34.2574\n\n\n12\nJulia\nVectorized\nnpv1\n235.322\n228.198\n35.7958\n\n\n13\nPython (Numba)\nAccumulator\nnpv_numba\nmissing\n626.0\n98.1961\n\n\n14\nPython\nAccumulator\nnpv_loop\nmissing\n2314.0\n362.98\n\n\n15\nPython (NumPy)\nVectorized\nnpv\nmissing\n14261.0\n2237.02\n\n\n16\nR\nVectorized\nnpv base\n4264.0\n46617.0\n7312.47\n\n\n17\nR\nAccumulator\nnpv_loop\n4346.0\n62275.7\n9768.74\n\n\n18\nR (data.table)\nVectorized\nnpv\n770554.0\n8.42767e5\n1.32199e5\n\n\n\n\n\n\n\n\n\nTo aid in visualizing results with such vast different orders of magnitude, this graph includes a physical length comparison to serve as a reference. The computation time is represented by the distance that light travels in the time for the computation to complete (comparing a nanosecond to one foot length goes at least back to Admiral Grace Hopper).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nFor more a more in-depth discussion of these results, see this post.\nAll of the benchmarked code can be found in the JuliaActuary Learn repository. Please file an issue or submit a PR request there for issues/suggestions."
  },
  {
    "objectID": "benchmarks.html#the-life-modeling-problem",
    "href": "benchmarks.html#the-life-modeling-problem",
    "title": "Benchmarks",
    "section": "",
    "text": "Inspired by the discussion in the ActuarialOpenSource GitHub community discussion, folks started submitted solutions to what someone referred to as the “Life Modeling Problem”. This user submitted a short snippet for consideration of a representative problem.\n\n\nAfter the original user submitted a proposal, others chimed in and submitted versions in their favorite languages. I have collected those versions, and run them on a consistent set of hardware.\nSome submissions were excluded because from the benchmarks they involved an entirely different approach, such as memoizing the function calls[^1].\n\n\n[ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n\n\n\n\nTable 1: Benchmarks for the Life Modeling Problem in nanoseconds (lower times are better).\n\n\n\n18×6 DataFrame\n\n\n\nRow\nlang\nalgorithm\nfunction_name\nmedian\nmean\nrelative_mean\n\n\n\nString15\nString15\nString15\nFloat64?\nFloat64\nFloat64\n\n\n\n\n1\nJulia\nAccumulator\nnpv9\n6.388\n6.375\n1.0\n\n\n2\nRust\nAccumulator\nnpv3\n7.0\n7.0\n1.09804\n\n\n3\nJulia\nAccumulator\nnpv8\n7.372\n7.375\n1.15686\n\n\n4\nJulia\nAccumulator\nnpv7\n7.92\n7.917\n1.24188\n\n\n5\nJulia\nAccumulator\nnpv6\n9.037\n9.009\n1.41318\n\n\n6\nJulia\nAccumulator\nnpv4\n10.764\n10.761\n1.688\n\n\n7\nJulia\nAccumulator\nnpv5\n11.49\n11.469\n1.79906\n\n\n8\nRust\nAccumulator\nnpv2\n14.0\n14.0\n2.19608\n\n\n9\nJulia\nAccumulator\nnpv3\n14.507\n14.487\n2.27247\n\n\n10\nRust\nAccumulator\nnpv1\n22.0\n22.0\n3.45098\n\n\n11\nJulia\nVectorized\nnpv2\n235.758\n218.391\n34.2574\n\n\n12\nJulia\nVectorized\nnpv1\n235.322\n228.198\n35.7958\n\n\n13\nPython (Numba)\nAccumulator\nnpv_numba\nmissing\n626.0\n98.1961\n\n\n14\nPython\nAccumulator\nnpv_loop\nmissing\n2314.0\n362.98\n\n\n15\nPython (NumPy)\nVectorized\nnpv\nmissing\n14261.0\n2237.02\n\n\n16\nR\nVectorized\nnpv base\n4264.0\n46617.0\n7312.47\n\n\n17\nR\nAccumulator\nnpv_loop\n4346.0\n62275.7\n9768.74\n\n\n18\nR (data.table)\nVectorized\nnpv\n770554.0\n8.42767e5\n1.32199e5\n\n\n\n\n\n\n\n\n\nTo aid in visualizing results with such vast different orders of magnitude, this graph includes a physical length comparison to serve as a reference. The computation time is represented by the distance that light travels in the time for the computation to complete (comparing a nanosecond to one foot length goes at least back to Admiral Grace Hopper).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nFor more a more in-depth discussion of these results, see this post.\nAll of the benchmarked code can be found in the JuliaActuary Learn repository. Please file an issue or submit a PR request there for issues/suggestions."
  },
  {
    "objectID": "benchmarks.html#irrs",
    "href": "benchmarks.html#irrs",
    "title": "Benchmarks",
    "section": "IRRs",
    "text": "IRRs\nTask: determine the IRR for a series of cashflows 701 elements long (e.g. monthly cashflows for 60 years).\n\nBenchmarks\n\nTimes are in nanoseconds:\n┌──────────┬──────────────────┬───────────────────┬─────────┬─────────────┬───────────────┐\n│ Language │          Package │          Function │  Median │        Mean │ Relative Mean │\n├──────────┼──────────────────┼───────────────────┼─────────┼─────────────┼───────────────┤\n│   Python │  numpy_financial │               irr │ missing │   519306422 │       123146x │\n│   Python │           better │ irr_binary_search │ missing │     3045229 │          722x │\n│   Python │           better │        irr_newton │ missing │      382166 │           91x │\n│    Julia │ ActuaryUtilities │               irr │    4185 │        4217 │            1x │\n└──────────┴──────────────────┴───────────────────┴─────────┴─────────────┴───────────────┘\n\n\n\nDiscussion\nThe ActuaryUtilities implementation is over 100,000 times faster than numpy_financial, and 91 to 722 times faster than the better Python package. The ActuaryUtilities.jl implementation is also more flexible, as it can be given an argument with timepoints, similar to Excel’s XIRR.\nExcel was used to attempt a benchmark, but the IRR formula returned a #DIV/0! error.\nAll of the benchmarked code can be found in the JuliaActuary Learn repository. Please file an issue or submit a PR request there for issues/suggestions."
  },
  {
    "objectID": "benchmarks.html#black-scholes-merton-european-option-pricing",
    "href": "benchmarks.html#black-scholes-merton-european-option-pricing",
    "title": "Benchmarks",
    "section": "Black-Scholes-Merton European Option Pricing",
    "text": "Black-Scholes-Merton European Option Pricing\nTask: calculate the price of a vanilla european call option using the Black-Scholes-Merton formula.\n\\[\\begin{align}\nC(S_t, t) &= N(d_1)S_t - N(d_2)Ke^{-r(T - t)} \\\\\n\nd_1 &= \\frac{1}{\\sigma\\sqrt{T - t}}\\left[\\ln\\left(\\frac{S_t}{K}\\right) + \\left(r + \\frac{\\sigma^2}{2}\\right)(T - t)\\right] \\\\\n\nd_2 &= d_1 - \\sigma\\sqrt{T - t}\n\n\\end{align}\\]\n\nBenchmarks\nTimes are in nanoseconds:\n┌──────────┬─────────┬─────────────┬───────────────┐\n│ Language │  Median │        Mean │ Relative Mean │\n├──────────┼─────────┼─────────────┼───────────────┤\n│   Python │ missing │    817000.0 │       19926.0 │\n│        R │  3649.0 │      3855.2 │          92.7 │\n│    Julia │    41.0 │        41.6 │           1.0 │\n└──────────┴─────────┴─────────────┴───────────────┘\n\n\nDiscussion\nJulia is nearly 20,000 times faster than Python, and two orders of magnitude faster than R."
  },
  {
    "objectID": "benchmarks.html#other-benchmarks",
    "href": "benchmarks.html#other-benchmarks",
    "title": "Benchmarks",
    "section": "Other benchmarks",
    "text": "Other benchmarks\nThese benchmarks have been performed by others, but provide relevant information for actuarial-related work:\n\nH2Oai DataFrames/Database-like Operations\nReading CSVs"
  },
  {
    "objectID": "benchmarks.html#colophone",
    "href": "benchmarks.html#colophone",
    "title": "Benchmarks",
    "section": "Colophone",
    "text": "Colophone\n\nCode\nAll of the benchmarked code can be found in the JuliaActuary Learn repository. Please file an issue or submit a PR request there for issues/suggestions."
  },
  {
    "objectID": "benchmarks.html#footnotes",
    "href": "benchmarks.html#footnotes",
    "title": "Benchmarks",
    "section": "Footnotes",
    "text": "Footnotes\n[^1] If benchmarking memoization, it’s essentially benchmarking how long it takes to perform hashing in a language. While interesting, especially in the context of incremental computing, it’s not the core issue at hand. Incremental computing libraries exist for all of the modern languages discussed here.\n[^2] Note that not all languages have both a mean and median result in their benchmarking libraries. Mean is a better representation for a garbage-collected modern language, because sometimes the computation just takes longer than the median result. Where the mean is not available in the graph below, median is substituted."
  },
  {
    "objectID": "all-posts.html",
    "href": "all-posts.html",
    "title": "All Posts",
    "section": "",
    "text": "Interactive Cashflow Analysis with ActuaryUtilities.jl\n\n\n\n\n\n\nactuaryutilities\n\n\nfinancemodels\n\n\nassets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversal Life Policy Account Mechanics as a Differential Equation\n\n\n\n\n\n\nmodeling\n\n\ndiffeq\n\n\nmortalitytables\n\n\nactuaryutilities\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUS Treasury Comparison Tool\n\n\n\n\n\n\nmodeling\n\n\nfinancemodels\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Markov-Chain-Monte-Carlo and Claims Data\n\n\n\n\n\n\nmortalitytables\n\n\nexposures\n\n\nexperience-analysis\n\n\ndataframes\n\n\ntutorial\n\n\nstatistics\n\n\nbayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested Projection Mechanics\n\n\n\n\n\n\nmodeling\n\n\nbenchmark\n\n\nactuaryutilities\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing MortaltiyTables.jl with DataFrames\n\n\n\n\n\n\nmortalitytables\n\n\ndataframes\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson approximation to Binomial\n\n\n\n\n\n\nmodeling\n\n\nstatistics\n\n\nexperience-analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian vs Limited Fluctuation Experience Analysis\n\n\n\n\n\n\nmodeling\n\n\nstatistics\n\n\nexperience-analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting Rate Data to Yield Curves\n\n\n\n\n\n\nmarket\n\n\nyieldcurves\n\n\nfinancemodels\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting survival data with MortaltityTables.jl\n\n\n\n\n\n\nsurvival\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive AAA Economic Scenario Generator\n\n\n\n\n\n\nmodeling\n\n\nscenario-generator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic claims projections demo\n\n\n\n\n\n\nmodeling\n\n\nbenchmark\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExposure Calculation with ExperienceAnalysis.jl\n\n\n\n\n\n\nmortalitytables\n\n\nexposures\n\n\nexperience-analysis\n\n\ndataframes\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplicating the AAA equtity generator\n\n\n\n\n\n\nmodeling\n\n\nscenario-generator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Mortality Comparison Tool\n\n\n\n\n\n\nexperience-analysis\n\n\nmortalitytables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinanceModels.jl - Evolving the JuliaActuary Ecosystem\n\n\n\n\n\n\ncode\n\n\nblog\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nModern Bayesian Statistics for Actuaries\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\nblog\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nJuliaActuary and Hacktoberfest 2022\n\n\n\n\n\n\ncode\n\n\ncontributing\n\n\nblog\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Julia for Actuaries\n\n\n\n\n\n\nindustry\n\n\nblog\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nOct 24, 2021\n\n\nAlec Loudenback and Dimitar Vanguelov\n\n\n\n\n\n\n\n\n\n\n\n\nThe Life Modeling Problem: A Comparison of Julia, Rust, Python, and R\n\n\n\n\n\n\ncode\n\n\nblog\n\n\n\n\n\n\n\n\n\nMay 16, 2021\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nJulia for Actuaries\n\n\n\n\n\n\ncode\n\n\nindustry\n\n\nr\n\n\npython\n\n\nblog\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\nAlec Loudenback\n\n\n\n\n\n\n\n\n\n\n\n\nCoding the Future\n\n\n\n\n\n\ncode\n\n\nindustry\n\n\nblog\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\nAlec Loudenback\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learn.html",
    "href": "learn.html",
    "title": "Learn",
    "section": "",
    "text": "Resources to help get started."
  },
  {
    "objectID": "learn.html#installation",
    "href": "learn.html#installation",
    "title": "Learn",
    "section": "Installation",
    "text": "Installation\nAfter installing Julia from the official website, head to the Packages to see how to install JuliaActuary packages."
  },
  {
    "objectID": "learn.html#documentation",
    "href": "learn.html#documentation",
    "title": "Learn",
    "section": "Documentation",
    "text": "Documentation\nEach package includes examples on the Github site and in the documentation."
  },
  {
    "objectID": "learn.html#introductory-programming-and-julia-resources",
    "href": "learn.html#introductory-programming-and-julia-resources",
    "title": "Learn",
    "section": "Introductory Programming and Julia Resources",
    "text": "Introductory Programming and Julia Resources\n\nJuliaLang.org, the home site with the downloads to get started, and links to learning resources.\nJuliaHub indexes open-source Julia packages and makes the entire ecosystem and documentation searchable from one place.\nJuliaAcademy, which has free short courses in Data Science, Introduction to Julia, DataFrames.jl, Machine Learning, and more.\nData Science Tutorials from the Alan Turing Institute.\nLearn Julia in Y minutes, a great quick-start if you are already comfortable with coding.\nThink Julia, a free e-book (or paid print edition) book which introduces programming from the start and teaches you valuable ways of thinking.\nDesign Patterns and Best Practices, a book that will help you as you transition from smaller, one-off scripts to designing larger packages and projects."
  },
  {
    "objectID": "learn.html#example-actuarial-and-finance-uses",
    "href": "learn.html#example-actuarial-and-finance-uses",
    "title": "Learn",
    "section": "Example Actuarial and Finance Uses",
    "text": "Example Actuarial and Finance Uses\nSee the examples page."
  },
  {
    "objectID": "learn.html#get-help",
    "href": "learn.html#get-help",
    "title": "Learn",
    "section": "Get Help",
    "text": "Get Help\n\nAsk questions or suggest ideas\n\n\nDiscussion and Questions\nIf you have other ideas or questions, join the JuliaActuary Github Discussions. Or come say hello on the community Zulip or Slack #actuary channel. We welcome all actuarial and related disciplines!\n\n\nHelp mode\nYou can also access help text when using the packages in the REPL by activating help mode, e.g.:\njulia&gt; ? survival\n    survival(mortality_vector,to_age)\n    survival(mortality_vector,from_age,to_age)\n\n\n  Returns the survival through attained age to_age. The start of the \n  calculation is either the start of the vector, or attained age `from_age` \n  and `to_age` need to be Integers. \n\n  Add a DeathDistribution as the last argument to handle floating point \n  and non-whole ages:\n\n    survival(mortality_vector,to_age,::DeathDistribution)\n    survival(mortality_vector,from_age,to_age,::DeathDistribution)\n\n\n  If given a negative to_age, it will return 1.0. Aside from simplifying the code, \n  this makes sense as for something to exist in order to decrement in the first place, \n  it must have existed and survived to the point of being able to be decremented.\n\n  Examples\n  ≡≡≡≡≡≡≡≡≡≡\n\n  julia&gt; qs = UltimateMortality([0.1,0.3,0.6,1]);\n\n  julia&gt; survival(qs,0)\n  1.0\n  julia&gt; survival(qs,1)\n  0.9\n\n  julia&gt; survival(qs,1,1)\n  1.0\n  julia&gt; survival(qs,1,2)\n  0.7\n\n  julia&gt; survival(qs,0.5,Uniform())\n  0.95"
  },
  {
    "objectID": "learn.html#integration-with-r-and-python",
    "href": "learn.html#integration-with-r-and-python",
    "title": "Learn",
    "section": "Integration with R and Python",
    "text": "Integration with R and Python\n\nUse other languages seamlessly\n\nJulia integrates with other languages, allowing you to leverage existing scripts and packages in R via RCall and in Python via PyCall."
  },
  {
    "objectID": "learn.html#contributing",
    "href": "learn.html#contributing",
    "title": "Learn",
    "section": "Contributing",
    "text": "Contributing\n\nContribute code or report issues.\n\n&lt;mark&gt;Thank you&lt;/mark&gt; for your interest in modern actuarial solutions, no matter how you participate in the community.&lt;/p&gt;\n\nPull Requests\nJuliaActuary is open source; you are free to modify, use, or change your copy of the code - but if you make enhancements please consider opening a pull request (basic walkthrough here). Beginners are welcome and we can help with your first pull request!\n\n\nIssues\nIf you find issues, please open an issue on the relevant package’s repository and we will try and address it as soon as possible.\n\n\nProject Board\nSee the Good first Issues project board on Github for simple, self-contained ways to contribute such as adding small new features, improving the documentation, or writing up a tutorial on how to do something simple!"
  },
  {
    "objectID": "learn.html#other-inquiries",
    "href": "learn.html#other-inquiries",
    "title": "Learn",
    "section": "Other Inquiries",
    "text": "Other Inquiries\nFor more directed inquires, please send email to inquiry@JuliaActuary.org."
  },
  {
    "objectID": "learn.html#share",
    "href": "learn.html#share",
    "title": "Learn",
    "section": "Share",
    "text": "Share\nFollow JuliaActuary on LinkedIn for updates and to share with colleagues!"
  }
]