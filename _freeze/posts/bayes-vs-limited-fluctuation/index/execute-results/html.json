{
  "hash": "8828aa41d6fbb9639d74d2c5c10d751b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Bayesian vs Limited Fluctuation Experience Analysis\"\ncategories: [modeling,statistics,experience-analysis]\n---\n\n\n\n\nAlternate title: Why actuaries should cease to care about the number 1082.\n\n## Introduction\n\nThis notebook briefly discusses two approaches to experience analysis (rate setting). One traditional method, Limited Fluctuation Credibility (LFC), has been around for a very long time although was never intended to be the most accurate predictive method. However, many actuaries still discuss the notion of \"full\" or \"partially\" credible indicating a reliance on the LFC concepts.\n\nThe Bayesian approach uses explicit assumptions about the statistical relationships and all data given to the model to make inferences. Small datasets lead to greater uncertainty, while larger datasets never reach a point that could be considered \"fully credible\", although the posterior density may narrow considerably.\n\nThis notebook argues that the Bayesian approach is superior to the LFC heuristics and should be adopted more widely in actuarial practice.\n\nThe example will use an auto-claims dataset and will use the first two years of experience to predict the third.\n\n::: {#a2d9afa2 .cell execution_count=1}\n``` {.julia .cell-code}\nusing CSV\nusing DataFramesMeta\nusing Distributions\nusing Turing\nusing MCMCChains\nusing DataFrames\nusing Logging; Logging.disable_logging(Logging.Warn);\nusing StatsFuns\nusing StatisticalRethinking\nusing CairoMakie\n```\n:::\n\n\n## Limited Fluctuation Credibility\n\n> The Limited Fluctuation Method was so named because it allowed premiums to fluctuate from year to year based on experience, while limiting those fluctuations by giving less than full credibility to premiums based on limited data. In contrast, setting premium rates by giving full credibility to recent experience could be called the Full Fluctuation Method. While every credibility method serves to limit fluctuations, this method acquired its name because it was the first. The Limited Fluctuation Method, also known as Classical Credibility, is the most widely-used credibility method because it can be relatively simple to apply. Outside North America, this method is sometimes referred to as American Credibility.\n\nQuoted from [Atkinson, 2019](https://www.soa.org/globalassets/assets/files/resources/tables-calcs-tools/credibility-methods-life-health-pensions.pdf).\n\n### Formulas\n\nThe Limited Fluctuation Method components: a credibility weight, $$Z$$, an\nObserved Rate, and a Prior Rate.\n\n$$\n\\text{Credibility-Weighted Rate} = Z × \\text{Observed Rate} + (1 – Z) × \\text{Prior Rate}\n$$\n\nWith probability equal to $LC_p$ that the Observed Rate does not differ from the true rate by more than $LC_r$.\n\n::: {#14d9a889 .cell execution_count=2}\n``` {.julia .cell-code}\nLC_p = 0.9\nLC_r  = 0.05\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n0.05\n```\n:::\n:::\n\n\n$$\n\\text{Claims for full credibility} = \\left(\\frac{\\text{Z-Score}}{\\text{ratio}}\\right)^{2}\n$$\n\n::: {#4bdb7eef .cell execution_count=3}\n``` {.julia .cell-code}\nLC_full_credibility = round(Int, (quantile(Normal(), 1 - (1 - LC_p) / 2) / LC_r)^2)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n1082\n```\n:::\n:::\n\n\nUsing the inputs above, we have a z-score corresponding to 0.9 of 1.6448536269514717, so: ( 1.6448536269514717 ÷ 0.05)² ≈  1082.\n\"\"\"\n\n[Atkinson](https://www.soa.org/globalassets/assets/files/resources/tables-calcs-tools/credibility-methods-life-health-pensions.pdf) goes on to nicely summarize the square root method which assigns full credibility, $$Z = 1$$, when the number of actual claims equals or exceeds the full credibility threshold of $LC_full_credibility claims. \n\nWhen the number of claims is less than full credibility:\n\n$$\nZ = \\sqrt{\\frac{\\text{no. claims in group}}{\\text{no. claims full credibility}}}\n$$\n\n### Issues with Limited Fluctuation\n\n- Ignores available information: \n  - Why does our $Z$ not vary by exposures?\n  - No mechanism for a related group to inform the credibility of another\n  - Results in a single point estimate with only approximate measure of estimate uncertainty\n- Relies on a number of assumptions/constraints:\n  - That aggregated claims can be approximated by a normal distribution\n  - The thresholds of $LC_p and $LC_r are arbitrary (e.g. all of the same arguments against p-value thresholds can apply to this approach)\n\"\"\"\n\n## Bayesian Approach\n\nBayes' formula of updating the posterior probability conditioned on the observed data:\n\n$$\nP(A\\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}\n$$\n\nThis is well known to most actuaries... but generally not applied frequently in practice!\n\nThe issue is that once the probability distributions and data become non-trivial, the formula becomes analytically intractable. Work over the last several decades has set the stage for addressing these situations by developing algorithms that let you sample the Bayesian posterior, even if you can't analytically say what that is.\n\nA full overview is beyond the scope of this notebook, which is simply to demonstrate that Limited Fluctuation Credibility is of questionable use in practice and that superior tools exist. For references on modern Bayesian statistics, see the end notes.\n\n\nNote that this is describing a *different*, more first-principles approach than the the Buhlman Bayesian approach, which attempts to simply relate group experience to population experience. The Bayesian approach described here is much more general and extensible.\n\n### The formulation\n\nContrary to Limited Fluctuation, the Bayesian approach forces one to be explicit about the presumed structure of the probability model. The flexibility of the statistical model allows one to incorporate actuarial judgement in a quantitative way. For example, in this example we assume that the claims experience of each group informs a global (hyperparameter) prior distribution which we could use as a starting point for a new type of observation. More on this once the data is introduced.\n\n## Sample Claims Prediction\n\nThe data comes from an [Allstate auto-claims data via Kaggle](https://www.kaggle.com/competitions/ClaimPredictionChallenge/data?select=dictionary.html). It contains exposure level information about predictor variable and claim amounts for calendar years 2005-2007. \n\nFor simplicity, we will focus on the narrow objective of estimating the claims rate at the level of automobile make and model. We will use the years 2005 and 2006 as the training data set and then 2007 to evaluate the predictive model.\n\nThe original data is over 13 million rows, we will load an already summarized CSV and split it into `train` and `test` sets based on the `Calendar_Year`.\n\n::: {#aa2c2ece .cell execution_count=4}\n``` {.julia .cell-code}\ntrain,test = let\n\tpth = download(\"https://raw.githubusercontent.com/JuliaActuary/Learn/master/data/condensed_auto_claims.csv\")\n\tdf = CSV.read(pth,DataFrame,normalizenames=true)\n\tdf.p_observed = df.claims ./ df.n\n\t\n\n\ttrain = df[df.Calendar_Year .< 2007,:]\n\ttest  = df[df.Calendar_Year .== 2007,:]\n\n\ttrain, test\n\t\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>(<span class=\"ansi-bold\">2413×5 DataFrame</span>\n<span class=\"ansi-bold\">  Row </span>│<span class=\"ansi-bold\"> Calendar_Year </span><span class=\"ansi-bold\"> Blind_Model </span><span class=\"ansi-bold\"> n     </span><span class=\"ansi-bold\"> claims </span><span class=\"ansi-bold\"> p_observed </span>\n      │<span class=\"ansi-bright-black-fg\"> Int64         </span><span class=\"ansi-bright-black-fg\"> String7     </span><span class=\"ansi-bright-black-fg\"> Int64 </span><span class=\"ansi-bright-black-fg\"> Int64  </span><span class=\"ansi-bright-black-fg\"> Float64    </span>\n──────┼───────────────────────────────────────────────────────\n    1 │          2005  K.78          6132      41  0.00668624\n    2 │          2005  Q.22         35141     270  0.00768333\n    3 │          2005  AR.41        18719     174  0.00929537\n    4 │          2006  AR.41        18868     196  0.010388\n    5 │          2005  D.20          4573      27  0.00590422\n    6 │          2006  D.20          5446      23  0.00422328\n    7 │          2006  AJ.129       50803     342  0.00673189\n    8 │          2006  AQ.17        44018     263  0.00597483\n    9 │          2005  AQ.17        42684     278  0.00651298\n   10 │          2005  BW.3         37903     215  0.00567237\n   11 │          2006  BW.3         37752     195  0.00516529\n  ⋮   │       ⋮             ⋮         ⋮      ⋮         ⋮\n 2404 │          2005  CA.5             1       0  0.0\n 2405 │          2005  J.5              1       0  0.0\n 2406 │          2006  J.5              1       0  0.0\n 2407 │          2005  CB.8             1       0  0.0\n 2408 │          2005  AQ.1             1       0  0.0\n 2409 │          2005  AJ.96            1       0  0.0\n 2410 │          2005  AJ.51            1       0  0.0\n 2411 │          2006  AJ.51            1       0  0.0\n 2412 │          2005  BQ.6             1       0  0.0\n 2413 │          2006  BQ.6             1       0  0.0\n<span class=\"ansi-cyan-fg\">                                             2392 rows omitted</span>, <span class=\"ansi-bold\">1289×5 DataFrame</span>\n<span class=\"ansi-bold\">  Row </span>│<span class=\"ansi-bold\"> Calendar_Year </span><span class=\"ansi-bold\"> Blind_Model </span><span class=\"ansi-bold\"> n      </span><span class=\"ansi-bold\"> claims </span><span class=\"ansi-bold\"> p_observed </span>\n      │<span class=\"ansi-bright-black-fg\"> Int64         </span><span class=\"ansi-bright-black-fg\"> String7     </span><span class=\"ansi-bright-black-fg\"> Int64  </span><span class=\"ansi-bright-black-fg\"> Int64  </span><span class=\"ansi-bright-black-fg\"> Float64    </span>\n──────┼────────────────────────────────────────────────────────\n    1 │          2007  X.45          99368     789  0.00794018\n    2 │          2007  Y.29          55783     435  0.00779807\n    3 │          2007  P.18           4923      46  0.0093439\n    4 │          2007  X.40           5573      22  0.0039476\n    5 │          2007  Y.42          25800     168  0.00651163\n    6 │          2007  AH.164        12564      88  0.00700414\n    7 │          2007  AH.119         4251      26  0.00611621\n    8 │          2007  W.3            7680      60  0.0078125\n    9 │          2007  BW.107        10897      53  0.00486372\n   10 │          2007  BW.79          6938      35  0.00504468\n   11 │          2007  AU.58         53424     508  0.00950883\n  ⋮   │       ⋮             ⋮         ⋮       ⋮         ⋮\n 1280 │          2007  BW.123            1       0  0.0\n 1281 │          2007  V.8               1       0  0.0\n 1282 │          2007  Z.18              1       0  0.0\n 1283 │          2007  X.14              2       0  0.0\n 1284 │          2007  AM.7              1       0  0.0\n 1285 │          2007  BU.37             1       0  0.0\n 1286 │          2007  AE.6              2       0  0.0\n 1287 │          2007  CB.6              1       0  0.0\n 1288 │          2007  BU.32             1       0  0.0\n 1289 │          2007  CA.5              1       0  0.0\n<span class=\"ansi-cyan-fg\">                                              1268 rows omitted</span>)</pre>\n```\n:::\n\n:::\n:::\n\n\n### Discussion of Bayesian model\n\nEach group (make and model combination) has an expected claim rate $μ_i$, which is informed by the global hyperparameter $μ$ and variance $\\sigma^2$. Then, the observed claims by group are assumed to be distributed according to a [Poisson distribution](https://juliaactuary.org/tutorials/poissonbinomial/).\n\nA complete overview of modern Bayesian models is beyond the scope of this notebook, but a few key points:\n\n- We are forced with the Bayesian approach to be explicit about the assumptions (versus all of the implicit assumptions of alternative techniques like LF)\n- We set *priors* which are the assumed distribution of model parameters before \"learning\" from the observations. With enough data, it can result in a posterior that the prior was very skeptical of beforehad.\n- With the volume of data in the training set, the priors we select here are not that important. Some comments on why they were chosen though:\n  - The rate of claims is linked with a *logistic* function, which looks like an integral sign of sorts. `logistic(0.0)` equals `0.5` while very negative inputs approach `0.0` and positive numbers approach `1.0`. We do this so that the rate of claims is always constrained in the range $(0,1)$\n  - ``μ \\sim Normal(-2,4) `` says that we expect the population of auto claims rate to be less than 0.5 (`logistic(-2) ≈ .12) but very wide range of possible values given the wide standard deviation.\n  - ``σ \\sim Exponential(0.25)`` says that we expect the standard devation of an individual group to be positive, but not super wide.\n  - ``μ_i \\sim Normal(μ,σ)`` is the actual prior for each group's rate of claim and is informed by the above hyperparameters. \n- `@model` is a Turing.jl macro which enables interpreting the nice `~` syntax, which makes the Julia code look very similar to the traditional mathematical notation.\n- Note the use of broadcasting with the dot syntax (e.g. the `.` in `.~`). This tells Julia to vectorize and fuse the computation. \n  - `data.claims .~ Poisson.(data.n .* logistic.(μ_i))` means \"each value in `data.claims` is a random outcome (`.~`) distributed accroding to a corresponding Poisson distribution with ``\\lambda =n \\times \\text{logistic}(\\mu_i)`` where ``text{logistic}(\\mu_i)`` is the average claims rate for each group.\n\n::: {#880fd3e0 .cell execution_count=5}\n``` {.julia .cell-code}\n@model function model_poisson(data)\n\t# hyperparameter that informs the prior for each group\n\tμ ~ Normal(-2,4) \n\tσ ~ Exponential(0.25)\n\n\t# the random variable representing the average claim rate for each group\n\t# filldist creates a set of random variable without needing to list out each one\n\tμ_i ~ filldist(Normal(μ,σ),length(unique(data.Blind_Model)))\n\n\t# use the poisson appproximation to the binomial claim outcome with a \n\t# logisitc link function to keep the probability between 0 and 1\n\tdata.claims .~ Poisson.(data.n .* logistic.(μ_i))\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nmodel_poisson (generic function with 2 methods)\n```\n:::\n:::\n\n\nHere's what the prior distribution of claims looks like... and peeking ahead to what the posterior for a single group of claims looks like. See how even though our posterior was very wide (favoring small claims rates), it was dominated by the data to create a very narrow posterior average claims rate.\n\nThe model is combined with the data and [Turing.jl](https://turing.ml/stable/) is used to computationally arrive at the posterior distribution for the parameters in the statistical model, $\\mu$, $\\mu_i$, and $\\sigma$.\n\n::: {#e860c943 .cell execution_count=6}\n``` {.julia .cell-code}\nmp = let\n\t# combine the different years in the training set \n\tcondensed = @chain train begin\n\t\tgroupby(:Blind_Model)\n\t\t@combine begin\n\t\t\t:n = sum(:n)\n\t\t\t:claims = sum(:claims)\n\t\tend\n\tend\n\tmodel_poisson(condensed);\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>DynamicPPL.Model{typeof(model_poisson), (:data,), (), (), Tuple{DataFrame}, Tuple{}, DynamicPPL.DefaultContext}(model_poisson, (data = <span class=\"ansi-bold\">1238×3 DataFrame</span>\n<span class=\"ansi-bold\">  Row </span>│<span class=\"ansi-bold\"> Blind_Model </span><span class=\"ansi-bold\"> n      </span><span class=\"ansi-bold\"> claims </span>\n      │<span class=\"ansi-bright-black-fg\"> String7     </span><span class=\"ansi-bright-black-fg\"> Int64  </span><span class=\"ansi-bright-black-fg\"> Int64  </span>\n──────┼─────────────────────────────\n    1 │ K.78          14058      94\n    2 │ Q.22          71401     532\n    3 │ AR.41         37587     370\n    4 │ D.20          10019      50\n    5 │ AJ.129       100824     693\n    6 │ AQ.17         86702     541\n    7 │ BW.3          75655     410\n    8 │ BW.167        42737     294\n    9 │ Y.9           92206     755\n   10 │ BH.29         15795     162\n   11 │ BW.49         27811     156\n  ⋮   │      ⋮         ⋮       ⋮\n 1229 │ AM.7              2       0\n 1230 │ AE.6              3       0\n 1231 │ BU.32             2       0\n 1232 │ BG.8              1       0\n 1233 │ CA.5              2       0\n 1234 │ J.5               2       0\n 1235 │ AQ.1              1       0\n 1236 │ AJ.96             1       0\n 1237 │ AJ.51             2       0\n 1238 │ BQ.6              2       0\n<span class=\"ansi-cyan-fg\">                   1217 rows omitted</span>,), NamedTuple(), DynamicPPL.DefaultContext())</pre>\n```\n:::\n\n:::\n:::\n\n\n### Bayesian Posterior Sampling\n\nHere's where recent advances in algorithms and computing power make Bayesian analyis possible. We can't analytically compute the posterior distribution, but we can genererate *samples* from the posterior such that the frequency of the sampled result appears in proportion to the true posterior density. This uses a technique called [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo), abbreviated MCMC. There are different algorithms in this family, and we will use one called the No-U-Turn Sampler (NUTS for short).\n\nThe result of the `sample` function is a set of data containing data that is generated in proportion to the posterior distributions and we will use this data to make predictions and understand the distribution of our parameters.\n\n\nRun the chain:\n\n::: {#5b0feb20 .cell execution_count=7}\n``` {.julia .cell-code}\ncp = sample(mp, NUTS(), 500) # this is the line that runs the MCMC sampler\n\n# ╔═╡ 6b5d2186-78ac-48ae-8529-e0dcd51d0b00\nlet \n\t# sample from the priors before learning from any data\n\tch_prior = sample(mp,Prior(),1500)\n\t\n\tf = Figure()\n\tax = Axis(f[1,1],title=\"Prior distribution of expected claims rate for a group\")\n\tμ = vec(ch_prior[\"μ_i[1]\"].data)\n\thist!(ax,logistic.(μ);bins=50)\n\tax2 = Axis(f[2,1],title=\"Posterior distribution of expected claims rate for a group\")\n\tμ = vec(cp[\"μ_i[1]\"].data)\n\thist!(ax2,logistic.(μ);bins=50)\n\tlinkxaxes!(ax,ax2)\n\tf\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n![](index_files/figure-html/cell-8-output-1.svg){}\n:::\n:::\n\n\n### Visualizing the Posterior Density\n\nThis is a plot of the posterior density for all of the many, many $\\mu$ parameters in our model. The black line shows the distribution which comes from our hyperparameters μ and σ. In the event of coming across a new group of interest (in our case, a new make of car), this is the prior distribution for the expected claims rate. The model has learned this from the data itself, and serves to regularize predictions.\n\n::: {#53514655 .cell execution_count=8}\n``` {.julia .cell-code code-fold=\"true\" code-summary=\"Utilitiy Function to Plot the Posterior\"}\n\"\"\"\n\tdplot(chain,parameter_string)\n\nplot the posterior density for model parameters matching the given string.\n\"\"\"\nfunction dplot(ch,param_string)\n\tf = Figure()\n\tax = Axis(f[1, 1])\n\n\t# plot each group posterior\n   for (ind, param) in enumerate(ch.name_map.parameters)\n\t\tif contains(string(param),param_string)\n\t\t\tv = vec(getindex(ch, param).data)\n\t\t\tdensity!(ax,logistic.(v), color = (:red, 0.0),\n    strokecolor = (:red,0.3), strokewidth = 1, strokearound = false, label=\"Individual group posterior\")\n\t\tend\n   end\n\t\n\t# Plot hyperparameters\n\thyper_mean = mean(getindex(ch, :μ).data)\n\thyper_sigma = mean(getindex(ch, :σ).data)\n\td = density!(ax,logistic.(rand(Normal(hyper_mean,hyper_sigma),1000)),color = (:black, 0.),\n    strokecolor = (:black,0.3), strokewidth = 3, strokearound = false,label=\"Hyper-parameter distribution\")\n\n\t\n\txlims!(0.0,0.03)\n\thideydecorations!(ax)\n\taxislegend(ax,unique=true)\n\tf\nend\n\ndplot(cp, \"μ\")\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n![](index_files/figure-html/cell-9-output-1.svg){}\n:::\n:::\n\n\n## Predictions and Results\n\nHere we compare four predictive models:\n\n1. Naive, where the predicted rate for the 2007 year is the average of each groups' for 2005-2006\n2. Limited Fluctuation Overall, where the \"prior\" in the LFC formula is the overall mean claim rate for 2005-2006\n3. Limited Fluctuation Year-by-Year where the \"prior\" in the LFC formula is the claim rate for the ith group in 2005 and updated using 2006 claim rates.\n4. Bayesian approach where the predicted claim rate is based on the bayesian model discussed above.\n\n### Predictions\n\n### Bayesian approach has lower error\n\nLooking at the accumulated absolute error, the bayesian approach has about 16% lower error than the Limited Fluctuation approaches.\n\n### Total Actual to Expected\n\nHere, offsetting errors happen to make the naive and LF year-by-year (the latter is a good proxy for the former) such that their total A/E is better than the Bayesian approach. \n\nGiven the lower error of the Bayesian approach, one would expect that over multiple years that it would produce more accurate predictions than the alternative methods.\n\n## Conclusion\n\nThis notebook shows that the Bayesian approach results in claims predictions with less total predictive error than the limited fluctuation method.\n\n### Further thoughts\n\nThe Bayesian approach could be extended to improve its accuracy even further:\n\n- Using vehicle make data to add more hierarchical structure to the statistical model. For example, one may observe that Porsches experience crashes at a higher rate than Volvos. LFC cannot embed that sort of overlapping hierarchy into its framework.\n- The Bayesian hyperparameter provides a framework to think about \"unseen\" make-model combinations\n\n### Downsides to the Bayesian approach\n\n- Computationally intensive. Complex `@models` can take very long time to run (many hours), compared to relatively quick frequentest methods like maximum likelihood estimation.\n\n### Further Reading\n\nIf this notebook has piqued your interest in Bayesian techniques, the following books are recommended learning resources (from easiest to most difficult):\n\n- Statistical Rethinking\n  - See also [StatisticalRethinking.jl](https://github.com/StatisticalRethinkingJulia/StatisticalRethinking.jl) for Julia implementations of all of the book's code and utility functions\n- Bayes Rules!\n- Bayeisan Data Analysis\n\n## Appendices\n\n\nAdditional legwork to get the alternate limited fluctuation approach data:\n\n::: {#afac079e .cell execution_count=9}\n``` {.julia .cell-code}\n# An alternate approach to LFC where the first year becomes the prior, adjusted by data from the seonc year.\nLF2 = let \n\t# split dataset by year and recombine\n\tdf2005 = @subset(train,:Calendar_Year .== 2005)\n\tdf2006 = @subset(train,:Calendar_Year .== 2006)\n\tdf = outerjoin(df2005,df2006,on=:Blind_Model,renamecols= \"_2005\" => \"_2006\")\n\n\t# use 2005 actuals as LFC prior, and the overall mean if model is missing\n\tμ_2005 = sum(skipmissing(df.claims_2005)) / sum(skipmissing(df.n_2005))\n\tdf.assumed = coalesce.(df.p_observed_2005,μ_2005)\n\t\n\tdf.LF2_Z = min.(1, .√(coalesce.(df.claims_2006,0.) ./ LC_full_credibility))\n\tdf.LF2_μ = let Z = df.LF2_Z\n\t\t# use the 2005 mean if the model not observed in 2006\n\t\tZ .* coalesce.(df.p_observed_2006,μ_2005) .+ (1 .- Z) .* df.assumed\n\tend\n\tdf\nend\n\nclaims_summary_posterior = let\n\t# combine the dataset by model\n\tdf = @chain train begin\n\t\tgroupby(:Blind_Model)\n\t\t@combine begin\n\t\t\t:n = sum(:n)\n\t\t\t:claims = sum(:claims)\n\t\tend\n\tend\n\tpop_μ = sum(df.claims) / sum(df.n)\n\tdf[!,:pop_μ] .= pop_μ\n\n\t## Bayesian prediction\n\t# get the mean of the posterior estimate for the ith model group\n\tmeans = map(1:length(unique(df.Blind_Model))) do i\n\t\td = logistic.(getindex(cp, Symbol(\"μ_i[$i]\")).data)\n\t\t(est = mean(d), se=std(d))\n\tend\n\n\tdf.p_observed = df.claims ./ df.n\n\tdf.bayes_μ = [x.est for x in means]\n\tdf.bayes_se = [x.se for x in means]\n\n\t## Limited Fluctuation (square root rule)\n\t# using overall population mean\n\t# using the square-root rule\n\tdf.LF_Z = min.(1, .√(df.claims ./ LC_full_credibility))\n\tdf.LF_μ = let Z = df.LF_Z\n\t\tZ .* (df.p_observed) .+ (1 .- Z) .* pop_μ\n\tend\n\n\t## Limited Fluctuation \n\t# using the first year as the prior, 2nd year as new data\n\t# using some additional procesing to get the LF2 dataframe, see appendix\n\tdf2005 = @subset(train, :Calendar_Year .== 2005)\n\tdict2005 = Dict(\n\t\tmodel => rate \n\t\tfor (model, rate) in zip(df2005.Blind_Model,df2005.p_observed)\n\t)\n\t\n\tdf = leftjoin(df,LF2[:,[:Blind_Model,:LF2_μ]],on=:Blind_Model,)\n\t\n\tdf = innerjoin(df,test;on=:Blind_Model,renamecols= \"_train\" => \"_test\")\n\n\t# predictions on the test set using the predictive rates time exposures\n\tdf.pred_bayes = df.n_test .* df.bayes_μ_train\n\tdf.pred_LF = df.n_test .* df.LF_μ_train\n\tdf.pred_LF2 = df.n_test .* df.LF2_μ_train\n\tdf.pred_naive = df.n_test .* df.p_observed_train\n\n\tsort!(df,:n_train,rev=true)\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div><div style = \"float: left;\"><span>1224×18 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1199 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Blind_Model</th><th style = \"text-align: left;\">n_train</th><th style = \"text-align: left;\">claims_train</th><th style = \"text-align: left;\">pop_μ_train</th><th style = \"text-align: left;\">p_observed_train</th><th style = \"text-align: left;\">bayes_μ_train</th><th style = \"text-align: left;\">bayes_se_train</th><th style = \"text-align: left;\">LF_Z_train</th><th style = \"text-align: left;\">LF_μ_train</th><th style = \"text-align: left;\">LF2_μ_train</th><th style = \"text-align: left;\">Calendar_Year_test</th><th style = \"text-align: left;\">n_test</th><th style = \"text-align: left;\">claims_test</th><th style = \"text-align: left;\">p_observed_test</th><th style = \"text-align: left;\">pred_bayes</th><th style = \"text-align: left;\">pred_LF</th><th style = \"text-align: left;\">pred_LF2</th><th style = \"text-align: left;\">pred_naive</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String7\" style = \"text-align: left;\">String7</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Union{Missing, Float64}\" style = \"text-align: left;\">Float64?</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">K.7</td><td style = \"text-align: right;\">382210</td><td style = \"text-align: right;\">3365</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00880406</td><td style = \"text-align: right;\">0.00878238</td><td style = \"text-align: right;\">0.000168854</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.00880406</td><td style = \"text-align: right;\">0.00853456</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">215223</td><td style = \"text-align: right;\">1798</td><td style = \"text-align: right;\">0.00835413</td><td style = \"text-align: right;\">1890.17</td><td style = \"text-align: right;\">1894.84</td><td style = \"text-align: right;\">1836.83</td><td style = \"text-align: right;\">1894.84</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">X.45</td><td style = \"text-align: right;\">192591</td><td style = \"text-align: right;\">1542</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0080066</td><td style = \"text-align: right;\">0.00800053</td><td style = \"text-align: right;\">0.000199085</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.0080066</td><td style = \"text-align: right;\">0.00801192</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">99368</td><td style = \"text-align: right;\">789</td><td style = \"text-align: right;\">0.00794018</td><td style = \"text-align: right;\">794.997</td><td style = \"text-align: right;\">795.6</td><td style = \"text-align: right;\">796.129</td><td style = \"text-align: right;\">795.6</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">AU.14</td><td style = \"text-align: right;\">188702</td><td style = \"text-align: right;\">1687</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00894002</td><td style = \"text-align: right;\">0.0089118</td><td style = \"text-align: right;\">0.00021057</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.00894002</td><td style = \"text-align: right;\">0.00874644</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">114742</td><td style = \"text-align: right;\">1071</td><td style = \"text-align: right;\">0.00933398</td><td style = \"text-align: right;\">1022.56</td><td style = \"text-align: right;\">1025.8</td><td style = \"text-align: right;\">1003.58</td><td style = \"text-align: right;\">1025.8</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">W.16</td><td style = \"text-align: right;\">150304</td><td style = \"text-align: right;\">1064</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00707899</td><td style = \"text-align: right;\">0.0070739</td><td style = \"text-align: right;\">0.000218869</td><td style = \"text-align: right;\">0.991647</td><td style = \"text-align: right;\">0.00708082</td><td style = \"text-align: right;\">0.00691906</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">83039</td><td style = \"text-align: right;\">510</td><td style = \"text-align: right;\">0.00614169</td><td style = \"text-align: right;\">587.41</td><td style = \"text-align: right;\">587.984</td><td style = \"text-align: right;\">574.552</td><td style = \"text-align: right;\">587.832</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">AU.11</td><td style = \"text-align: right;\">133445</td><td style = \"text-align: right;\">1188</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00890254</td><td style = \"text-align: right;\">0.0088575</td><td style = \"text-align: right;\">0.000264302</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.00890254</td><td style = \"text-align: right;\">0.00878751</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">70028</td><td style = \"text-align: right;\">642</td><td style = \"text-align: right;\">0.00916776</td><td style = \"text-align: right;\">620.273</td><td style = \"text-align: right;\">623.427</td><td style = \"text-align: right;\">615.372</td><td style = \"text-align: right;\">623.427</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">BO.38</td><td style = \"text-align: right;\">125206</td><td style = \"text-align: right;\">775</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0061898</td><td style = \"text-align: right;\">0.00619077</td><td style = \"text-align: right;\">0.000223624</td><td style = \"text-align: right;\">0.846325</td><td style = \"text-align: right;\">0.00636009</td><td style = \"text-align: right;\">0.0061673</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">68966</td><td style = \"text-align: right;\">437</td><td style = \"text-align: right;\">0.00633646</td><td style = \"text-align: right;\">426.952</td><td style = \"text-align: right;\">438.63</td><td style = \"text-align: right;\">425.334</td><td style = \"text-align: right;\">426.886</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">AO.7</td><td style = \"text-align: right;\">109746</td><td style = \"text-align: right;\">1055</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00961311</td><td style = \"text-align: right;\">0.00956512</td><td style = \"text-align: right;\">0.000305165</td><td style = \"text-align: right;\">0.987444</td><td style = \"text-align: right;\">0.00958404</td><td style = \"text-align: right;\">0.00956317</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">59534</td><td style = \"text-align: right;\">543</td><td style = \"text-align: right;\">0.00912084</td><td style = \"text-align: right;\">569.45</td><td style = \"text-align: right;\">570.576</td><td style = \"text-align: right;\">569.334</td><td style = \"text-align: right;\">572.307</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">AJ.58</td><td style = \"text-align: right;\">107538</td><td style = \"text-align: right;\">891</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00828544</td><td style = \"text-align: right;\">0.00826714</td><td style = \"text-align: right;\">0.000252584</td><td style = \"text-align: right;\">0.907455</td><td style = \"text-align: right;\">0.00819405</td><td style = \"text-align: right;\">0.00824136</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">52176</td><td style = \"text-align: right;\">394</td><td style = \"text-align: right;\">0.00755136</td><td style = \"text-align: right;\">431.346</td><td style = \"text-align: right;\">427.533</td><td style = \"text-align: right;\">430.001</td><td style = \"text-align: right;\">432.301</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">AJ.52</td><td style = \"text-align: right;\">107252</td><td style = \"text-align: right;\">691</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00644277</td><td style = \"text-align: right;\">0.00643023</td><td style = \"text-align: right;\">0.000263138</td><td style = \"text-align: right;\">0.799145</td><td style = \"text-align: right;\">0.00661453</td><td style = \"text-align: right;\">0.00644592</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">51952</td><td style = \"text-align: right;\">292</td><td style = \"text-align: right;\">0.00562057</td><td style = \"text-align: right;\">334.063</td><td style = \"text-align: right;\">343.638</td><td style = \"text-align: right;\">334.879</td><td style = \"text-align: right;\">334.715</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">AJ.129</td><td style = \"text-align: right;\">100824</td><td style = \"text-align: right;\">693</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00687336</td><td style = \"text-align: right;\">0.00684782</td><td style = \"text-align: right;\">0.000263334</td><td style = \"text-align: right;\">0.8003</td><td style = \"text-align: right;\">0.00695814</td><td style = \"text-align: right;\">0.00685673</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">48920</td><td style = \"text-align: right;\">323</td><td style = \"text-align: right;\">0.00660262</td><td style = \"text-align: right;\">334.995</td><td style = \"text-align: right;\">340.392</td><td style = \"text-align: right;\">335.431</td><td style = \"text-align: right;\">336.245</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">AU.58</td><td style = \"text-align: right;\">98015</td><td style = \"text-align: right;\">970</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00989644</td><td style = \"text-align: right;\">0.00980722</td><td style = \"text-align: right;\">0.000301882</td><td style = \"text-align: right;\">0.94683</td><td style = \"text-align: right;\">0.00975828</td><td style = \"text-align: right;\">0.00989406</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">53424</td><td style = \"text-align: right;\">508</td><td style = \"text-align: right;\">0.00950883</td><td style = \"text-align: right;\">523.941</td><td style = \"text-align: right;\">521.326</td><td style = \"text-align: right;\">528.58</td><td style = \"text-align: right;\">528.708</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">X.38</td><td style = \"text-align: right;\">97642</td><td style = \"text-align: right;\">837</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00857213</td><td style = \"text-align: right;\">0.00851555</td><td style = \"text-align: right;\">0.000300658</td><td style = \"text-align: right;\">0.879527</td><td style = \"text-align: right;\">0.00841862</td><td style = \"text-align: right;\">0.00854678</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">49682</td><td style = \"text-align: right;\">396</td><td style = \"text-align: right;\">0.00797069</td><td style = \"text-align: right;\">423.069</td><td style = \"text-align: right;\">418.254</td><td style = \"text-align: right;\">424.621</td><td style = \"text-align: right;\">425.881</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">Y.34</td><td style = \"text-align: right;\">92959</td><td style = \"text-align: right;\">734</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00789595</td><td style = \"text-align: right;\">0.00785963</td><td style = \"text-align: right;\">0.000281347</td><td style = \"text-align: right;\">0.823634</td><td style = \"text-align: right;\">0.00779048</td><td style = \"text-align: right;\">0.00785037</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">53780</td><td style = \"text-align: right;\">443</td><td style = \"text-align: right;\">0.00823726</td><td style = \"text-align: right;\">422.691</td><td style = \"text-align: right;\">418.972</td><td style = \"text-align: right;\">422.193</td><td style = \"text-align: right;\">424.644</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1213</td><td style = \"text-align: left;\">BU.32</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00630026</td><td style = \"text-align: right;\">0.00164119</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00630026</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1214</td><td style = \"text-align: left;\">CA.5</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00636515</td><td style = \"text-align: right;\">0.00160636</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00636515</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1215</td><td style = \"text-align: left;\">D.18</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00630744</td><td style = \"text-align: right;\">0.0015326</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">512</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0.00390625</td><td style = \"text-align: right;\">3.22941</td><td style = \"text-align: right;\">3.73652</td><td style = \"text-align: right;\">3.83434</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1216</td><td style = \"text-align: left;\">Q.7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0063304</td><td style = \"text-align: right;\">0.00166114</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0189912</td><td style = \"text-align: right;\">0.0218937</td><td style = \"text-align: right;\">0.0224668</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1217</td><td style = \"text-align: left;\">BW.139</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00648606</td><td style = \"text-align: right;\">0.00170544</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00648606</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1218</td><td style = \"text-align: left;\">BG.8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00645148</td><td style = \"text-align: right;\">0.00169365</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">28</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.180642</td><td style = \"text-align: right;\">0.204341</td><td style = \"text-align: right;\">0.20969</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1219</td><td style = \"text-align: left;\">R.35</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00639538</td><td style = \"text-align: right;\">0.00160778</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0127908</td><td style = \"text-align: right;\">0.0145958</td><td style = \"text-align: right;\">0.0149779</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1220</td><td style = \"text-align: left;\">I.6</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00648387</td><td style = \"text-align: right;\">0.00182023</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00648387</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1221</td><td style = \"text-align: left;\">BM.8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00642479</td><td style = \"text-align: right;\">0.00180695</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00642479</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.00748894</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1222</td><td style = \"text-align: left;\">AJ.96</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00635007</td><td style = \"text-align: right;\">0.00161312</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00635007</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1223</td><td style = \"text-align: left;\">AQ.1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0063907</td><td style = \"text-align: right;\">0.00171822</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0063907</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1224</td><td style = \"text-align: left;\">AJ.118</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00646154</td><td style = \"text-align: right;\">0.00171032</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2007</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.00646154</td><td style = \"text-align: right;\">0.0072979</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {#45653480 .cell execution_count=10}\n``` {.julia .cell-code}\nlet df = claims_summary_posterior\n\tf = Figure()\n\tax = Axis(f[1,1],title=\"Cumulative Predictive Residual Error\", subtitle=\"(Lower is better predictive accuracy)\",xlabel=L\"$i^{th}$ vehicle group\",ylabel=L\"absolute error\")\n\tlines!(ax,cumsum(abs.(df.pred_LF .- df.claims_test)),label=\"Limited Fluctuation Overall\",color=(:purple,0.5))\n\tlines!(ax,cumsum(abs.(df.pred_LF2 .- df.claims_test)),label=\"Limited Fluctuation Year-by-Year\",color=(:blue,0.5))\n\tlines!(ax,cumsum(abs.(df.pred_bayes .- df.claims_test)),label=\"Bayesian\",color=(:red,0.5))\n\tlines!(ax,cumsum(abs.(df.pred_naive .- df.claims_test)),label=\"Naive\",color=(:grey10,0.5))\n\t# xlims!(0,40)\n\t# Legend(f[1,1],[s1,s2],[\"LFC\"halign=:right,valign=:top)\n\taxislegend(ax,position=:rb)\n\tf\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n![](index_files/figure-html/cell-11-output-1.svg){}\n:::\n:::\n\n\n::: {#1ee57b00 .cell execution_count=11}\n``` {.julia .cell-code}\nlet \n\tpost = claims_summary_posterior\n\tX = @chain vcat(train,test) begin\n\t\tgroupby(:Calendar_Year)\n\t\t@combine :claim_rate = sum(:claims) / sum(:n)\n\tend\n\tf = Figure()\n\tax = Axis(f[1,1],xlabel=\"year\",ylabel=\"claims rate\")\n\tscatter!(ax,X.Calendar_Year,X.claim_rate, label=\"data\")\n\tscatter!(ax,[2007],[sum(post.pred_bayes)/sum(post.n_test)], label=\"Bayes\", marker=:hline,markersize=30)\n\tscatter!(ax,[2007],[sum(post.pred_LF)/sum(post.n_test)], label=\"LF Overall\",marker=:hline,markersize=30)\n\tscatter!(ax,[2007],[sum(post.pred_LF2)/sum(post.n_test)], label=\"LF Year-by-Year\",marker=:hline,markersize=30)\n\tscatter!(ax,[2007],[sum(post.pred_naive)/sum(post.n_test)], label=\"naive\",marker=:hline,markersize=30)\n\taxislegend(ax,position=:rb)\n\tylims!(0.005,0.008)\n\tf\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](index_files/figure-html/cell-12-output-1.svg){}\n:::\n:::\n\n\n## Some Remarks about the Results\n\nWith limited, real-world data drawing conclusions is a little bit messy because we don't know what the ground-truth should be, but here are some thoughts that seem to be consistent with what the data and results suggest:\n\n- The Bayesian approach partially pools the data: it's an approach in-between assuming each group is independent from the rest and assuming that a single rate applies to all exposures.\n- The partial pooling limits over-fitting, which happens with the naive approach. Our Bayesian approach is somewhat skeptical of groups with low observation counts that stand out from the rest. But it also doesn't forgoe useful data, as it learns from even low exposures according to what's consistent with probability theory (Baye's rule).\n- The naive approach is pretty close to the textbook definition of over-fitting. The LF approaches appear to be under-fitting group-level as there are only $(sum(claims_summary_posterior.LF_Z_train .>= .9999)) groups with \"full credibility\" (``Z=1``), even though there are groups with less than \"full credibility\" with over 100,000 observations in the training set.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}